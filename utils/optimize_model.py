#!/usr/bin/env python3
"""
Modell-Optimierung fÃ¼r Bundeskanzler-KI
FÃ¼hrt alle Optimierungen durch und zeigt Ergebnisse an
"""

import sys
import os
import logging
import numpy as np
import time
from pathlib import Path

# Logging konfigurieren
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Projekt-Pfad hinzufÃ¼gen
sys.path.insert(0, '/home/tobber/bkki_venv')

try:
    from model_optimizer import ModelOptimizer
except ImportError as e:
    logger.error(f"Fehler beim Importieren: {e}")
    sys.exit(1)

def create_sample_data():
    """Erstellt Beispieldaten fÃ¼r Training und Test"""
    logger.info("ğŸ”„ Erstelle Beispieldaten...")

    # Vereinfachte Text-Daten fÃ¼r Demo
    vocab_size = 1000
    max_length = 100
    num_samples = 1000

    # Dummy-Daten generieren
    x_train = np.random.randint(0, vocab_size, size=(num_samples, max_length))
    y_train = np.random.randint(0, vocab_size, size=(num_samples, vocab_size))

    x_test = np.random.randint(0, vocab_size, size=(200, max_length))
    y_test = np.random.randint(0, vocab_size, size=(200, vocab_size))

    logger.info(f"âœ… Beispieldaten erstellt: {num_samples} Train, {len(x_test)} Test Samples")
    return x_train, y_train, x_test, y_test

def train_base_model(x_train, y_train):
    """Trainiert ein Basis-Modell falls keines vorhanden ist"""
    model_path = "bundeskanzler_ki_model.keras"

    # LÃ¶sche altes Modell falls vorhanden (wegen GPU-Problemen)
    if os.path.exists(model_path):
        logger.info("ğŸ—‘ï¸  LÃ¶sche altes Modell wegen KompatibilitÃ¤tsproblemen...")
        os.remove(model_path)

    logger.info("ğŸ”„ Trainiere neues Basis-Modell...")

    try:
        # TensorFlow-Modell direkt erstellen (vereinfacht fÃ¼r Demo)
        import tensorflow as tf

        # GPU-Probleme vermeiden - CPU-only fÃ¼r Training
        tf.config.set_visible_devices([], 'GPU')

        vocab_size = 1000
        embedding_dim = 128
        rnn_units = 256

        model = tf.keras.Sequential([
            tf.keras.layers.Embedding(vocab_size, embedding_dim),
            tf.keras.layers.GRU(rnn_units, return_sequences=True),
            tf.keras.layers.GRU(rnn_units),
            tf.keras.layers.Dense(vocab_size)
        ])

        model.compile(
            loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
            optimizer='adam',
            metrics=['accuracy']
        )

        # Kurzes Training fÃ¼r Demo
        logger.info("âš ï¸  Hinweis: FÃ¼r vollstÃ¤ndige Optimierung echte Trainingsdaten verwenden")
        model.fit(x_train, y_train, epochs=2, batch_size=32, verbose=1)

        # Modell speichern
        model.save(model_path)
        logger.info(f"âœ… Basis-Modell gespeichert: {model_path}")

        return model_path

    except Exception as e:
        logger.error(f"Fehler beim Training des Basis-Modells: {e}")
        return None

def run_optimization():
    """FÃ¼hrt die vollstÃ¤ndige Modell-Optimierung durch"""
    logger.info("ğŸš€ Starte Modell-Optimierung fÃ¼r Bundeskanzler-KI")

    try:
        # Beispieldaten erstellen
        x_train, y_train, x_test, y_test = create_sample_data()

        # Basis-Modell trainieren
        model_path = train_base_model(x_train, y_train)
        if not model_path:
            logger.error("Konnte kein Basis-Modell erstellen")
            return

        # Modell-Optimierer initialisieren
        optimizer = ModelOptimizer(model_path)

        # VollstÃ¤ndige Optimierung durchfÃ¼hren
        report = optimizer.optimize_all(x_train, y_train, x_test, y_test)

        # Ergebnisse anzeigen
        print("\n" + "="*60)
        print("ğŸ“Š MODELL-OPTIMIERUNG ERGEBNISSE")
        print("="*60)

        print(f"\nğŸ“ Original-Modell: {model_path}")
        print(f"ğŸ”§ Optimierungen angewendet: {', '.join(optimizer.optimized_models.keys())}")

        print(f"\nğŸ“ˆ Benchmark-Ergebnisse:")
        print("-" * 40)

        for model_name, results in report['benchmark_results'].items():
            print(f"\nğŸ”¹ {model_name.upper()}:")
            if 'error' in results:
                print(f"   âŒ Fehler: {results['error']}")
            else:
                if 'inference_time' in results:
                    print(".3f")
                if 'accuracy' in results:
                    print(".3f")
                if 'model_size' in results:
                    size_mb = results['model_size'] / (1024 * 1024)
                    print(".2f")
                if 'parameters' in results:
                    print(f"   ğŸ“ Parameter: {results['parameters']:,}")
                if 'type' in results:
                    print(f"   ğŸ”§ Typ: {results['type']}")

        if report['recommendations']:
            print(f"\nğŸ’¡ EMPFEHLUNGEN:")
            print("-" * 40)
            for rec in report['recommendations']:
                print(f"   â€¢ {rec}")

        print(f"\nğŸ“„ Detaillierter Bericht: model_optimization_report.json")

        print("\n" + "="*60)
        print("âœ… Modell-Optimierung erfolgreich abgeschlossen!")
        print("="*60)

    except Exception as e:
        logger.error(f"Fehler bei der Modell-Optimierung: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    run_optimization()