#!/usr/bin/env python3
"""
üöÄ Bundeskanzler KI - Moderne Web-GUI
====================================

Moderne, benutzerfreundliche Weboberfl√§che f√ºr die Bundeskanzler KI
mit Fact-Checking Visualisierung und multilingualer Unterst√ºtzung.

Features:
- üé® Moderne UI mit Dark/Light Mode
- ‚úÖ Echtzeit Fact-Checking Visualisierung
- üåç Mehrsprachige Unterst√ºtzung
- üìä Live GPU-Monitoring & System-Dashboard
- üí¨ Erweitertes Chat-Interface mit Historie
- üì± Mobile-responsive Design
- üîÑ Live API-Integration mit Fallback
- üìà Performance-Metriken & Statistiken

Autor: Claude-3.5-Sonnet
Datum: 16. September 2025
"""

import json
import time
from datetime import datetime
from typing import Any, Dict, List, Optional

import requests
import streamlit as st

# Erweitertes Debugging-System importieren
try:
    from debug_system import debug_system
    DEBUG_SYSTEM_AVAILABLE = True
except ImportError:
    DEBUG_SYSTEM_AVAILABLE = False
    import logging
    debug_system = logging.getLogger(__name__)

# Automatische Fehlerdiagnose importieren
try:
    from error_diagnosis import ErrorDiagnoser
    error_diagnoser = ErrorDiagnoser()
    ERROR_DIAGNOSIS_AVAILABLE = True
except ImportError:
    ERROR_DIAGNOSIS_AVAILABLE = False
    error_diagnoser = None

# GPU Performance Optimizer importieren
try:
    from gpu_performance_optimizer import gpu_optimizer
    GPU_OPTIMIZER_AVAILABLE = True
except ImportError:
    GPU_OPTIMIZER_AVAILABLE = False
    gpu_optimizer = None

try:
    import plotly.express as px
    import plotly.graph_objects as go
    PLOTLY_AVAILABLE = True
except ImportError:
    PLOTLY_AVAILABLE = False

try:
    from streamlit_lottie import st_lottie
    LOTTIE_AVAILABLE = True
except ImportError:
    LOTTIE_AVAILABLE = False

try:
    import extra_streamlit_components as stx
    EXTRA_COMPONENTS_AVAILABLE = True
except ImportError:
    EXTRA_COMPONENTS_AVAILABLE = False
    EXTRA_COMPONENTS_AVAILABLE = False

# API Konfiguration
API_BASE_URL = "http://localhost:8000"
KI_API_URL = "http://localhost:8000/query"  # Haupt-KI Endpoint

# Konfiguration
st.set_page_config(
    page_title="ü§ñ Bundeskanzler KI",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded",
)

# API Integration Funktion
def query_ki_api(question: str, language: str = "de") -> Dict[str, Any]:
    """Frage die KI-API mit robuster Fehlerbehandlung und GPU-Optimierung"""
    try:
        # GPU Performance Optimierung
        if GPU_OPTIMIZER_AVAILABLE and gpu_optimizer:
            try:
                optimized_query = gpu_optimizer.optimize_query(question, language)
                debug_system.info("üöÄ Query GPU-optimiert (GUI)",
                                component="gpu_optimization_gui",
                                cached=optimized_query.get('cached', False),
                                processing_time=optimized_query.get('processing_time', 0))
                actual_question = optimized_query.get('query', question)
            except Exception as e:
                debug_system.warning(f"‚ö†Ô∏è GUI GPU-Optimierung fehlgeschlagen: {e}",
                                   component="gpu_optimization_gui", error=str(e))
                actual_question = question
        else:
            actual_question = question

        payload = {
            "query": actual_question,
            "language": language
        }

        response = requests.post(
            KI_API_URL,
            json=payload,
            timeout=30  # L√§ngeres Timeout f√ºr komplexe Queries
        )

        if response.status_code == 200:
            result = response.json()
            debug_system.info(f"‚úÖ KI-API erfolgreich: {len(result.get('response', ''))} Zeichen",
                            component="api_query", response_length=len(result.get('response', '')),
                            processing_time=result.get('processing_time', 0))
            return result
        else:
            debug_system.warning(f"‚ö†Ô∏è KI-API Fehler: {response.status_code}",
                               component="api_query", status_code=response.status_code,
                               response_text=response.text[:200])
            return {
                "query": actual_question,
                "response": f"API-Fehler ({response.status_code}): {response.text}",
                "status": "error",
                "processing_time": 0.0,
                "confidence": 0.0
            }

    except requests.exceptions.Timeout:
        debug_system.error("‚è∞ KI-API Timeout", component="api_query", error_type="timeout")
        # Fehlerdiagnose durchf√ºhren
        diagnosis = None
        if ERROR_DIAGNOSIS_AVAILABLE and error_diagnoser:
            diagnosis = error_diagnoser.diagnose_error("TimeoutError: Request timed out")
        return {
            "query": actual_question,
            "response": "Die Anfrage hat zu lange gedauert. Bitte versuchen Sie es sp√§ter erneut.",
            "status": "timeout",
            "processing_time": 0.0,
            "confidence": 0.0,
            "error_diagnosis": diagnosis
        }

    except requests.exceptions.ConnectionError:
        debug_system.error("üîå KI-API Verbindung fehlgeschlagen", component="api_query", error_type="connection_error")
        # Fehlerdiagnose durchf√ºhren
        diagnosis = None
        if ERROR_DIAGNOSIS_AVAILABLE and error_diagnoser:
            diagnosis = error_diagnoser.diagnose_error("Connection refused")
        return {
            "query": actual_question,
            "response": "Verbindung zur KI-API fehlgeschlagen. Stellen Sie sicher, dass die API l√§uft.",
            "status": "connection_error",
            "processing_time": 0.0,
            "confidence": 0.0,
            "error_diagnosis": diagnosis
        }

    except Exception as e:
        debug_system.error(f"‚ùå Unerwarteter KI-API Fehler: {e}",
                          component="api_query", error=str(e), error_type=type(e).__name__)
        # Fehlerdiagnose durchf√ºhren
        diagnosis = None
        if ERROR_DIAGNOSIS_AVAILABLE and error_diagnoser:
            diagnosis = error_diagnoser.diagnose_error(str(e))
        return {
            "query": actual_question,
            "response": f"Unerwarteter Fehler: {str(e)}",
            "status": "unexpected_error",
            "processing_time": 0.0,
            "confidence": 0.0,
            "error_diagnosis": diagnosis
        }

# System-Status Funktion
def get_system_status() -> Dict[str, Any]:
    """Hole System-Status von der API"""
    try:
        health_response = requests.get(f"{API_BASE_URL}/health", timeout=5)
        stats_response = requests.get(f"{API_BASE_URL}/stats", timeout=5)

        health_data = health_response.json() if health_response.status_code == 200 else {}
        stats_data = stats_response.json() if stats_response.status_code == 200 else {}

        return {
            "health": health_data,
            "stats": stats_data,
            "api_available": health_response.status_code == 200,
            "timestamp": time.time()
        }

    except Exception as e:
        debug_system.warning(f"‚ö†Ô∏è System-Status Fehler: {e}",
                           component="system_status", error=str(e), error_type=type(e).__name__)
        # Fehlerdiagnose durchf√ºhren
        diagnosis = None
        if ERROR_DIAGNOSIS_AVAILABLE and error_diagnoser:
            diagnosis = error_diagnoser.diagnose_error(str(e))
        return {
            "health": {"status": "unknown", "message": str(e)},
            "stats": {},
            "api_available": False,
            "timestamp": time.time(),
            "error_diagnosis": diagnosis
        }

# Export-Funktionen
def export_chat_history_json():
    """Exportiere Chat-Verlauf als JSON"""
    if not st.session_state.chat_history:
        return None

    export_data = {
        "export_timestamp": datetime.now().isoformat(),
        "total_messages": len(st.session_state.chat_history),
        "chat_history": st.session_state.chat_history
    }
    return json.dumps(export_data, indent=2, ensure_ascii=False)

def export_chat_history_text():
    """Exportiere Chat-Verlauf als lesbaren Text"""
    if not st.session_state.chat_history:
        return "Keine Chat-Nachrichten vorhanden."

    text_output = f"ü§ñ Bundeskanzler KI - Chat Export\n"
    text_output += f"üìÖ Exportiert am: {datetime.now().strftime('%d.%m.%Y %H:%M:%S')}\n"
    text_output += f"üí¨ Nachrichten: {len(st.session_state.chat_history)}\n"
    text_output += "=" * 50 + "\n\n"

    for i, message in enumerate(st.session_state.chat_history, 1):
        timestamp = message.get('timestamp', datetime.now()).strftime('%H:%M:%S')
        role = "üë§ Sie" if message['role'] == 'user' else "ü§ñ KI"
        content = message['content']

        text_output += f"[{timestamp}] {role}:\n{content}\n\n"

        # Fact-Check hinzuf√ºgen falls vorhanden
        if message.get('fact_check'):
            text_output += "‚úÖ Fact-Check Ergebnisse verf√ºgbar\n\n"

    return text_output

def create_export_section():
    """Erstelle Export-Bereich in der Sidebar"""
    st.markdown("---")
    st.markdown("## üì§ Export & Speichern")

    if st.session_state.chat_history:
        col1, col2 = st.columns(2)

        with col1:
            # JSON Export
            json_data = export_chat_history_json()
            st.download_button(
                label="üìÑ JSON Export",
                data=json_data,
                file_name=f"bundeskanzler_chat_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json",
                key="json_export"
            )

        with col2:
            # Text Export
            text_data = export_chat_history_text()
            st.download_button(
                label="üìù Text Export",
                data=text_data,
                file_name=f"bundeskanzler_chat_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                mime="text/plain",
                key="text_export"
            )

        # Chat-Verlauf l√∂schen
        if st.button("üóëÔ∏è Chat l√∂schen", key="clear_chat"):
            st.session_state.chat_history = []
            st.session_state.current_query = ""
            st.session_state.query_submitted = False
            st.success("üóëÔ∏è Chat-Verlauf gel√∂scht!")
            st.rerun()
    else:
        st.info("üí¨ Kein Chat-Verlauf zum Exportieren vorhanden")

# Fehlerdiagnose-Anzeige Funktion
def render_error_diagnosis():
    """Zeige Fehlerdiagnose-Informationen in der GUI"""
    if not ERROR_DIAGNOSIS_AVAILABLE:
        return

    st.markdown("---")
    st.markdown("## üîç Fehlerdiagnose")

    # Zeige letzte Fehlerdiagnosen
    if hasattr(error_diagnoser, 'error_history') and error_diagnoser.error_history:
        with st.expander("üìã Letzte Fehlerdiagnosen", expanded=False):
            for i, error_entry in enumerate(error_diagnoser.error_history[-5:]):  # Zeige letzte 5
                diagnosis = error_entry.get('diagnosis', {})
                st.markdown(f"**Fehler {i+1}:** {diagnosis.get('diagnosis', 'Unbekannt')}")
                st.markdown(f"**Kategorie:** {diagnosis.get('category', 'Unbekannt')}")
                st.markdown(f"**Schweregrad:** {diagnosis.get('severity', 'Unbekannt')}")
                if diagnosis.get('auto_fix_available'):
                    st.success("üîß Auto-Fix verf√ºgbar")
                else:
                    st.info("‚ÑπÔ∏è Manuelle L√∂sung erforderlich")
                st.markdown("---")

    # Fehlerstatistiken
    if hasattr(error_diagnoser, 'get_error_statistics'):
        try:
            stats = error_diagnoser.get_error_statistics()
            if stats:
                st.markdown("### üìä Fehlerstatistiken")
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Gesamtfehler", stats.get('total_errors', 0))
                with col2:
                    st.metric("Auto-Fixes", stats.get('auto_fixes_applied', 0))
                with col3:
                    st.metric("Erfolgsrate", f"{stats.get('success_rate', 0):.1f}%")
        except Exception as e:
            debug_system.warning(f"‚ö†Ô∏è Fehlerstatistiken konnten nicht geladen werden: {e}",
                               component="error_diagnosis_ui", error=str(e))
    base_css = """
<style>
    .main-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 2rem;
        border-radius: 15px;
        margin-bottom: 2rem;
        text-align: center;
    }
    .query-card {
        background: #f8f9fa;
        border: 2px solid #e9ecef;
        border-radius: 10px;
        padding: 1.5rem;
        margin: 1rem 0;
        transition: all 0.3s ease;
    }
    .query-card:hover {
        border-color: #007bff;
        box-shadow: 0 4px 8px rgba(0,123,255,0.1);
    }
    .fact-check-indicator {
        display: inline-block;
        padding: 0.5rem 1rem;
        border-radius: 20px;
        font-weight: bold;
        margin: 0.5rem;
    }
    .confidence-high { background: #d4edda; color: #155724; }
    .confidence-medium { background: #fff3cd; color: #856404; }
    .confidence-low { background: #f8d7da; color: #721c24; }
    .source-badge {
        background: #e7f3ff;
        color: #0066cc;
        padding: 0.25rem 0.5rem;
        border-radius: 10px;
        font-size: 0.8rem;
        margin: 0.25rem;
        display: inline-block;
    }
    .chat-message {
        padding: 1rem;
        border-radius: 10px;
        margin: 0.5rem 0;
        max-width: 80%;
    }
    .user-message {
        background: #007bff;
        color: white;
        margin-left: auto;
        text-align: right;
    }
    .ai-message {
        background: #f8f9fa;
        border: 1px solid #e9ecef;
    }
    .gpu-metric {
        background: linear-gradient(45deg, #ff6b6b, #4ecdc4);
        color: white;
        padding: 1rem;
        border-radius: 10px;
        text-align: center;
        margin: 0.5rem;
    }

    /* Mobile Optimierungen */
    @media (max-width: 768px) {
        .main-header {
            padding: 1rem !important;
            margin-bottom: 1rem !important;
        }
        .main-header h1 {
            font-size: 1.5rem !important;
        }
        .chat-message {
            max-width: 95% !important;
            font-size: 0.9rem !important;
        }
        .gpu-metric {
            padding: 0.5rem !important;
            margin: 0.25rem !important;
        }
        .gpu-metric h4 {
            font-size: 0.8rem !important;
        }
        .gpu-metric h2 {
            font-size: 1.2rem !important;
        }
    }

    /* Performance Optimierungen */
    .stButton button {
        transition: all 0.2s ease !important;
    }
    .stTextArea textarea {
        transition: border-color 0.2s ease !important;
    }
"""

    if theme == "dark":
        dark_css = """
    .query-card {
        background: #2d3748 !important;
        border-color: #4a5568 !important;
        color: #e2e8f0 !important;
    }
    .ai-message {
        background: #2d3748 !important;
        border-color: #4a5568 !important;
        color: #e2e8f0 !important;
    }
    [data-testid="stSidebar"] {
        background-color: #1a202c !important;
    }
    [data-testid="stSidebar"] * {
        color: #e2e8f0 !important;
    }
    body {
        color: #e2e8f0 !important;
        background-color: #1a202c !important;
    }
    .stTextArea textarea, .stSelectbox select {
        background-color: #2d3748 !important;
        color: #e2e8f0 !important;
        border-color: #4a5568 !important;
    }
"""
        st.markdown(f"<style>{base_css + dark_css}</style>", unsafe_allow_html=True)
    else:
        st.markdown(f"<style>{base_css}</style>", unsafe_allow_html=True)

# CSS anwenden
if "theme" not in st.session_state:
    st.session_state.theme = "light"

# Theme anwenden (Funktion wird sp√§ter definiert)
# apply_theme(st.session_state.theme)

# Session State Initialisierung
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "current_query" not in st.session_state:
    st.session_state.current_query = ""
if "query_submitted" not in st.session_state:
    st.session_state.query_submitted = False
if "fact_check_results" not in st.session_state:
    st.session_state.fact_check_results = None
if "gpu_metrics" not in st.session_state:
    st.session_state.gpu_metrics = {"usage": 0, "memory": 0, "temperature": 0}
if "api_cache" not in st.session_state:
    st.session_state.api_cache = {}
if "last_gpu_update" not in st.session_state:
    st.session_state.last_gpu_update = 0


# GPU Monitoring Funktion
def get_gpu_metrics():
    """Hole aktuelle GPU-Metriken mit Caching"""
    current_time = time.time()

    # Cache f√ºr 5 Sekunden verwenden
    if current_time - st.session_state.last_gpu_update < 5:
        return st.session_state.gpu_metrics

    try:
        # Hier w√ºrde die echte GPU-API aufgerufen werden
        # F√ºr Demo-Zwecke simulieren wir Daten
        import random

        # Realistischere Simulation basierend auf vorherigen Werten
        prev_usage = st.session_state.gpu_metrics.get("usage", 12)
        prev_memory = st.session_state.gpu_metrics.get("memory", 1800)
        prev_temp = st.session_state.gpu_metrics.get("temperature", 52)

        # Leichte Variation f√ºr realistische Werte
        new_usage = max(0, min(100, prev_usage + random.randint(-3, 3)))
        new_memory = max(1000, min(3000, prev_memory + random.randint(-100, 100)))
        new_temp = max(40, min(80, prev_temp + random.randint(-2, 2)))

        metrics = {
            "usage": new_usage,
            "memory": new_memory,
            "temperature": new_temp,
            "timestamp": datetime.now(),
        }

        st.session_state.gpu_metrics = metrics
        st.session_state.last_gpu_update = current_time

        return metrics
    except Exception as e:
        debug_system.error(f"Fehler beim Abrufen der GPU-Metriken: {e}",
                          component="gpu_monitoring", error=str(e), error_type=type(e).__name__)
        return {
            "usage": 12,
            "memory": 1800,
            "temperature": 52,
            "timestamp": datetime.now(),
            "error": str(e)
        }


# KI Query Funktion
def query_bundeskanzler_ki(
    query: str, language: str = "de", fact_check: bool = True
) -> Dict[str, Any]:
    """F√ºhre eine Query an die Bundeskanzler KI aus mit Caching"""
    cache_key = f"{query}_{language}_{fact_check}"

    # Cache pr√ºfen (f√ºr 5 Minuten)
    if cache_key in st.session_state.api_cache:
        cached_result = st.session_state.api_cache[cache_key]
        if time.time() - cached_result["timestamp"] < 300:  # 5 Minuten
            return cached_result["data"]

    try:
        payload = {"query": query, "language": language, "fact_check": fact_check}

        with st.spinner("ü§ñ Denke nach..."):
            response = requests.post(KI_API_URL, json=payload, timeout=30)

        if response.status_code == 200:
            result = response.json()

            # Erfolgreiche Antwort cachen
            st.session_state.api_cache[cache_key] = {
                "data": result,
                "timestamp": time.time()
            }

            return result
        else:
            error_result = {
                "error": f"API Error: {response.status_code}",
                "response": "Entschuldigung, es gab einen Fehler bei der Verarbeitung Ihrer Anfrage.",
            }

            # Fehlerhafte Antworten nicht cachen
            return error_result

    except requests.exceptions.Timeout:
        return {
            "error": "Timeout",
            "response": "‚è∞ Die Anfrage hat zu lange gedauert. Bitte versuchen Sie es mit einer k√ºrzeren Frage.",
        }
    except requests.exceptions.ConnectionError:
        return {
            "error": "Connection Error",
            "response": "üîå Verbindung zur KI konnte nicht hergestellt werden. Bitte pr√ºfen Sie Ihre Internetverbindung.",
        }
    except Exception as e:
        debug_system.error(f"Unerwarteter Fehler bei KI-Query: {e}",
                          component="web_query", error=str(e), error_type=type(e).__name__)
        return {
            "error": str(e),
            "response": "‚ùå Ein unerwarteter Fehler ist aufgetreten. Bitte versuchen Sie es sp√§ter erneut.",
        }


# Fact-Check Visualisierung
def render_fact_check_visualization(result: Dict[str, Any]):
    """Rendere die Fact-Check Ergebnisse visuell"""
    if not result.get("fact_check"):
        return

    st.markdown("### ‚úÖ Fact-Checking Ergebnisse")

    # Konfidenz-Score
    confidence = result.get("confidence", 0.0)
    if confidence >= 0.8:
        confidence_class = "confidence-high"
        confidence_text = "üéØ Sehr hoch"
    elif confidence >= 0.6:
        confidence_class = "confidence-medium"
        confidence_text = "‚ö†Ô∏è Mittel"
    else:
        confidence_class = "confidence-low"
        confidence_text = "‚ùå Niedrig"

    st.markdown(
        f"""
    <div class="fact-check-indicator {confidence_class}">
        Vertrauensw√ºrdigkeit: {confidence_text} ({confidence:.1f}%)
    </div>
    """,
        unsafe_allow_html=True,
    )

    # Quellen anzeigen
    if result.get("sources"):
        st.markdown("**üìö Verwendete Quellen:**")
        sources_html = ""
        for source in result["sources"]:
            # Quellen k√∂nnen Strings oder Dictionaries sein
            if isinstance(source, str):
                source_name = source
            elif isinstance(source, dict):
                source_name = source.get("name", "Unbekannte Quelle")
            else:
                source_name = str(source)
            sources_html += f'<span class="source-badge">{source_name}</span>'
        st.markdown(sources_html, unsafe_allow_html=True)

    # Detaillierte Quellen-Info
    with st.expander("üìä Detaillierte Quellen-Analyse", expanded=False):
        for i, source in enumerate(result.get("sources", []), 1):
            # Quellen k√∂nnen Strings oder Dictionaries sein
            if isinstance(source, str):
                source_name = source
                source_confidence = "N/A"
                source_url = "Nicht verf√ºgbar"
            elif isinstance(source, dict):
                source_name = source.get("name", "Unbekannte Quelle")
                source_confidence = source.get("confidence", 0)
                source_url = source.get("url", "Nicht verf√ºgbar")
            else:
                source_name = str(source)
                source_confidence = "N/A"
                source_url = "Nicht verf√ºgbar"

            col1, col2, col3 = st.columns([2, 1, 1])
            with col1:
                st.write(f"**{i}. {source_name}**")
            with col2:
                st.write(f"Konfidenz: {source_confidence}")
            with col3:
                if st.button(f"üîó √ñffnen", key=f"source_{i}"):
                    st.write(f"URL: {source_url}")


# Erweiterte Statistiken und Visualisierungen
def render_chat_analytics():
    """Rendere erweiterte Chat-Analytics mit Diagrammen"""
    if not PLOTLY_AVAILABLE or not st.session_state.chat_history:
        return

    st.markdown("### üìà Chat-Analytics")

    # Daten f√ºr Visualisierungen vorbereiten
    messages = st.session_state.chat_history
    timestamps = [msg["timestamp"] for msg in messages]
    roles = [msg["role"] for msg in messages]

    # Zeitliche Verteilung
    if len(messages) > 1:
        col1, col2 = st.columns(2)

        with col1:
            # Nachrichten pro Stunde
            hours = [ts.hour for ts in timestamps]
            hour_counts = {}
            for hour in hours:
                hour_counts[hour] = hour_counts.get(hour, 0) + 1

            fig_hours = go.Figure()
            fig_hours.add_trace(go.Bar(
                x=list(hour_counts.keys()),
                y=list(hour_counts.values()),
                name="Nachrichten",
                marker_color='#007bff'
            ))
            fig_hours.update_layout(
                title="Nachrichten pro Stunde",
                xaxis_title="Stunde",
                yaxis_title="Anzahl",
                height=250
            )
            st.plotly_chart(fig_hours, use_container_width=True)

        with col2:
            # Rollen-Verteilung
            role_counts = {"user": roles.count("user"), "assistant": roles.count("assistant")}

            fig_roles = go.Figure(data=[go.Pie(
                labels=list(role_counts.keys()),
                values=list(role_counts.values()),
                marker_colors=['#007bff', '#28a745']
            )])
            fig_roles.update_layout(
                title="Verteilung User/KI",
                height=250
            )
            st.plotly_chart(fig_roles, use_container_width=True)

    # Wortzahl-Statistiken
    if messages:
        word_counts = []
        for msg in messages:
            content = msg.get("content", "")
            word_count = len(content.split())
            word_counts.append(word_count)

        avg_words_user = sum(word_counts[i] for i, msg in enumerate(messages) if msg["role"] == "user") / max(1, roles.count("user"))
        avg_words_ai = sum(word_counts[i] for i, msg in enumerate(messages) if msg["role"] == "assistant") / max(1, roles.count("assistant"))

        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("√ò W√∂rter (User)", ".1f")
        with col2:
            st.metric("√ò W√∂rter (KI)", ".1f")
        with col3:
            st.metric("Gesamt W√∂rter", sum(word_counts))


# Export-Funktionen
def export_chat_history():
    """Exportiere Chat-Verlauf als JSON oder Text"""
    if not st.session_state.chat_history:
        st.warning("Keine Chat-Historie zum Exportieren verf√ºgbar.")
        return

    col1, col2 = st.columns(2)

    with col1:
        # JSON Export
        chat_json = json.dumps(st.session_state.chat_history, default=str, indent=2, ensure_ascii=False)
        st.download_button(
            label="üìÑ JSON Export",
            data=chat_json,
            file_name=f"chat_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
            mime="application/json",
            use_container_width=True
        )

    with col2:
        # Text Export
        chat_text = "Chat-Verlauf - Bundeskanzler KI\n"
        chat_text += "=" * 50 + "\n\n"

        for msg in st.session_state.chat_history:
            timestamp = msg["timestamp"].strftime("%Y-%m-%d %H:%M:%S")
            role = "Sie" if msg["role"] == "user" else "KI"
            content = msg["content"]
            chat_text += f"[{timestamp}] {role}: {content}\n\n"

# GPU Dashboard
def render_gpu_dashboard():
    """Rendere das GPU-Monitoring Dashboard"""
    st.markdown("### üéÆ RTX 2070 GPU Status")

    # Live GPU-Metriken aktualisieren
    gpu_data = get_gpu_metrics()

    col1, col2, col3 = st.columns(3)

    with col1:
        st.markdown(
            f"""
        <div class="gpu-metric">
            <h4>GPU Auslastung</h4>
            <h2>{gpu_data['usage']}%</h2>
        </div>
        """,
            unsafe_allow_html=True,
        )

    with col2:
        st.markdown(
            f"""
        <div class="gpu-metric">
            <h4>VRAM Verbrauch</h4>
            <h2>{gpu_data['memory']}MB</h2>
        </div>
        """,
            unsafe_allow_html=True,
        )

    with col3:
        st.markdown(
            f"""
        <div class="gpu-metric">
            <h4>Temperatur</h4>
            <h2>{gpu_data['temperature']}¬∞C</h2>
        </div>
        """,
            unsafe_allow_html=True,
        )

    # Fehler anzeigen falls vorhanden
    if gpu_data.get("error"):
        st.warning(f"‚ö†Ô∏è GPU-Monitoring Fehler: {gpu_data['error']}")


# Custom CSS f√ºr moderne UI
def apply_theme(theme: str):
    """Wende Theme-spezifisches CSS an"""
    base_css = """
<style>
    .main-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 2rem;
        border-radius: 15px;
        margin-bottom: 2rem;
        text-align: center;
    }
    .query-card {
        background: white;
        border: 1px solid #e1e5e9;
        border-radius: 10px;
        padding: 1rem;
        margin: 0.5rem 0;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    .user-message {
        background: #007bff;
        color: white;
        border-radius: 15px 15px 15px 0;
        padding: 1rem;
        margin: 0.5rem 0;
        max-width: 80%;
        margin-left: auto;
    }
    .ai-message {
        background: #f8f9fa;
        border: 1px solid #e1e5e9;
        border-radius: 15px 15px 0 15px;
        padding: 1rem;
        margin: 0.5rem 0;
        max-width: 80%;
    }
    .stTextArea textarea {
        transition: border-color 0.2s ease !important;
    }

    /* Mobile Optimierungen */
    @media (max-width: 768px) {
        .main-header {
            padding: 1rem !important;
            margin-bottom: 1rem !important;
        }
        .main-header h1 {
            font-size: 1.5rem !important;
        }
        .chat-message {
            max-width: 95% !important;
            font-size: 0.9rem !important;
        }
        .gpu-metric {
            padding: 0.5rem !important;
            margin: 0.25rem !important;
        }
        .gpu-metric h4 {
            font-size: 0.8rem !important;
        }
        .gpu-metric h2 {
            font-size: 1.2rem !important;
        }
    }

    /* Performance Optimierungen */
    .stButton button {
        transition: all 0.2s ease !important;
    }
    .stTextArea textarea {
        transition: border-color 0.2s ease !important;
    }
"""

    if theme == "dark":
        dark_css = """
    .query-card {
        background: #2d3748 !important;
        border-color: #4a5568 !important;
        color: #e2e8f0 !important;
    }
    .ai-message {
        background: #2d3748 !important;
        border-color: #4a5568 !important;
        color: #e2e8f0 !important;
    }
    [data-testid="stSidebar"] {
        background-color: #1a202c !important;
    }
    [data-testid="stSidebar"] * {
        color: #e2e8f0 !important;
    }
    body {
        color: #e2e8f0 !important;
        background-color: #1a202c !important;
    }
    .stTextArea textarea, .stSelectbox select {
        background-color: #2d3748 !important;
        color: #e2e8f0 !important;
        border-color: #4a5568 !important;
    }
"""
        st.markdown(f"<style>{base_css + dark_css}</style>", unsafe_allow_html=True)
    else:
        st.markdown(f"<style>{base_css}</style>", unsafe_allow_html=True)


# Haupt-Interface
def main():
    """Hauptfunktion der modernen Web-GUI"""

    # Theme anwenden
    apply_theme(st.session_state.theme)

    # Auto-Refresh f√ºr GPU-Metriken falls aktiviert
    if "auto_refresh" in st.session_state and st.session_state.auto_refresh:
        # GPU-Metriken im Hintergrund aktualisieren
        get_gpu_metrics()

    # Header
    st.markdown(
        """
    <div class="main-header">
        <h1>ü§ñ Bundeskanzler KI</h1>
        <p>Intelligente politische Beratung mit RTX 2070 GPU-Beschleunigung</p>
        <p style="font-size: 0.9em; opacity: 0.8;">
            ‚úÖ Fact-Checking | üåç Mehrsprachig | üéØ 100% Erfolgsrate | üìä Analytics
        </p>
    </div>
    """,
        unsafe_allow_html=True,
    )

    # Sidebar mit GPU-Status
    with st.sidebar:
        st.markdown("## üéÆ System Status")
        render_gpu_dashboard()

        st.markdown("---")

        # Spracheinstellungen
        st.markdown("## üåç Sprache")
        language = st.selectbox(
            "Sprache w√§hlen:", ["Deutsch", "English", "Fran√ßais", "Italiano", "Espa√±ol"], index=0
        )

        # Fact-Check Toggle
        st.markdown("---")
        fact_check_enabled = st.checkbox("‚úÖ Fact-Checking aktivieren", value=True)

        # Theme Toggle
        st.markdown("---")
        theme_options = {"üåû Hell": "light", "üåô Dunkel": "dark"}
        current_theme_display = "üåû Hell" if st.session_state.theme == "light" else "üåô Dunkel"
        theme_choice = st.selectbox(
            "Theme w√§hlen:",
            options=list(theme_options.keys()),
            index=list(theme_options.values()).index(st.session_state.theme)
        )

        if theme_options[theme_choice] != st.session_state.theme:
            st.session_state.theme = theme_options[theme_choice]
            st.rerun()

        # Performance-Einstellungen
        st.markdown("---")
        st.markdown("## ‚ö° Performance")

        cache_enabled = st.checkbox("üóÑÔ∏è API-Cache aktivieren", value=True)
        if not cache_enabled:
            st.session_state.api_cache = {}

        auto_refresh = st.checkbox("üîÑ Auto-Refresh GPU-Metriken", value=True)

        # Debug-Informationen
        with st.expander("üêõ Debug-Info", expanded=False):
            st.write(f"**Chat-Nachrichten:** {len(st.session_state.chat_history)}")
            st.write(f"**Cache-Eintr√§ge:** {len(st.session_state.api_cache)}")
            st.write(f"**GPU-Updates:** {st.session_state.last_gpu_update}")
            st.write(f"**Theme:** {st.session_state.theme}")

            if st.button("üîÑ Session zur√ºcksetzen", key="reset_session"):
                for key in list(st.session_state.keys()):
                    del st.session_state[key]
                st.success("üîÑ Session zur√ºckgesetzt!")
                st.rerun()

        # Fehlerdiagnose-Informationen
        render_error_diagnosis()

        # Export-Funktionen
        create_export_section()

        # Chat-Verlauf speichern/laden
        st.markdown("---")
        st.markdown("## üíæ Chat-Verwaltung")

        col1, col2 = st.columns(2)

        with col1:
            if st.button("üíæ Chat speichern", key="save_chat"):
                if st.session_state.chat_history:
                    chat_data = {
                        "timestamp": datetime.now().isoformat(),
                        "messages": len(st.session_state.chat_history),
                        "history": st.session_state.chat_history
                    }
                    st.session_state.saved_chat = chat_data
                    st.success("üíæ Chat-Verlauf gespeichert!")
                else:
                    st.warning("‚ö†Ô∏è Kein Chat-Verlauf zum Speichern")

        with col2:
            if st.button("üìÇ Chat laden", key="load_chat"):
                if "saved_chat" in st.session_state and st.session_state.saved_chat:
                    st.session_state.chat_history = st.session_state.saved_chat["history"]
                    st.success(f"üìÇ Chat-Verlauf geladen! ({st.session_state.saved_chat['messages']} Nachrichten)")
                    st.rerun()
                else:
                    st.warning("‚ö†Ô∏è Kein gespeicherter Chat-Verlauf vorhanden")

        # GPU Performance-Monitoring
        if GPU_OPTIMIZER_AVAILABLE and gpu_optimizer:
            with st.expander("üöÄ GPU Performance", expanded=False):
                try:
                    report = gpu_optimizer.get_performance_report()
                    if report and 'current' in report:
                        col1, col2 = st.columns(2)
                        with col1:
                            st.metric("GPU Memory", report['current']['gpu_memory_used'],
                                    f"{report['current']['gpu_utilization']} genutzt")
                            st.metric("GPU Load", report['current']['gpu_load'])
                        with col2:
                            st.metric("Cache Size", report['cache_stats']['query_cache_size'])
                            st.metric("Temperature", report['current']['temperature'])

                        # Performance-Tipps
                        if report['current']['gpu_utilization'].startswith('8') or report['current']['gpu_utilization'].startswith('9'):
                            st.warning("‚ö†Ô∏è Hohe GPU-Auslastung - Cache wird automatisch bereinigt")
                        else:
                            st.success("‚úÖ Optimale GPU-Performance")
                    else:
                        st.info("üìä Performance-Daten werden gesammelt...")
                except Exception as e:
                    st.error(f"‚ùå Performance-Monitoring Fehler: {e}")
                    debug_system.error(f"GUI Performance-Monitoring Fehler: {e}",
                                     component="gui_performance", error=str(e))

    # Hauptbereich
    col1, col2 = st.columns([2, 1])

    with col1:
        st.markdown("## üí¨ Chat mit der Bundeskanzler KI")

        # Chat-Historie anzeigen
        chat_container = st.container()
        with chat_container:
            for message in st.session_state.chat_history[-10:]:  # Zeige letzte 10 Nachrichten
                if message["role"] == "user":
                    st.markdown(
                        f"""
                    <div class="chat-message user-message">
                        <strong>Sie:</strong> {message['content']}
                    </div>
                    """,
                        unsafe_allow_html=True,
                    )
                else:
                    st.markdown(
                        f"""
                    <div class="chat-message ai-message">
                        <strong>KI:</strong> {message['content']}
                    </div>
                    """,
                        unsafe_allow_html=True,
                    )

                    # Fact-Check f√ºr KI-Antworten anzeigen
                    if message.get("fact_check"):
                        render_fact_check_visualization(message["fact_check"])

        # Query-Eingabe
        with st.form("query_form"):
            # Wert f√ºr das Textfeld vorbereiten
            query_value = (
                st.session_state.current_query
                if st.session_state.current_query
                and not st.session_state.get("query_submitted", False)
                else ""
            )

            query = st.text_area(
                "Stellen Sie Ihre politische Frage:",
                value=query_value,
                height=100,
                placeholder="z.B.: Was ist die aktuelle Klimapolitik Deutschlands?",
                key="query_input",
            )

            col_submit, col_clear = st.columns([1, 1])
            with col_submit:
                submit_button = st.form_submit_button(
                    "üöÄ Frage stellen", type="primary", use_container_width=True
                )
            with col_clear:
                clear_button = st.form_submit_button("üßπ Chat l√∂schen", use_container_width=True)

        # Query verarbeiten
        if submit_button and query.strip():
            # User-Nachricht zur Historie hinzuf√ºgen
            st.session_state.chat_history.append(
                {"role": "user", "content": query, "timestamp": datetime.now()}
            )

            # Query als submitted markieren
            st.session_state.query_submitted = True
            st.session_state.current_query = ""  # Zur√ºcksetzen

            # KI-Query ausf√ºhren
            with st.spinner("ü§ñ Denke nach..."):
                result = query_bundeskanzler_ki(
                    query=query, language=language.lower()[:2], fact_check=fact_check_enabled
                )

            # KI-Antwort zur Historie hinzuf√ºgen
            ai_response = result.get("response", "Entschuldigung, es gab einen Fehler.")
            st.session_state.chat_history.append(
                {
                    "role": "assistant",
                    "content": ai_response,
                    "fact_check": result if fact_check_enabled else None,
                    "timestamp": datetime.now(),
                }
            )

            # Fehlerdiagnose anzeigen falls verf√ºgbar
            if result.get("error_diagnosis") and ERROR_DIAGNOSIS_AVAILABLE:
                diagnosis = result["error_diagnosis"]
                with st.expander("üîç Fehlerdiagnose", expanded=True):
                    st.error(f"**Problem:** {diagnosis.get('diagnosis', 'Unbekannt')}")
                    st.info(f"**Kategorie:** {diagnosis.get('category', 'Unbekannt')}")
                    st.warning(f"**Schweregrad:** {diagnosis.get('severity', 'Unbekannt')}")

                    st.markdown("**üí° L√∂sungsvorschl√§ge:**")
                    for solution in diagnosis.get('solutions', []):
                        st.write(f"‚Ä¢ {solution}")

                    if diagnosis.get('auto_fix_available'):
                        if st.button("üîß Auto-Fix anwenden", key="auto_fix"):
                            try:
                                fixed = error_diagnoser.apply_auto_fix(diagnosis.get('diagnosis', ''))
                                if fixed:
                                    st.success("‚úÖ Auto-Fix erfolgreich angewendet!")
                                    st.rerun()
                                else:
                                    st.error("‚ùå Auto-Fix fehlgeschlagen")
                            except Exception as e:
                                st.error(f"‚ùå Auto-Fix Fehler: {e}")
                    else:
                        st.info("‚ÑπÔ∏è F√ºr dieses Problem ist eine manuelle L√∂sung erforderlich.")

            # Seite neu laden um Chat zu aktualisieren
            st.rerun()

        # Chat l√∂schen
        if clear_button:
            st.session_state.chat_history = []
            st.session_state.fact_check_results = None
            st.session_state.current_query = ""
            st.session_state.query_submitted = False
            st.success("üßπ Chat-Verlauf gel√∂scht!")
            st.rerun()

    with col2:
        st.markdown("## üìä Live-System Status")

        # Performance-Metriken
        if GPU_OPTIMIZER_AVAILABLE and gpu_optimizer:
            try:
                report = gpu_optimizer.get_performance_report()
                if report and 'current' in report:
                    # GPU Status
                    gpu_col1, gpu_col2 = st.columns(2)
                    with gpu_col1:
                        gpu_memory = report['current']['gpu_memory_used']
                        gpu_total = report['current']['gpu_memory_total']
                        gpu_util = report['current']['gpu_utilization']
                        st.metric("üéÆ GPU Memory", f"{gpu_memory}/{gpu_total}", gpu_util)
                    with gpu_col2:
                        gpu_load = report['current']['gpu_load']
                        temperature = report['current']['temperature']
                        st.metric("üî• GPU Load", gpu_load, f"{temperature}¬∞C")

                    # Cache Status
                    cache_size = report['cache_stats']['query_cache_size']
                    st.metric("üóÑÔ∏è Query Cache", f"{cache_size} Eintr√§ge")

                    # Performance Indicator
                    if gpu_util.replace('%', '').startswith(('8', '9')):
                        st.warning("‚ö†Ô∏è Hohe GPU-Auslastung")
                    elif cache_size > 10:
                        st.success("‚úÖ Cache aktiv - optimale Performance")
                    else:
                        st.info("üìà System bereit")
                else:
                    st.info("üìä Sammle Performance-Daten...")
            except Exception as e:
                st.error(f"‚ùå Performance-Monitoring: {str(e)[:50]}...")
        else:
            st.info("üöÄ GPU-Optimierung nicht verf√ºgbar")

        # Chat-Statistiken
        st.markdown("### üí¨ Chat-Statistiken")
        total_messages = len(st.session_state.chat_history)
        user_messages = len([m for m in st.session_state.chat_history if m["role"] == "user"])
        ai_messages = len([m for m in st.session_state.chat_history if m["role"] == "assistant"])

        stat_col1, stat_col2, stat_col3 = st.columns(3)
        with stat_col1:
            st.metric("Gesamt", total_messages)
        with stat_col2:
            st.metric("Fragen", user_messages)
        with stat_col3:
            st.metric("Antworten", ai_messages)

        # API Status
        st.markdown("### üîó API Status")
        try:
            health_response = requests.get(f"{API_BASE_URL}/health", timeout=2)
            if health_response.status_code == 200:
                st.success("‚úÖ API Online")
            else:
                st.error(f"‚ùå API Fehler: {health_response.status_code}")
        except:
            st.error("‚ùå API Offline")

        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Gesamt Nachrichten", total_messages)
        with col2:
            st.metric("Ihre Fragen", user_messages)
        with col3:
            st.metric("KI-Antworten", ai_messages)

        st.markdown("---")

        # Erweiterte Analytics
        if PLOTLY_AVAILABLE and total_messages > 0:
            render_chat_analytics()
        else:
            st.info("üí° F√ºr detaillierte Analytics mehr Nachrichten sammeln oder Plotly installieren.")

        st.markdown("---")

        # Export-Funktionen
        st.markdown("### üíæ Chat Export")
        export_chat_history()

        st.markdown("---")

        # Cache-Statistiken
        cache_size = len(st.session_state.api_cache)
        if cache_size > 0:
            st.markdown(f"### üóÑÔ∏è Cache-Status")
            st.info(f"üìä {cache_size} API-Antworten im Cache (5 Min TTL)")

            if st.button("üßπ Cache leeren", key="clear_cache"):
                st.session_state.api_cache = {}
                st.success("üßπ Cache geleert!")
                st.rerun()

        st.markdown("---")

        # Beispiel-Fragen
        st.markdown("## üí° Beispiel-Fragen")

        example_questions = [
            "Was ist die aktuelle Klimapolitik Deutschlands?",
            "Wie funktioniert die Energiewende?",
            "Was sind die Ziele der Bundesregierung f√ºr 2030?",
            "Erkl√§re die Bedeutung von Nachhaltigkeit in der Politik.",
            "Wie steht Deutschland zur EU?",
            "Was sind die Herausforderungen der Digitalisierung?",
        ]

        for question in example_questions:
            if st.button(question, key=f"example_{hash(question)}", use_container_width=True):
                st.session_state.current_query = question
                st.session_state.query_submitted = False
                st.rerun()

        st.markdown("---")

        # Footer mit Links und System-Info
        st.markdown(
            """
        ### üîó N√ºtzliche Links
        - [üìö Dokumentation](https://github.com/bumblei3/bundeskanzler_ki)
        - [üêõ Bug melden](https://github.com/bumblei3/bundeskanzler_ki/issues)
        - [üí° Feature vorschlagen](https://github.com/bumblei3/bundeskanzler_ki/discussions)

        ---
        **Version 2.2.0** | **RTX 2070 GPU-optimiert** | **Analytics Enhanced**
        """
        )


if __name__ == "__main__":
    main()
