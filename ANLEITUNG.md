# ğŸ¤– Bundeskanzler KI - Multimodale Anleitung (RTX 2070 Edition)

**Version 2.0.0** - Optimiert fÃ¼r NVIDIA RTX 2070 GPUs mit 8GB VRAM

## ğŸš€ **Schnellstart (RTX 2070 optimiert)**

### ğŸ“‹ **System-Anforderungen**
- **GPU**: NVIDIA RTX 2070 (8GB VRAM) oder besser
- **RAM**: 16GB+ empfohlen
- **Python**: 3.12+
- **Speicher**: 50GB+ freier Festplattenspeicher

### âš¡ **Installation & Start**

```bash
# 1. Arbeitsverzeichnis
cd /home/tobber/bkki_venv

# 2. Virtuelle Umgebung aktivieren
source bin/activate

# 3. RTX 2070 optimierte Modelle laden (erster Start dauert ~8 Sekunden)
python -c "from multimodal_ki import MultimodalTransformerModel; model = MultimodalTransformerModel(model_tier='rtx2070')"

# 4. API-Server starten
uvicorn bundeskanzler_api:app --host 0.0.0.0 --port 8001 --reload

# 5. Web-Interface starten (NEUES TERMINAL)
streamlit run webgui_ki.py --server.port 8501 --server.address 0.0.0.0
```

### ğŸŒ **Zugriff**
- **Web-Interface**: http://localhost:8501
- **API-Dokumentation**: http://localhost:8001/docs
- **Admin-Login**: `admin` / `admin123!`

## ğŸ® **RTX 2070 Optimierung verstehen**

Das System ist speziell fÃ¼r Ihre RTX 2070 optimiert:

```python
# Automatische RTX 2070 Optimierung
from multimodal_ki import MultimodalTransformerModel

model = MultimodalTransformerModel(model_tier='rtx2070')
# âœ… 8-bit Quantisierung fÃ¼r alle Modelle
# âœ… GPU-Memory-Management (~774MB Verbrauch)
# âœ… Optimierte Modell-Verteilung
# âœ… Automatische Speicherbereinigung
```

### ğŸ“Š **Performance-Metriken**
- **GPU-Speicher**: 774MB / 8GB verwendet (9.7% Auslastung)
- **Ladezeit**: ~8 Sekunden fÃ¼r alle Modelle
- **Inference**: ~9 Sekunden pro Query
- **Stromverbrauch**: Optimiert fÃ¼r RTX 2070

## ğŸ¨ **Multimodale Modi ausprobieren**

### ğŸ“ **1. Text-Modus (Hauptfunktion)**

```bash
# Im Web-Interface (empfohlen)
# 1. Gehe zu: http://localhost:8501
# 2. Login: admin / admin123!
# 3. WÃ¤hle "Text-Chat" Modus
# 4. Frage stellen: "Was ist die Aufgabe des Bundeskanzlers?"
```

**Curl-Beispiel:**
```bash
curl -X POST "http://localhost:8000/api/chat" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"message": "ErklÃ¤re die Bundesregierung"}'
```

### ğŸ‘ï¸ **2. Bild-Analyse Modus**

```bash
# Web-Interface
# 1. Gehe zu "Multimodal" Tab
# 2. WÃ¤hle Bild hochladen
# 3. Frage: "Analysiere dieses politische Plakat"
```

**API-Beispiel:**
```bash
curl -X POST "http://localhost:8000/api/multimodal" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -F "text=Analysiere dieses Bild" \
  -F "image=@politik_plakat.jpg"
```

### ğŸ¤ **3. Audio-Transkription Modus**

```bash
# Web-Interface
# 1. Gehe zu "Audio" Tab
# 2. Audio-Datei hochladen (.wav, .mp3)
# 3. Automatische Transkription mit Whisper
```

**UnterstÃ¼tzte Formate:**
- WAV, MP3, M4A, FLAC
- Deutsche und englische Sprache
- Bis zu 30 Minuten Audio

### ğŸ”„ **4. Kombinierter Multimodal-Modus**

```python
# Erweiterte Analyse
from multimodal_ki import MultimodalTransformerModel

model = MultimodalTransformerModel(model_tier='rtx2070')

result = model.process_multimodal(
    text="Analysiere diese politische Rede",
    image="rede_plakat.jpg",      # Optional
    audio="rede_audio.wav"        # Optional
)

print(f"Multimodale Analyse: {result}")
```

## ğŸ§  **Kontinuierliches Lernen**

Das System lernt aus jeder Interaktion:

```python
# Feedback geben (im Web-Interface)
# 1. Nach jeder Antwort: Thumbs up/down klicken
# 2. System lernt automatisch aus Bewertungen
# 3. Modell-Verbesserung Ã¼ber Nacht
```

### ğŸ“ˆ **Lern-Features**
- **User-Feedback**: Jede Antwort kann bewertet werden
- **Kontext-Lernen**: System merkt sich GesprÃ¤chskontext
- **Performance-Tracking**: Live-Metriken im Admin-Panel
- **Automatische Optimierung**: Modell verbessert sich kontinuierlich

## ğŸ“Š **Admin-Panel Features**

### ğŸ–¥ï¸ **Dashboard**
- **Live-Metriken**: GPU-Speicher, RAM, CPU
- **System-Status**: Modell-Ladezustand, API-Status
- **Performance-Graphen**: Response-Zeiten, Durchsatz

### ğŸ‘¥ **Benutzer-Management**
- **User-Verwaltung**: HinzufÃ¼gen/LÃ¶schen von Benutzern
- **Berechtigungen**: Rollenbasierte Zugriffe
- **Session-Management**: Aktive Sessions Ã¼berwachen

### ğŸ“‹ **Log-Monitoring**
- **Live-Logs**: Echtzeit-Logging mit Filterung
- **Fehler-Analyse**: Detaillierte Fehlerberichte
- **Performance-Logs**: Langsame Queries identifizieren

### ğŸ’¾ **Memory-Management**
- **GPU-Monitoring**: RTX 2070 Speicher-Ãœberwachung
- **Memory-Optimierung**: Automatische Speicherbereinigung
- **Cache-Management**: TemporÃ¤re Dateien verwalten

## ğŸ”§ **Erweiterte Konfiguration**

### âš™ï¸ **Model-Tiers**

```python
# RTX 2070 optimiert (empfohlen fÃ¼r Ihre Hardware)
model = MultimodalTransformerModel(model_tier='rtx2070')

# Alternative Tiers
model_advanced = MultimodalTransformerModel(model_tier='advanced')  # GrÃ¶ÃŸere Modelle
model_basic = MultimodalTransformerModel(model_tier='basic')       # CPU-Only
model_premium = MultimodalTransformerModel(model_tier='premium')   # GPT-4/Claude APIs
```

### ğŸ”‘ **API-Keys fÃ¼r Premium-Features**

```bash
# Optional fÃ¼r GPT-4/Claude Integration
export OPENAI_API_KEY="sk-your-openai-key"
export ANTHROPIC_API_KEY="sk-ant-your-anthropic-key"

# Dann Premium-Modus verwenden
model = MultimodalTransformerModel(model_tier='premium')
```

### ğŸ›ï¸ **System-Konfiguration**

```yaml
# config.yaml bearbeiten fÃ¼r erweiterte Einstellungen
gpu_memory_limit: 0.9          # 90% GPU-Speicher verwenden
max_concurrent_requests: 5     # Gleichzeitige Anfragen
log_level: INFO                # Logging-Level
cache_ttl: 3600               # Cache-GÃ¼ltigkeit in Sekunden
```

## ğŸ§ª **Tests & Troubleshooting**

### âœ… **System-Tests**

```bash
# VollstÃ¤ndige System-Validierung
python -c "
from multimodal_ki import MultimodalTransformerModel
import torch

print('ğŸ§ª RTX 2070 System-Test...')

# Modell laden
model = MultimodalTransformerModel(model_tier='rtx2070')

# GPU-Check
gpu_mem = torch.cuda.memory_allocated() // 1024**2
print(f'âœ… GPU-Speicher: {gpu_mem}MB verwendet')

# Inference-Test
response = model.process_text('Test')
print(f'âœ… Inference funktioniert: {len(response)} Zeichen')

print('ğŸ‰ RTX 2070 Optimierung erfolgreich!')
"
```

### ğŸ” **Problembehandlung**

#### **GPU-Speicher Fehler**
```bash
# Bei CUDA out-of-memory:
# 1. System neu starten
# 2. Weniger Tabs im Browser offen lassen
# 3. GPU-Speicher in config.yaml reduzieren
gpu_memory_limit: 0.8  # Auf 80% reduzieren
```

#### **Lange Ladezeiten**
```bash
# Modelle werden beim ersten Start heruntergeladen
# Das kann 5-10 Minuten dauern bei langsamer Internetverbindung
# Geduld haben - danach geht es schnell!
```

#### **Audio funktioniert nicht**
```bash
# ZusÃ¤tzliche Audio-AbhÃ¤ngigkeiten installieren
pip install librosa soundfile
# FFmpeg fÃ¼r Audio-Konvertierung sicherstellen
sudo apt-get install ffmpeg
```

## ğŸ³ **Docker-Deployment (RTX 2070)**

```bash
# GPU-Version fÃ¼r RTX 2070 (empfohlen)
docker-compose up -d

# CPU-Fallback (langsamer)
docker-compose -f docker-compose.cpu.yml up -d

# Logs prÃ¼fen
docker-compose logs -f
```

## ğŸ“š **API-Referenz**

### ğŸ” **Authentifizierung**
```bash
# Admin-Token erhalten
curl -X POST "http://localhost:8000/auth/admin-token" \
  -H "Content-Type: application/json" \
  -d '{"username": "admin", "password": "admin123!"}'

# Token fÃ¼r weitere Requests verwenden
TOKEN="your_token_here"
```

### ğŸ¤– **Haupt-Endpunkte**

#### **Text-Chat**
```bash
curl -X POST "http://localhost:8000/api/chat" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"message": "Ihre politische Frage"}'
```

#### **Multimodale Analyse**
```bash
curl -X POST "http://localhost:8000/api/multimodal" \
  -H "Authorization: Bearer $TOKEN" \
  -F "text=Ihre Analyse-Anfrage" \
  -F "image=@bild.jpg" \
  -F "audio=@audio.wav"
```

#### **Feedback geben**
```bash
curl -X POST "http://localhost:8000/api/feedback" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "user_input": "Frage",
    "ai_response": "Antwort",
    "rating": 5,
    "session_id": "session_123"
  }'
```

## ğŸ“ **Support & Hilfe**

### ğŸ†˜ **HÃ¤ufige Probleme**

**Q: System startet nicht?**
A: Virtuelle Umgebung aktivieren: `source bin/activate`

**Q: GPU wird nicht erkannt?**
A: NVIDIA-Treiber prÃ¼fen: `nvidia-smi`

**Q: Zu wenig Speicher?**
A: GPU-Memory-Limit in config.yaml reduzieren

**Q: Audio funktioniert nicht?**
A: FFmpeg installieren: `sudo apt-get install ffmpeg`

### ğŸ“§ **Kontakt**
- **Issues**: [GitHub Issues](https://github.com/bumblei3/bundeskanzler_ki/issues)
- **Dokumentation**: [Wiki](https://github.com/bumblei3/bundeskanzler_ki/wiki)
- **Email**: support@bundeskanzler-ki.de

## ğŸ¯ **NÃ¤chste Schritte**

1. **System starten** und erste Tests durchfÃ¼hren
2. **Multimodale Modi** ausprobieren (Text, Bild, Audio)
3. **Admin-Panel** erkunden und System Ã¼berwachen
4. **Feedback geben** um kontinuierliches Lernen zu aktivieren
5. **Konfiguration anpassen** fÃ¼r Ihre BedÃ¼rfnisse

## ğŸ“ˆ **Performance-Tipps**

- **GPU-Speicher**: ~774MB Basis-Verbrauch - genug fÃ¼r gleichzeitige Sessions
- **RAM-Optimierung**: System verwendet ~7GB RAM bei voller Auslastung
- **Cache**: Automatische Bereinigung verhindert Speicherlecks
- **Updates**: RegelmÃ¤ÃŸige Modell-Updates durch kontinuierliches Lernen

---

**Bundeskanzler KI v2.0.0** - RTX 2070 Edition
**Optimiert fÃ¼r**: NVIDIA RTX 2070 (8GB VRAM)
**Python**: 3.12+
**Letzte Aktualisierung**: September 2025

## ğŸ’¬ Verschiedene Wege zum Ausprobieren

### ğŸ¯ Methode 1: Admin-Panel (Empfohlen!)
```bash
streamlit run webgui_ki.py
```
**Features:**
- ï¿½ **Dashboard**: Live-System-Metriken
- ğŸ‘¥ **Benutzer-Management**: User verwalten
- ï¿½ **Log-Viewer**: Live-Logs mit Filterung
- ğŸ’¾ **Memory-Management**: Statistiken und Verwaltung (neu behoben!)
- âš™ï¸ **Konfiguration**: System-Einstellungen

### ğŸŒ Methode 2: Web-Browser (Swagger UI)
1. API starten (siehe oben)
2. Browser Ã¶ffnen: http://localhost:8000/docs
3. **Features:**
   - ğŸ“– Automatische API-Dokumentation
   - ğŸ§ª Direkte API-Tests im Browser
   - ğŸ” Authentifizierung testen

### ğŸ”§ Methode 3: curl Kommandos
```bash
# Health Check
curl http://localhost:8000/health

# Admin-Login
TOKEN=$(curl -s -X POST "http://localhost:8000/auth/admin-token" \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "username=admin&password=admin123!" | jq -r .access_token)

# Memory-Stats (neu behoben!)
curl -H "Authorization: Bearer $TOKEN" \
  http://localhost:8000/admin/memory/stats

# Chat
curl -X POST "http://localhost:8000/chat" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"message": "Wie steht es um die Klimapolitik?"}'
```

### ğŸ§ª Methode 4: VollstÃ¤ndiger Test
```bash
python direct_test.py
```

## ğŸ’¡ Was du ausprobieren kannst

### ğŸ—£ï¸ Chat-Themen
- **Klimapolitik**: "Wie steht es um den Klimaschutz in Deutschland?"
- **Wirtschaft**: "Wie entwickelt sich die deutsche Wirtschaft?"
- **Digitalisierung**: "Welche DigitalisierungsmaÃŸnahmen plant die Regierung?"
- **Energiewende**: "KÃ¶nnen Sie mir zur Energiewende etwas sagen?"
- **Soziales**: "Was unternimmt die Regierung fÃ¼r soziale Gerechtigkeit?"

### ğŸ§  Memory-System
```bash
# In der Demo:
/memory Deutschland plant bis 2030 eine fÃ¼hrende Rolle in der KI-Entwicklung
/search KI Deutschland
/stats
```

### ğŸ”— Webhooks testen
```bash
curl -X POST "http://localhost:8000/webhook/news_update" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "content": "Neue KI-Strategie angekÃ¼ndigt",
    "source": "Bundespressekonferenz",
    "timestamp": "2025-09-14T12:00:00"
  }'
```

## ğŸ¯ Demo-Beispiele

### Beispiel-Konversation:
```
Sie: Hallo! Wie geht es Deutschland?
ğŸ¤–: Vielen Dank fÃ¼r Ihre Anfrage. Deutschland steht vor wichtigen 
    Herausforderungen und Chancen...

Sie: /memory Deutschland investiert 10 Milliarden in KI-Forschung
âœ… Erinnerung hinzugefÃ¼gt!

Sie: Was plant Deutschland fÃ¼r KÃ¼nstliche Intelligenz?
ğŸ¤–: Deutschland hat eine umfassende KI-Strategie entwickelt...

Sie: /search KI Forschung
ğŸ” Gefunden 1 Ergebnisse:
   1. Deutschland investiert 10 Milliarden in KI-Forschung...
```

## ğŸ” Anmeldedaten

### Admin-Panel (Streamlit)
- **Username**: `admin`
- **Password**: `admin123!`

### API-Zugang (fÃ¼r Entwickler)
- **Username**: `bundeskanzler`
- **Password**: `ki2025`

## ğŸ“Š URLs

### Admin-Panel
- **Streamlit GUI**: http://localhost:8501 (Empfohlen!)
- **HTML Admin-Panel**: http://localhost:8000/admin (Einfach)

### API-Endpunkte
- **API Base**: http://localhost:8000
- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc
- **Health**: http://localhost:8000/health

### Admin-API (erfordert Token)
- **Login**: `POST /auth/admin-token`
- **Memory-Stats**: `GET /admin/memory/stats` âœ… (Neu behoben!)
- **System-Stats**: `GET /admin/system-stats`
- **Logs**: `GET /admin/logs/{type}`
- **Users**: `GET /admin/users`

## ğŸš¨ ProblemlÃ¶sung

### API startet nicht?
```bash
# PrÃ¼fe ob Port belegt ist
lsof -i :8000

# Stoppe andere uvicorn Prozesse
pkill -f uvicorn

# Starte neu
python -m uvicorn bundeskanzler_api:app --host 0.0.0.0 --port 8000 --reload
```

### Demo verbindet nicht?
- âœ… Stelle sicher, dass die API lÃ¤uft (siehe oben)
- âœ… Warte 5-10 Sekunden nach API-Start
- âœ… PrÃ¼fe dass kein Firewall die Verbindung blockiert

### Authentifizierung schlÃ¤gt fehl?
- âœ… Verwende exakt: `bundeskanzler` / `ki2025`
- âœ… PrÃ¼fe dass die API vollstÃ¤ndig gestartet ist

## ğŸ‰ Viel SpaÃŸ beim Ausprobieren!

Die Bundeskanzler KI ist bereit fÃ¼r deine Fragen! ğŸ¤–