#!/usr/bin/env python3
"""
Test-Suite fÃ¼r Dynamic Batching System
Testet RTX 2070 optimierte Request-Batching FunktionalitÃ¤t
"""

import asyncio
import logging
import time
from typing import List

logger = logging.getLogger(__name__)


async def test_batch_config():
    """Testet Batch-Konfiguration"""
    try:
        from core.dynamic_batching import DynamicBatchProcessor, BatchConfig

        logger.info("âš™ï¸ Teste Batch-Konfiguration...")

        # Teste RTX 2070 spezifische Konfiguration
        processor = DynamicBatchProcessor(gpu_memory_gb=8.0)

        assert processor.config.max_batch_size == 8, f"RTX 2070 sollte max_batch_size=8 haben, bekam {processor.config.max_batch_size}"
        assert processor.config.max_wait_time == 0.05, f"RTX 2070 sollte max_wait_time=0.05 haben, bekam {processor.config.max_wait_time}"
        assert processor.config.gpu_memory_threshold == 0.85, f"RTX 2070 sollte gpu_memory_threshold=0.85 haben, bekam {processor.config.gpu_memory_threshold}"

        # Teste kleinere GPU
        processor_small = DynamicBatchProcessor(gpu_memory_gb=4.0)
        assert processor_small.config.max_batch_size == 4, f"Kleine GPU sollte max_batch_size=4 haben, bekam {processor_small.config.max_batch_size}"

        logger.info("âœ… Batch-Konfiguration funktioniert")

        return True

    except Exception as e:
        logger.error(f"âŒ Batch-Konfiguration Test fehlgeschlagen: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_request_batching():
    """Testet Request-Batching FunktionalitÃ¤t"""
    try:
        from core.dynamic_batching import DynamicBatchProcessor, BatchRequest

        logger.info("ğŸ“¦ Teste Request-Batching...")

        processor = DynamicBatchProcessor(gpu_memory_gb=8.0)

        # Sammle Ergebnisse
        results = []

        async def collect_result(result):
            results.append(result)

        # Submitte mehrere Requests
        requests = []
        for i in range(5):
            req = BatchRequest(
                id=f"req_{i}",
                data=f"data_{i}",
                callback=collect_result
            )
            batch_id = await processor.submit_request(req)
            requests.append(req)

        # Warte auf Verarbeitung
        await asyncio.sleep(0.2)

        # Debug: PrÃ¼fe aktuellen Status
        metrics = processor.get_batch_metrics()
        logger.info(f"Debug - Pending: {metrics['pending_requests']}, Processing: {metrics['processing_batches']}, Results: {len(results)}")

        # PrÃ¼fe Ergebnisse
        print(f"Bekam {len(results)} Ergebnisse: {results}")
        assert len(results) == 5, f"Erwartet 5 Ergebnisse, bekam {len(results)}"

        # PrÃ¼fe Metriken
        metrics = processor.get_batch_metrics()
        assert metrics["total_requests"] == 5, f"Erwartet 5 total_requests, bekam {metrics['total_requests']}"
        assert metrics["total_batches"] >= 1, f"Erwartet mindestens 1 Batch, bekam {metrics['total_batches']}"

        logger.info("âœ… Request-Batching funktioniert")

        return True

    except Exception as e:
        logger.error(f"âŒ Request-Batching Test fehlgeschlagen: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_batch_performance():
    """Testet Batch-Performance und Metriken"""
    try:
        from core.dynamic_batching import DynamicBatchProcessor, BatchRequest

        logger.info("ğŸ“Š Teste Batch-Performance...")

        processor = DynamicBatchProcessor(gpu_memory_gb=8.0)

        # Sammle viele Requests
        num_requests = 20
        results = []

        async def collect_result(result):
            results.append(result)

        start_time = time.time()

        # Submitte Requests in schneller Folge
        for i in range(num_requests):
            req = BatchRequest(
                id=f"perf_req_{i}",
                data=f"perf_data_{i}",
                callback=collect_result
            )
            await processor.submit_request(req)

        # Warte auf vollstÃ¤ndige Verarbeitung
        await asyncio.sleep(0.5)

        total_time = time.time() - start_time

        # PrÃ¼fe, dass alle Requests verarbeitet wurden
        assert len(results) == num_requests, f"Erwartet {num_requests} Ergebnisse, bekam {len(results)}"

        # PrÃ¼fe Performance-Metriken
        metrics = processor.get_batch_metrics()
        assert metrics["total_requests"] == num_requests
        assert metrics["avg_batch_size"] > 0, "Durchschnittliche Batch-GrÃ¶ÃŸe sollte > 0 sein"
        assert metrics["throughput_requests_per_sec"] > 0, "Throughput sollte > 0 sein"

        logger.info(f"âœ… Batch-Performance: {metrics['throughput_requests_per_sec']:.1f} req/s, avg batch size: {metrics['avg_batch_size']:.1f}")

        return True

    except Exception as e:
        logger.error(f"âŒ Batch-Performance Test fehlgeschlagen: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_adaptive_batching():
    """Testet adaptives Batching basierend auf GPU-Speicher"""
    try:
        from core.dynamic_batching import DynamicBatchProcessor

        logger.info("ğŸ›ï¸ Teste adaptives Batching...")

        # Teste verschiedene GPU-GrÃ¶ÃŸen
        gpu_configs = [
            (4.0, 4),   # Kleine GPU
            (8.0, 8),   # RTX 2070
            (12.0, 16), # RTX 3080
            (24.0, 16), # GroÃŸe GPU
        ]

        for gpu_gb, expected_batch_size in gpu_configs:
            processor = DynamicBatchProcessor(gpu_memory_gb=gpu_gb)
            assert processor.config.max_batch_size == expected_batch_size, \
                f"GPU {gpu_gb}GB sollte max_batch_size={expected_batch_size} haben, bekam {processor.config.max_batch_size}"

            logger.info(f"âœ… GPU {gpu_gb}GB: max_batch_size={processor.config.max_batch_size}")

        logger.info("âœ… Adaptives Batching funktioniert")

        return True

    except Exception as e:
        logger.error(f"âŒ Adaptives Batching Test fehlgeschlagen: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """Hauptfunktion fÃ¼r Dynamic Batching Tests"""
    print("ğŸ¯ Dynamic Batching Test Suite")
    print("=" * 50)

    tests = [
        ("Batch Config", test_batch_config),
        ("Request Batching", test_request_batching),
        ("Batch Performance", test_batch_performance),
        ("Adaptive Batching", test_adaptive_batching),
    ]

    results = []

    for test_name, test_func in tests:
        try:
            print(f"\n{'='*20} {test_name} {'='*20}")
            success = await test_func()
            results.append((test_name, success))
        except Exception as e:
            print(f"âŒ Test '{test_name}' fehlgeschlagen: {e}")
            results.append((test_name, False))

    # Zusammenfassung
    print("\n" + "="*60)
    print("ğŸ“‹ TEST-ZUSAMMENFASSUNG")
    print("="*60)

    passed = 0
    total = len(results)

    for test_name, success in results:
        status = "âœ…" if success else "âŒ"
        print(f"{status} {test_name}")
        if success:
            passed += 1

    print(f"\nğŸ“Š Ergebnis: {passed}/{total} Tests erfolgreich")

    if passed == total:
        print("ğŸ‰ Alle Dynamic Batching Tests erfolgreich!")
        print("âœ… RTX 2070 optimierte Batch-Verarbeitung aktiv")
        print("âœ… Adaptive GPU-Auslastung funktioniert")
        print("âœ… Performance-Monitoring bereit")
    elif passed >= total * 0.8:
        print("ğŸ‘ Meisten Tests erfolgreich. Dynamic Batching teilweise aktiv.")
    else:
        print("âš ï¸ Einige Tests fehlgeschlagen. Fallback auf sequentielle Verarbeitung.")

    print("\nğŸ’¡ Dynamic Batching Features:")
    print("   â€¢ RTX 2070 spezifische Batch-GrÃ¶ÃŸenoptimierung")
    print("   â€¢ Adaptive Batch-Verarbeitung basierend auf GPU-Speicher")
    print("   â€¢ Performance-Monitoring und Metriken")
    print("   â€¢ Asynchrone Request-Verarbeitung")
    print("   â€¢ Memory-optimierte GPU-Nutzung")


if __name__ == "__main__":
    asyncio.run(main())