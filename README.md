
# ğŸ¤– Bundeskanzler KI - Multimodale KI mit RTX 2070 Optimierung

Eine fortschrittliche multimodale KI fÃ¼r politische Fragen und Beratung, optimiert fÃ¼r RTX 2070 GPUs. UnterstÃ¼tzt Text, Bilder, Audio und Video mit kontinuierlichem Lernen und professionellem Admin-Panel.

[![Python](https://img.shields.io/badge/Python-3.12+-blue.svg)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.8+-red.svg)](https://pytorch.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## ğŸ¯ ProjektÃ¼bersicht

Die Bundeskanzler KI ist ein hochmodernes multimodales KI-System fÃ¼r politische Fragen, Beratung und Analyse. Sie kombiniert fortschrittliche KI-Modelle mit GPU-Optimierung fÃ¼r RTX 2070 Hardware, kontinuierliches Lernen und professionelle Verwaltungstools.

### ğŸš€ **Kernfeatures (2025)**

#### ğŸ¤– **Multimodale KI-FÃ¤higkeiten**
- ğŸ“ **Text-Verarbeitung**: GPT-2 Medium mit 8-bit Quantisierung
- ğŸ‘ï¸ **Bild-Analyse**: SigLIP Vision-Modelle fÃ¼r BildverstÃ¤ndnis
- ğŸ¤ **Audio-Verarbeitung**: Whisper Base fÃ¼r Spracherkennung
- ğŸ¥ **Video-Support**: Integrierte Video-Frame-Analyse (zukÃ¼nftig)

#### ğŸ® **RTX 2070 GPU-Optimierung**
- âš¡ **8-bit Quantisierung**: BitsAndBytes fÃ¼r optimale Speichereffizienz
- ğŸ§  **GPU-Memory-Management**: Automatische Speicherbereinigung
- ğŸš€ **Device Mapping**: Optimierte Modell-Verteilung auf GPU
- ğŸ“Š **Speicher-Monitoring**: Live GPU-Speicher-Ãœberwachung

#### ğŸ§  **Kontinuierliches Lernen**
- ğŸ“ˆ **Feedback-Schleifen**: Automatische Modell-Verbesserung
- ï¿½ **Memory-Netzwerke**: Hierarchische GedÃ¤chtnis-Systeme
- ğŸ”„ **Adaptive Responses**: KontextabhÃ¤ngige Antwort-Optimierung
- ğŸ“Š **Performance-Tracking**: Detaillierte Metriken und Analysen

#### ğŸ” **Sicherheit & Verwaltung**
- ğŸ” **Erweiterte Sicherheit**: API-Key-Management und Content-Filtering
- ğŸ‘¥ **Benutzer-Management**: Rollenbasierte Zugriffskontrolle
- ğŸ“Š **Admin-Panel**: Live-Monitoring und Systemstatistiken
- ğŸ“‹ **Log-Management**: Strukturierte Logging-Systeme
- ğŸ” **Debug-System**: Erweiterte Fehlerbehebung mit API-Call-Tracking und Live-Debugging

## ğŸ—ï¸ **System-Architektur**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web-Interface â”‚    â”‚     FastAPI     â”‚    â”‚   Admin-Panel   â”‚
â”‚    (Streamlit)  â”‚â—„â”€â”€â–ºâ”‚      API        â”‚â—„â”€â”€â–ºâ”‚  (Monitoring)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Multimodale KI     â”‚
                    â”‚  (RTX 2070 opt.)    â”‚
                    â”‚                     â”‚
                    â”‚ â€¢ GPT-2 Medium      â”‚
                    â”‚ â€¢ SigLIP Vision     â”‚
                    â”‚ â€¢ Whisper Audio     â”‚
                    â”‚ â€¢ Memory Networks   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ **Schnellstart**

### ğŸ“‹ **Voraussetzungen**
- **Python**: 3.12+
- **GPU**: NVIDIA RTX 2070 (8GB VRAM) oder besser
- **RAM**: 16GB+ empfohlen
- **Speicher**: 50GB+ freier Festplattenspeicher

### âš¡ **Installation & Start**

```bash
# 1. Virtuelle Umgebung aktivieren
cd /home/tobber/bkki_venv
source bin/activate

# 2. AbhÃ¤ngigkeiten installieren (falls nicht vorhanden)
pip install -r requirements.txt

# 3. RTX 2070 optimierte Modelle laden
python -c "from multimodal_ki import MultimodalTransformerModel; model = MultimodalTransformerModel(model_tier='rtx2070')"

# 4. API-Server starten
uvicorn bundeskanzler_api:app --host 0.0.0.0 --port 8001 --reload

# 5. Web-Interface starten (neues Terminal)
streamlit run webgui_ki.py --server.port 8501 --server.address 0.0.0.0
```

### ï¿½ **Zugriff auf die Anwendung**

- **Web-Interface**: http://localhost:8501
- **API-Dokumentation**: http://localhost:8001/docs
- **Admin-Login**: `admin` / `admin123!`

## ğŸ® **RTX 2070 Optimierung**

Das System ist speziell fÃ¼r RTX 2070 GPUs optimiert:

```python
# Automatische RTX 2070 Optimierung
from multimodal_ki import MultimodalTransformerModel

model = MultimodalTransformerModel(model_tier='rtx2070')
# â€¢ 8-bit Quantisierung fÃ¼r alle Modelle
# â€¢ GPU-Memory-Management
# â€¢ Optimierte Modell-Verteilung
# â€¢ ~774MB GPU-Speicher Verbrauch
```

### ğŸ“Š **Performance-Metriken**
- **GPU-Speicher**: 774MB / 8GB verwendet (9.7% Auslastung)
- **Ladezeit**: ~8 Sekunden fÃ¼r alle Modelle
- **Inference**: ~9 Sekunden pro Query
- **Speicher-Effizienz**: 75% weniger RAM-Verbrauch

## ğŸ¨ **Multimodale Modi**

### ğŸ“ **Text-Modus**
```python
from multimodal_ki import MultimodalTransformerModel

model = MultimodalTransformerModel()
response = model.process_text("Was ist die Aufgabe des Bundeskanzlers?")
print(response)
```

### ï¿½ï¸ **Bild-Modus**
```python
# Bild hochladen und analysieren
image_path = "politik_bild.jpg"
analysis = model.process_image(image_path)
print(f"Bild-Analyse: {analysis}")
```

### ğŸ¤ **Audio-Modus**
```python
# Audio-Datei transkribieren
audio_path = "rede.wav"
transcription = model.process_audio(audio_path)
print(f"Transkription: {transcription}")
```

### ğŸ”„ **Multimodal Kombination**
```python
# Kombinierte Analyse
result = model.process_multimodal(
    text="Analysiere diese politische Rede",
    image="rede_bild.jpg",
    audio="rede_audio.wav"
)
```

## ğŸ§  **Kontinuierliches Lernen**

Das System lernt kontinuierlich aus User-Interaktionen:

```python
from continuous_learning import ContinuousLearningSystem

learning_system = ContinuousLearningSystem()
learning_system.add_feedback(user_input="Frage", ai_response="Antwort", rating=5)
learning_system.update_model()  # Automatische Modell-Verbesserung
```

### ğŸ“ˆ **Lern-Features**
- **Feedback-Schleifen**: User-Ratings fÃ¼r Antwort-Verbesserung
- **Kontext-Lernen**: SituationsabhÃ¤ngige Antwort-Optimierung
- **Memory-Netzwerke**: LangzeitgedÃ¤chtnis fÃ¼r Konversationen
- **Performance-Tracking**: Detaillierte Analysen und Metriken

## ğŸ”§ **Konfiguration**

### âš™ï¸ **Model-Tiers**
```python
# RTX 2070 optimiert (empfohlen)
model = MultimodalTransformerModel(model_tier='rtx2070')

# Alternative Tiers
model_advanced = MultimodalTransformerModel(model_tier='advanced')  # GrÃ¶ÃŸere Modelle
model_basic = MultimodalTransformerModel(model_tier='basic')       # CPU-Only
model_premium = MultimodalTransformerModel(model_tier='premium')   # API-Modelle
```

### ğŸ”‘ **API-Keys (Optional)**
```bash
# FÃ¼r Premium-Features
export OPENAI_API_KEY="your-openai-key"
export ANTHROPIC_API_KEY="your-anthropic-key"
```

## ğŸ“Š **Monitoring & Admin**

### ğŸ–¥ï¸ **Admin-Panel Features**
- **Live-Dashboard**: System-Metriken und Performance
- **GPU-Monitoring**: Speicher- und Temperatur-Ãœberwachung
- **Log-Viewer**: Strukturierte Logs mit Filterung
- **Memory-Management**: Detaillierte Speicher-Statistiken
- **User-Management**: Benutzer- und Berechtigungs-Verwaltung

### ğŸ“ˆ **System-Metriken**
- **GPU-Auslastung**: Live RTX 2070 Monitoring
- **Memory-Usage**: RAM und GPU-Speicher Tracking
- **API-Performance**: Response-Zeiten und Durchsatz
- **Model-Accuracy**: Antwort-QualitÃ¤t und Feedback-Ratings

## ğŸ§ª **Tests & Validierung**

```bash
# VollstÃ¤ndige System-Tests
python -c "
from multimodal_ki import MultimodalTransformerModel
import torch

# RTX 2070 Test
model = MultimodalTransformerModel(model_tier='rtx2070')
print(f'GPU Memory: {torch.cuda.memory_allocated() // 1024**2}MB')
print('âœ… RTX 2070 Optimierung funktioniert!')
"

# Unit-Tests
python -m pytest tests/ -v

# Integration-Tests
python test_integration.py
```

## ğŸ³ **Docker-Deployment**

```bash
# GPU-Version (empfohlen fÃ¼r RTX 2070)
docker-compose up -d

# CPU-Version (Fallback)
docker-compose -f docker-compose.cpu.yml up -d
```

## ğŸ“š **API-Referenz**

### ğŸ” **Authentifizierung**
```bash
# Admin-Token erhalten
curl -X POST "http://localhost:8000/auth/admin-token" \
  -H "Content-Type: application/json" \
  -d '{"username": "admin", "password": "admin123!"}'
```

### ğŸ¤– **KI-Endpunkte**
```bash
# Text-Verarbeitung
curl -X POST "http://localhost:8000/api/chat" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"message": "Ihre Frage hier"}'

# Multimodale Analyse
curl -X POST "http://localhost:8000/api/multimodal" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -F "text=Ihre Frage" \
  -F "image=@bild.jpg"
```

## ğŸ”§ **Entwicklung**

### ğŸ“¦ **AbhÃ¤ngigkeiten**
```
torch>=2.8.0
transformers>=4.56.0
fastapi>=0.116.0
uvicorn>=0.35.0
streamlit>=1.47.0
numpy>=2.3.0
pandas>=2.3.0
scikit-learn>=1.7.0
accelerate>=1.4.0
datasets>=3.2.0
anthropic>=0.30.0
openai>=1.40.0
psutil>=6.1.0
SQLAlchemy>=2.0.0
redis>=5.2.0
pytest>=8.4.0
```

### ğŸ—ï¸ **Projekt-Struktur**
```
bundeskanzler_ki/
â”œâ”€â”€ multimodal_ki.py          # Haupt-KI-Modul (RTX 2070 opt.)
â”œâ”€â”€ bundeskanzler_ki.py       # Legacy-KI-System
â”œâ”€â”€ continuous_learning.py    # Kontinuierliches Lernen
â”œâ”€â”€ advanced_security.py      # Sicherheit & Authentifizierung
â”œâ”€â”€ bundeskanzler_api.py      # FastAPI-Backend
â”œâ”€â”€ webgui_ki.py             # Streamlit Web-Interface
â”œâ”€â”€ admin_panel_server.py    # Admin-Panel Server
â””â”€â”€ tests/                    # Test-Suite
```

## ğŸ¤ **Beitragen**

1. Fork das Repository
2. Erstelle einen Feature-Branch (`git checkout -b feature/AmazingFeature`)
3. Commit deine Ã„nderungen (`git commit -m 'Add some AmazingFeature'`)
4. Push zum Branch (`git push origin feature/AmazingFeature`)
5. Ã–ffne einen Pull Request

## ğŸ“„ **Lizenz**

Dieses Projekt ist unter der MIT-Lizenz lizenziert - siehe die [LICENSE](LICENSE) Datei fÃ¼r Details.

## ğŸ™ **Danksagungen**

- **PyTorch Team** fÃ¼r die GPU-Optimierung
- **Hugging Face** fÃ¼r die Transformer-Modelle
- **FastAPI** fÃ¼r das hochperformante Backend
- **Streamlit** fÃ¼r das intuitive Web-Interface

## ğŸ“ **Support**

Bei Fragen oder Problemen:
- ğŸ“§ **Email**: support@bundeskanzler-ki.de
- ğŸ› **Issues**: [GitHub Issues](https://github.com/bumblei3/bundeskanzler_ki/issues)
- ğŸ“– **Dokumentation**: [Wiki](https://github.com/bumblei3/bundeskanzler_ki/wiki)

---

**Letzte Aktualisierung**: 15. September 2025
**Version**: 2.1.0 (RTX 2070 Edition mit Debug-System)
**Python**: 3.12+
**GPU**: NVIDIA RTX 2070 (8GB VRAM)

```bash
# HTML-Admin-Panel (einfach)
# URL: http://localhost:8000/admin

# CLI-Admin-Tool
python3 admin_cli.py
```

### API-Endpunkte

```bash
# Health-Check
GET /health

# Chat mit KI
POST /chat
Content-Type: application/json
{
  "message": "Wie steht es um die Klimapolitik?",
  "user_id": "optional_user_id"
}

# Admin-Login
POST /auth/admin-token
Content-Type: application/x-www-form-urlencoded
{
  "username": "admin",
  "password": "admin123!"
}

# Admin Memory-Stats (behoben!)
GET /admin/memory/stats
Authorization: Bearer <admin_token>
Response: {
  "kurzzeitgedaechtnis_entries": 0,
  "langzeitgedaechtnis_entries": 0,
  "total_entries": 0,
  "memory_efficiency": 0.0,
  "status": "success"
}

# Weitere Admin-Endpunkte
GET /admin/system-stats          # System-Metriken
GET /admin/health               # Health-Check
GET /admin/logs/{type}          # Live-Logs
GET /admin/users                # Benutzer-Management
POST /admin/memory/clear        # Memory leeren
GET /admin/config               # Konfiguration
PUT /admin/config               # Konfiguration aktualisieren
```

### Web-Interface

```bash
# Streamlit Admin-Panel (Empfohlen)
streamlit run webgui_ki.py
URL: http://localhost:8501

# HTML Admin-Panel (Einfach)
URL: http://localhost:8000/admin

# API Dokumentation
Swagger UI: http://localhost:8000/docs
ReDoc: http://localhost:8000/redoc
```

## ğŸ§ª Testen

```bash
# VollstÃ¤ndige Test-Suite
python comprehensive_test.py

# Einzelne Komponenten testen
python test_gpu_batching.py
python test_memory_optimization.py
python test_admin_panel.py

# API-Tests
pytest tests/ -v
```

## ğŸ—ï¸ Architektur

### GPU-Batching-System ğŸš€
- **CUDA/ROCm Support**: Automatische Hardware-Erkennung
- **Async Processing**: Parallele Batch-Verarbeitung
- **Performance**: 47.730 Embeddings/Sekunde
- **Fallback**: CPU-Modus bei fehlender GPU

### Optimiertes Memory-System ğŸ§ 
- **Quantisierung**: int8/float16 fÃ¼r 75% Speicherersparnis
- **LRU-Caching**: Automatische Cache-Verwaltung
- **Memory Pooling**: Effiziente Wiederverwendung von Arrays
- **Hierarchisches Design**: Kurz-/LangzeitgedÃ¤chtnis

### Admin-Panel ğŸ“Š
- **Live-Monitoring**: Echtzeit-Systemstatistiken
- **Memory-Insights**: Detaillierte Speichernutzung (neu behoben!)
- **GPU-Monitoring**: Batch-Performance und Hardware-Status
- **Log-Viewer**: Strukturierte Live-Logs mit Filterung
- **Benutzer-Management**: VollstÃ¤ndige User-Verwaltung
- **Konfiguration**: Runtime-Systemeinstellungen
- **JWT-Sicherheit**: GeschÃ¼tzte Admin-Bereiche

### Memory-System ğŸ§  (Neu optimiert!)
- **Quantisierung**: int8/float16 fÃ¼r 75% Speicherersparnis
- **LRU-Caching**: Automatische Cache-Verwaltung
- **Memory Pooling**: Effiziente Wiederverwendung von Arrays
- **Hierarchisches Design**: Kurz-/LangzeitgedÃ¤chtnis
- **Fallback-Initialisierung**: Automatische Reparatur bei Fehlern
- **Live-Statistiken**: Detaillierte Memory-Metriken

## ğŸ“¦ Technische Details

### AbhÃ¤ngigkeiten
```txt
fastapi>=0.116.0
uvicorn>=0.35.0
pydantic>=2.11.0
python-jose[cryptography]>=3.5.0
numpy>=2.3.0
pandas>=2.3.0
scikit-learn>=1.7.0
torch>=2.8.0
transformers>=4.56.0
accelerate>=1.4.0
datasets>=3.2.0
streamlit>=1.47.0
SQLAlchemy>=2.0.0
psycopg2-binary>=2.9.0
redis>=5.2.0
pytest>=8.4.0
psutil>=6.1.0
```

### Systemanforderungen
- **RAM**: 4GB+ fÃ¼r optimale Performance
- **GPU**: NVIDIA/AMD GPU mit CUDA/ROCm (empfohlen)
- **Speicher**: 2GB+ freier Festplattenspeicher

## ï¿½ Konfiguration

### Umgebungsvariablen
```bash
export BUNDESKANZLER_SECRET_KEY="your-secret-key"
export CUDA_VISIBLE_DEVICES="0"  # GPU-Device
```

### Memory-Konfiguration
```python
memory_system = OptimizedHierarchicalMemory(
    short_term_capacity=200,
    long_term_capacity=5000,
    embedding_dim=512,
    enable_quantization=True,
    enable_caching=True,
    cache_size=1000,
    memory_pool_size=2000
)
```

### GPU-Konfiguration
```python
gpu_processor = GPUBatchProcessor(
    batch_size=16,
    max_workers=4,
    device="auto",  # cuda/rocm/cpu/auto
    embedding_dim=512,
    enable_async=True
)
```

## ğŸ“Š Performance

| Komponente | Performance | Optimierung |
|------------|-------------|-------------|
| GPU-Batching | 47.730 Emb/s | CUDA/ROCm |
| Memory | 75% weniger RAM | Quantisierung |
| API | <100ms Response | FastAPI |
| Admin-Panel | Live-Updates | WebSocket |

## ğŸ¨ Admin-Interface

Das Admin-Panel bietet vollstÃ¤ndige Systemkontrolle:

- **Dashboard**: Ãœbersicht aller Systemmetriken
- **Memory-Monitoring**: Live-Speicherstatistiken
- **GPU-Status**: Batch-Performance und Hardware-Info
- **Log-Viewer**: Strukturierte Logs mit Filterung
- **Konfiguration**: Runtime-Einstellungen anpassen

## ğŸ” Sicherheit

- **JWT-Authentifizierung**: Sichere Token-basierte Authentifizierung
- **Rate Limiting**: 60 Requests/Minute pro Client
- **CORS-Konfiguration**: EingeschrÃ¤nkte Origin-Kontrolle
- **Input-Validation**: VollstÃ¤ndige Pydantic-Validierung

## ğŸ§ª Testabdeckung

```bash
# Test-Ãœbersicht
pytest --cov=bundeskanzler_api --cov=optimized_memory --cov=gpu_batching

# Performance-Tests
python -m pytest tests/ -k "performance" --tb=short

# Integration-Tests
python comprehensive_test.py
```

## ğŸ“‹ Roadmap

- [x] GPU-Batching Implementation
- [x] Memory-Optimierung (Quantisierung)
- [x] Admin-Panel mit Live-Monitoring
- [x] JWT-Authentifizierung
- [x] **Memory-Stats-API behoben** (Fallback-Initialisierung)
- [x] Streamlit Web-Interface
- [x] Log-Monitoring System
- [x] Benutzer-Management
- [x] **Debug-System Integration** (Live-Debugging in Web-GUI)
- [x] **Erweiterte API-Call-Tracking**
- [ ] Mehrsprachige UnterstÃ¼tzung
- [ ] FaktenprÃ¼fung & Quellenvalidierung
- [ ] Erweiterte Admin-Analytics
- [ ] Kubernetes-Deployment

## ğŸ¤ Beitragen

BeitrÃ¤ge sind willkommen! Bitte:

1. Fork das Repository
2. Erstelle einen Feature-Branch
3. FÃ¼ge Tests fÃ¼r neue Features hinzu
4. Stelle einen Pull Request

### Entwicklungsrichtlinien
- **Code-Style**: Black + isort
- **Tests**: 100% Coverage fÃ¼r neue Features
- **Dokumentation**: Docstrings fÃ¼r alle Ã¶ffentlichen APIs
- **Performance**: Benchmarks fÃ¼r Performance-Ã¤ndernde Ã„nderungen

## ğŸ“ Lizenz

MIT License - siehe [LICENSE](LICENSE) fÃ¼r Details.

## ğŸ™ Danksagung

Danke an die Open-Source-Community fÃ¼r die groÃŸartigen Tools und Libraries!

## ğŸ“¬ Kontakt

- **Issues**: [GitHub Issues](https://github.com/bumblei3/bundeskanzler_ki/issues)
- **Discussions**: FÃ¼r Fragen und Feedback

---

**ğŸš€ Das System ist vollstÃ¤ndig funktionsfÃ¤hig und bereit fÃ¼r den Produktiveinsatz!**
