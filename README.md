# Bundeskanzler KI

Ein KI-gestÃ¼tztes System zur Verarbeitung und Beantwortung von Fragen zu deutschen Regierungspositionen und politischen Themen.

## ğŸ¯ ProjektÃ¼bersicht

Die Bundeskanzler KI ist ein intelligentes Modell, das auf Basis von Deep Learning politische Fragen und Themen verarbeitet. Das System nutzt eine optimierte LSTM-Architektur mit fortgeschrittenen Regularisierungstechniken, um prÃ¤zise und kontextrelevante Antworten zu generieren.

## ğŸš€ Features

- **Intelligente Textverarbeitung**: Verarbeitung natÃ¼rlicher Sprache fÃ¼r politische Themen
- **Kontextbewusstsein**: BerÃ¼cksichtigung des GesprÃ¤chskontexts bei Antworten
- **Regularisierte Architektur**: Optimierte Modellarchitektur gegen Overfitting
- **Konfigurierbare Pipeline**: Flexible Anpassung von Trainingsparametern
- **Deutschsprachige Basis**: Speziell fÃ¼r den deutschen politischen Kontext optimiert

## ğŸ›  Installation

1. **Python-Umgebung erstellen**:
   ```bash
   python -m venv bkki_venv
   source bkki_venv/bin/activate  # Linux/Mac
   # oder
   .\bkki_venv\Scripts\activate   # Windows
   ```

2. **AbhÃ¤ngigkeiten installieren**:
   ```bash
   pip install -r requirements.txt
   ```

## ğŸ’» Verwendung

### Training

```bash
python bundeskanzler_ki.py train --epochs 200 --batch_size 4
```

### Interaktiver Modus

```bash
python bundeskanzler_ki.py interact
```

### Batch-Verarbeitung

```bash
python bundeskanzler_ki.py batch --input fragen.txt --output antworten.csv
```

## ğŸ”§ Konfiguration

Die Konfiguration erfolgt Ã¼ber mehrere Dateien:

- `tf_config.py`: TensorFlow-spezifische Einstellungen
- `config.yaml`: Allgemeine Projekteinstellungen
- `transformer_model.py`: Modellarchitektur und Trainingsparameter

### Wichtige Konfigurationsparameter

```yaml
model:
  embedding_dim: 16
  lstm_units: 8
  dropout_rate: 0.6
  learning_rate: 0.00001

training:
  batch_size: 4
  epochs: 200
  validation_split: 0.2
```

## ğŸ“Š Modellarchitektur

Das System verwendet eine optimierte LSTM-basierte Architektur:

1. **Embedding Layer**: 
   - DimensionalitÃ¤t: 16
   - L1/L2 Regularisierung
   - MaxNorm Constraints

2. **LSTM Layer**:
   - 8 Units
   - Bidirektional
   - Dropout: 0.6
   - Kernel, Recurrent und Bias Regularisierung

3. **Dense Layer**:
   - ReLU Aktivierung
   - Batch Normalization
   - L1/L2 Regularisierung

## ğŸ”„ Training

Das Training ist optimiert fÃ¼r kleine DatensÃ¤tze und verwendet:

- Early Stopping mit erhÃ¶hter Geduld
- Learning Rate Reduction
- Gradient Clipping
- L1/L2 Regularisierung
- Hohe Dropout-Raten

## ğŸ“‹ Geplante Erweiterungen

- [ ] Erweiterter Trainingskorpus
- [ ] Zero-shot/Few-shot Learning
- [ ] Mehrsprachige UnterstÃ¼tzung
- [ ] FaktenprÃ¼fung und Quellenangaben
- [ ] Verbessertes KontextverstÃ¤ndnis

## ğŸ¤ Beitragen

BeitrÃ¤ge sind willkommen! Bitte beachten Sie:

1. Fork des Repositories
2. Feature-Branch erstellen
3. Ã„nderungen committen
4. Push zum Branch
5. Pull Request erstellen

## ğŸ“ Lizenz

Dieses Projekt ist unter der MIT-Lizenz lizenziert.

## ğŸ™ Danksagung

Besonderer Dank gilt allen Mitwirkenden und der Open-Source-Community fÃ¼r ihre wertvollen BeitrÃ¤ge und UnterstÃ¼tzung.

## ğŸ“¬ Kontakt

Bei Fragen oder Anregungen kÃ¶nnen Sie ein Issue erstellen oder sich direkt an die Projektbetreuer wenden.

---

**Hinweis**: Dieses Projekt befindet sich in aktiver Entwicklung. Feedback und VerbesserungsvorschlÃ¤ge sind jederzeit willkommen!
