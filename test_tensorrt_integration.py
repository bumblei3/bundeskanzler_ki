#!/usr/bin/env python3
"""
TensorRT Integration Test fÃ¼r Bundeskanzler KI
Testet die neue TensorRT-Optimierung fÃ¼r RTX 2070 GPU
"""

import sys
import os
import logging
import time
from pathlib import Path

# Projekt-Root zum Python-Pfad hinzufÃ¼gen
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# Graceful Shutdown importieren
from graceful_shutdown import setup_graceful_shutdown

# Logging konfigurieren
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def test_tensorrt_integration():
    """Testet die TensorRT-Integration"""
    logger.info("ğŸ§ª Starte erweiterte TensorRT Integration Test...")

    model = None
    try:
        # Import der multimodalen KI
        from multimodal_ki import MultimodalTransformerModel

        # Erstelle RTX 2070 optimiertes Modell
        logger.info("ğŸ¯ Erstelle RTX 2070 optimiertes Modell...")
        model = MultimodalTransformerModel(model_tier="rtx2070")

        # ÃœberprÃ¼fe TensorRT-VerfÃ¼gbarkeit
        if hasattr(model, 'tensorrt_optimizer') and model.tensorrt_optimizer:
            logger.info("âœ… TensorRT Optimizer verfÃ¼gbar")

            # Zeige TensorRT-Status
            logger.info(f"ğŸ® RTX 2070 erkannt: {model.is_rtx2070}")
            logger.info(f"ğŸ’¾ GPU Memory: {model.gpu_memory_gb}GB")

            # Teste echte TensorRT Engine-Erstellung
            logger.info("ğŸš€ Teste echte TensorRT Engine-Erstellung...")
            optimization_results = model.optimize_with_tensorrt()

            logger.info("ğŸ“Š Optimierungsergebnisse:")
            logger.info(f"  Status: {optimization_results['status']}")
            logger.info(f"  Optimierte Modelle: {optimization_results['models_optimized']}")

            if optimization_results['status'] == 'completed':
                logger.info("âœ… TensorRT Engine-Erstellung erfolgreich!")

                # Zeige Performance-Gains
                for model_name, gains in optimization_results.get('performance_gains', {}).items():
                    if isinstance(gains, dict):
                        speedup = gains.get('expected_speedup', 'N/A')
                        memory = gains.get('memory_efficiency', 'N/A')
                        logger.info(f"  {model_name}: {speedup} schneller, {memory} Speicherersparnis")

                # Teste TensorRT Inference
                test_tensorrt_inference(model, optimization_results)

            else:
                logger.warning("âš ï¸ TensorRT Engine-Erstellung fehlgeschlagen")
                if 'errors' in optimization_results:
                    for error in optimization_results['errors']:
                        logger.error(f"  Fehler: {error}")

            # TensorRT Engines Status
            logger.info(f"ğŸ”§ TensorRT Engines: {len(model.tensorrt_engines)} geladen")

            logger.info("âœ… Erweiterte TensorRT Integration Test erfolgreich abgeschlossen")
            return True

        else:
            logger.warning("âš ï¸ TensorRT Optimizer nicht verfÃ¼gbar")
            logger.info("â„¹ï¸ ÃœberprÃ¼fe TensorRT-Installation...")

            # Zeige verfÃ¼gbare Komponenten
            logger.info(f"ğŸ® RTX 2070 erkannt: {model.is_rtx2070}")
            logger.info(f"ğŸ’¾ GPU Memory: {model.gpu_memory_gb}GB")
            logger.info(f"ğŸ“ Text-Modell: {model.text_model_type if model.text_model else 'Nicht verfÃ¼gbar'}")
            logger.info(f"ğŸ–¼ï¸ Vision-Modell: {model.vision_model_type if model.vision_model else 'Nicht verfÃ¼gbar'}")
            logger.info(f"ğŸµ Audio-Modell: {model.audio_model_type if model.audio_model else 'Nicht verfÃ¼gbar'}")

            return True

    except ImportError as e:
        logger.error(f"âŒ Import-Fehler: {e}")
        logger.info("â„¹ï¸ Stelle sicher, dass alle AbhÃ¤ngigkeiten installiert sind")
        return False

    except Exception as e:
        logger.error(f"âŒ Unerwarteter Fehler: {e}")
        return False

    finally:
        # Sicherstellen, dass das Modell ordnungsgemÃ¤ÃŸ beendet wird
        if model:
            try:
                del model
            except:
                pass

def test_tensorrt_inference(model, optimization_results):
    """Testet TensorRT Inference mit echten Daten"""
    logger.info("ğŸ”¬ Teste TensorRT Inference...")

    try:
        # Text-Test
        if "text" in optimization_results.get('tensorrt_engines', {}):
            logger.info("ğŸ“ Teste Text-Inference mit TensorRT...")
            test_text = "Die Bundeskanzlerin sprach Ã¼ber wichtige Themen der Wirtschaftspolitik."

            # Vergleiche Original vs TensorRT
            start_time = time.time()
            result_original = model.process_text(test_text, max_length=50)
            original_time = time.time() - start_time

            start_time = time.time()
            result_tensorrt = model.process_text_tensorrt(test_text, max_length=50)
            tensorrt_time = time.time() - start_time

            speedup = original_time / tensorrt_time if tensorrt_time > 0 else 0

            logger.info(f"  Original Ergebnis: {result_original[:100]}...")
            logger.info(f"  TensorRT Ergebnis: {result_tensorrt[:100]}...")
            logger.info(f"  Speedup: {speedup:.2f}x")

        # Vision-Test (vereinfacht)
        if "vision" in optimization_results.get('tensorrt_engines', {}):
            logger.info("ğŸ–¼ï¸ Vision-Modell mit TensorRT optimiert verfÃ¼gbar")

        # Audio-Test (vereinfacht)
        if "audio" in optimization_results.get('tensorrt_engines', {}):
            logger.info("ğŸµ Audio-Modell mit TensorRT optimiert verfÃ¼gbar")

    except Exception as e:
        logger.error(f"âŒ Fehler bei TensorRT Inference-Test: {e}")


def test_tensorrt_availability():
    """Testet grundlegende TensorRT-VerfÃ¼gbarkeit"""
    logger.info("ğŸ” Teste TensorRT-VerfÃ¼gbarkeit...")

    try:
        import tensorrt as trt
        logger.info(f"âœ… TensorRT {trt.__version__} verfÃ¼gbar")

        # Teste Logger
        logger_trt = trt.Logger(trt.Logger.WARNING)
        logger.info("âœ… TensorRT Logger erstellt")

        return True

    except ImportError:
        logger.error("âŒ TensorRT nicht installiert")
        logger.info("â„¹ï¸ Installiere TensorRT mit: pip install tensorrt")
        return False

    except Exception as e:
        logger.error(f"âŒ TensorRT-Fehler: {e}")
        return False

def main():
    """Hauptfunktion fÃ¼r TensorRT-Tests"""
    logger.info("ğŸš€ Bundeskanzler KI - TensorRT Integration Test")
    logger.info("=" * 60)

    # Graceful Shutdown aktivieren
    shutdown_handler = setup_graceful_shutdown()
    logger.info("ğŸ›¡ï¸ Graceful Shutdown aktiviert")

    # Teste TensorRT-VerfÃ¼gbarkeit
    if not test_tensorrt_availability():
        logger.error("âŒ TensorRT nicht verfÃ¼gbar - beende Test")
        return 1

    # Teste Integration
    if test_tensorrt_integration():
        logger.info("âœ… Alle Tests erfolgreich abgeschlossen")

        # Warte kurz um Shutdown zu testen
        logger.info("â³ Warte 5 Sekunden um Shutdown zu testen...")
        time.sleep(5)

        return 0
    else:
        logger.error("âŒ Tests fehlgeschlagen")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)