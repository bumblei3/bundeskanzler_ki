#!/usr/bin/env python3
"""
Integrationstest f√ºr die aktualisierte bundeskanzler_ki.py mit AdvancedTransformerModel
"""

import sys
import os
sys.path.append('/home/tobber/bkki_venv')

from bundeskanzler_ki import init_model, generate_transformer_response
from advanced_transformer_model import AdvancedTransformerModel
from tensorflow.keras.preprocessing.text import Tokenizer
import numpy as np

def test_init_model():
    """Teste die aktualisierte init_model Funktion"""
    print("Testing init_model Funktion...")

    try:
        # Erstelle einen einfachen Tokenizer
        corpus = [
            "Die KI-Entwicklung ist wichtig f√ºr Deutschland.",
            "K√ºnstliche Intelligenz ver√§ndert unsere Welt.",
            "Bundeskanzler Scholz besucht Berlin.",
            "Die Digitalisierung schreitet voran."
        ]

        tokenizer = Tokenizer(num_words=1000)
        tokenizer.fit_on_texts(corpus)

        maxlen = 50
        output_size = len(corpus)

        # Teste init_model mit Transformer-Unterst√ºtzung
        model = init_model(tokenizer, maxlen, output_size, use_transformer=True)

        print("‚úì init_model mit Transformer-Unterst√ºtzung erfolgreich")
        print(f"‚úì Modell-Typ: {type(model)}")
        print(f"‚úì Modell-Input-Shape: {model.input_shape}")
        print(f"‚úì Modell-Output-Shape: {model.output_shape}")

        return True

    except Exception as e:
        print(f"‚úó init_model Test fehlgeschlagen: {e}")
        return False

def test_transformer_response():
    """Teste die generate_transformer_response Funktion"""
    print("\nTesting generate_transformer_response Funktion...")

    try:
        # Initialisiere Transformer-Modell
        transformer_model = AdvancedTransformerModel(model_type="gpt2", model_name="gpt2")

        # Teste Antwortgenerierung
        test_question = "Was ist der Sinn des Lebens?"
        response = generate_transformer_response(test_question, transformer_model, max_length=50)

        print("‚úì Transformer-Antwortgenerierung erfolgreich")
        print(f"‚úì Frage: {test_question}")
        print(f"‚úì Antwort: {response}")

        return True

    except Exception as e:
        print(f"‚úó Transformer-Response Test fehlgeschlagen: {e}")
        return False

def test_fallback_to_lstm():
    """Teste Fallback zu LSTM wenn Transformer fehlschl√§gt"""
    print("\nTesting Fallback zu LSTM...")

    try:
        # Erstelle einen einfachen Tokenizer
        corpus = ["Test sentence one.", "Test sentence two."]
        tokenizer = Tokenizer(num_words=100)
        tokenizer.fit_on_texts(corpus)

        maxlen = 20
        output_size = len(corpus)

        # Teste init_model ohne Transformer-Unterst√ºtzung
        model = init_model(tokenizer, maxlen, output_size, use_transformer=False)

        print("‚úì Fallback zu LSTM erfolgreich")
        print(f"‚úì Modell-Typ: {type(model)}")

        return True

    except Exception as e:
        print(f"‚úó LSTM Fallback Test fehlgeschlagen: {e}")
        return False

def main():
    """F√ºhre alle Integrationstests aus"""
    print("üöÄ Starting Bundeskanzler KI Integration Tests")
    print("=" * 60)

    tests = [
        test_init_model,
        test_transformer_response,
        test_fallback_to_lstm
    ]

    passed = 0
    total = len(tests)

    for test in tests:
        if test():
            passed += 1

    print("\n" + "=" * 60)
    print(f"Integration Test Results: {passed}/{total} tests passed")

    if passed == total:
        print("üéâ All integration tests passed! Bundeskanzler KI is ready with Transformer support.")
        return 0
    else:
        print("‚ö†Ô∏è  Some integration tests failed. Check the output above.")
        return 1

if __name__ == "__main__":
    sys.exit(main())