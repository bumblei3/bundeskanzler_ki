#!/usr/bin/env python3
"""
Erweiterte Bundeskanzler KI mit multimodalen FÃ¤higkeiten
UnterstÃ¼tzt Text, Bilder, Audio und Video fÃ¼r umfassende Interaktion
MIT UPGRADE-OPTIONEN: GPT-4, Claude, LLaVA, SigLIP, Whisper Large
"""

# Optionale API-Imports (nur wenn APIs verwendet werden)
try:
    import anthropic
    from openai import OpenAI
    API_IMPORTS_AVAILABLE = True
except ImportError:
    API_IMPORTS_AVAILABLE = False

# Lokale Imports (immer verfÃ¼gbar)
import cv2
import gc
import hashlib
import io
import logging
import os
import time
from datetime import datetime
from typing import Any, Dict, List, Optional, Union

import librosa
import numpy as np
import requests
import tensorflow as tf
import torch
from PIL import Image
from tensorflow.keras import layers
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    CLIPModel,
    CLIPProcessor,
    SiglipModel,
    SiglipProcessor,
    WhisperForConditionalGeneration,
    WhisperProcessor,
    pipeline,
)

# Request Batching System
try:
    from core.request_batching import get_request_batcher, RequestBatcher
    REQUEST_BATCHING_AVAILABLE = True
except ImportError:
    REQUEST_BATCHING_AVAILABLE = False

# Auto-Scaling System
try:
    from core.auto_scaling import get_auto_scaler, AutoScaler
    AUTO_SCALING_AVAILABLE = True
except ImportError:
    AUTO_SCALING_AVAILABLE = False

logger = logging.getLogger(__name__)

if not API_IMPORTS_AVAILABLE:
    logger.warning("API-Imports nicht verfÃ¼gbar - API-Features deaktiviert")

if not REQUEST_BATCHING_AVAILABLE:
    logger.warning("Request Batching System nicht verfÃ¼gbar - Batch-Features deaktiviert")

if not AUTO_SCALING_AVAILABLE:
    logger.warning("Auto-Scaling System nicht verfÃ¼gbar - Auto-Scaling-Features deaktiviert")

# Import RTX 2070 LLM Manager fÃ¼r erweiterte lokale Modelle
try:
    from core.rtx2070_llm_manager import RTX2070LLMManager
    RTX2070_MANAGER_AVAILABLE = True
except ImportError:
    RTX2070_MANAGER_AVAILABLE = False

# Import Quantization Optimizer fÃ¼r intelligente Modell-Quantisierung
try:
    from core.quantization_optimizer import QuantizationOptimizer, quantization_optimizer
    QUANTIZATION_OPTIMIZER_AVAILABLE = True
except ImportError:
    QUANTIZATION_OPTIMIZER_AVAILABLE = False

# Import Intelligent Cache System fÃ¼r erweiterte Caching-Funktionen
try:
    from core.intelligent_cache import (
        IntelligentCacheManager,
        get_intelligent_cache,
        get_intelligent_cache_stats,
        intelligent_cache_manager
    )
    INTELLIGENT_CACHE_AVAILABLE = True
except ImportError:
    INTELLIGENT_CACHE_AVAILABLE = False

# Import Lokales Monitoring-System
try:
    from core.local_monitoring import get_monitoring_system
    MONITORING_AVAILABLE = True
except ImportError:
    MONITORING_AVAILABLE = False

logger = logging.getLogger(__name__)

if not RTX2070_MANAGER_AVAILABLE:
    logger.warning("RTX2070 LLM Manager nicht verfÃ¼gbar - verwende Fallback")

if not QUANTIZATION_OPTIMIZER_AVAILABLE:
    logger.warning("Quantization Optimizer nicht verfÃ¼gbar - verwende Fallback")

if not INTELLIGENT_CACHE_AVAILABLE:
    logger.warning("Intelligent Cache System nicht verfÃ¼gbar - verwende Fallback")


class MultimodalTransformerModel:
    """
    Erweiterte multimodale Transformer-KI fÃ¼r Text, Bilder, Audio und Video.

    Diese Klasse integriert verschiedene KI-Modelle und Optimierungssysteme fÃ¼r eine
    umfassende multimodale Interaktion. UnterstÃ¼tzt verschiedene Performance-Tiers
    und automatische Optimierungen.

    Attributes:
        device (torch.device): Computing device (CPU/CUDA)
        model_tier (str): Performance-Tier ("rtx2070", "basic", "advanced", "premium")
        gpu_memory_gb (int): VerfÃ¼gbarer GPU-Speicher in GB
        is_rtx2070 (bool): True wenn RTX 2070 oder vergleichbare GPU verfÃ¼gbar

        # Subsysteme
        rtx2070_manager (Optional[RTX2070LLMManager]): RTX 2070 spezifischer Manager
        quantization_optimizer (Optional[Any]): Intelligenter Quantization Optimizer
        intelligent_cache_manager (Optional[Any]): Intelligent Cache System
        request_batcher (Optional[RequestBatcher]): Request Batching System
        auto_scaler (Optional[Any]): Auto-Scaling System
        monitoring (Optional[Any]): Monitoring System

        # API Clients
        openai_client (Optional[OpenAI]): OpenAI API Client
        anthropic_client (Optional[Any]): Anthropic API Client

        # Modelle
        text_model (Optional[Any]): Text-Modell
        text_tokenizer (Optional[Any]): Text-Tokenizer
        vision_model (Optional[Any]): Vision-Modell
        vision_processor (Optional[Any]): Vision-Processor
        audio_model (Optional[Any]): Audio-Modell
        audio_processor (Optional[Any]): Audio-Processor
        video_model (Optional[Any]): Video-Modell (zukÃ¼nftig)

        # Caches
        embedding_cache (Optional[Any]): Embedding Cache
        response_cache (Optional[Any]): Response Cache
        search_cache (Optional[Any]): Search Results Cache
    """

    def __init__(self, model_tier: str = "rtx2070") -> None:
        """
        Initialisiert das multimodale Transformer-Modell.

        Args:
            model_tier: Performance-Tier fÃ¼r Modell-Optimierung
                - "rtx2070": RTX 2070 optimiert mit intelligentem Quantization
                - "basic": Grundlegende lokale Modelle
                - "advanced": Fortgeschrittene lokale Modelle
                - "premium": API-Modelle (GPT-4, Claude)

        Raises:
            ValueError: Bei ungÃ¼ltigem model_tier
        """
        if model_tier not in ["rtx2070", "basic", "advanced", "premium"]:
            raise ValueError(f"UngÃ¼ltiger model_tier: {model_tier}. Muss einer der folgenden sein: rtx2070, basic, advanced, premium")

        self.device: torch.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model_tier: str = model_tier

        # RTX 2070 Spezifikation
        self.gpu_memory_gb: int = 8 if torch.cuda.is_available() else 0
        self.is_rtx2070: bool = (
            torch.cuda.is_available()
            and torch.cuda.get_device_properties(0).total_memory >= 7.5 * 1024**3
        )  # ~8GB

        # RTX 2070 LLM Manager fÃ¼r erweiterte lokale Modelle
        self.rtx2070_manager: Optional[Any] = None
        if RTX2070_MANAGER_AVAILABLE and self.is_rtx2070:
            try:
                self.rtx2070_manager = RTX2070LLMManager()
                logger.info("âœ… RTX 2070 LLM Manager initialisiert")
            except Exception as e:
                logger.warning(f"âš ï¸ RTX 2070 Manager Initialisierung fehlgeschlagen: {e}")

        # Quantization Optimizer fÃ¼r intelligente Modell-Quantisierung
        self.quantization_optimizer: Optional[Any] = None
        if QUANTIZATION_OPTIMIZER_AVAILABLE:
            try:
                self.quantization_optimizer = quantization_optimizer
                logger.info("âœ… Quantization Optimizer initialisiert")
            except Exception as e:
                logger.warning(f"âš ï¸ Quantization Optimizer Initialisierung fehlgeschlagen: {e}")

        # Intelligent Cache System fÃ¼r erweiterte Caching-Funktionen
        self.intelligent_cache_manager: Optional[Any] = None
        self.embedding_cache: Optional[Any] = None
        self.response_cache: Optional[Any] = None
        self.search_cache: Optional[Any] = None

        if INTELLIGENT_CACHE_AVAILABLE:
            try:
                self.intelligent_cache_manager = intelligent_cache_manager
                self.embedding_cache = get_intelligent_cache("embeddings")
                self.response_cache = get_intelligent_cache("responses")
                self.search_cache = get_intelligent_cache("search_results")
                logger.info("âœ… Intelligent Cache System initialisiert")
            except Exception as e:
                logger.warning(f"âš ï¸ Intelligent Cache System Initialisierung fehlgeschlagen: {e}")

        # Request Batching System fÃ¼r optimierte Batch-Verarbeitung
        self.request_batcher: Optional[RequestBatcher] = None
        if REQUEST_BATCHING_AVAILABLE:
            try:
                self.request_batcher = get_request_batcher()
                logger.info("âœ… Request Batching System initialisiert")
            except Exception as e:
                logger.warning(f"âš ï¸ Request Batching System Initialisierung fehlgeschlagen: {e}")

        # Auto-Scaling System fÃ¼r Performance-Monitoring und Optimierungen
        self.auto_scaler: Optional[Any] = None
        if AUTO_SCALING_AVAILABLE:
            try:
                self.auto_scaler = get_auto_scaler()
                logger.info("âœ… Auto-Scaling System initialisiert")
            except Exception as e:
                logger.warning(f"âš ï¸ Auto-Scaling System Initialisierung fehlgeschlagen: {e}")

        # Lokales Monitoring-System
        self.monitoring: Optional[Any] = None
        if MONITORING_AVAILABLE:
            try:
                self.monitoring = get_monitoring_system()
                self.monitoring.start_monitoring(interval_seconds=10.0)  # Alle 10 Sekunden
                logger.info("âœ… Lokales Monitoring-System gestartet")
            except Exception as e:
                logger.warning(f"âš ï¸ Monitoring-System Initialisierung fehlgeschlagen: {e}")

        # API-Clients fÃ¼r Premium-Modelle
        self.openai_client: Optional[OpenAI] = None
        self.anthropic_client: Optional[Any] = None

        # Text-Modelle
        self.text_model: Optional[Any] = None
        self.text_tokenizer: Optional[Any] = None
        self.text_model_type: str = "local"  # "local", "openai", "anthropic"

        # Bild-Modelle
        self.vision_model: Optional[Any] = None
        self.vision_processor: Optional[Any] = None
        self.vision_model_type: str = "clip"  # "clip", "siglip"

        # Audio-Modelle
        self.audio_model: Optional[Any] = None
        self.audio_processor: Optional[Any] = None
        self.audio_model_type: str = "whisper"  # "whisper_base", "whisper_tiny"

        # Video-Modelle (fÃ¼r zukÃ¼nftige Erweiterung)
        self.video_model: Optional[Any] = None

        self.load_models()

    def load_models(self) -> None:
        """
        LÃ¤dt alle multimodalen Modelle basierend auf dem gewÃ¤hlten Tier.

        Diese Methode orchestriert das Laden aller Modelle (Text, Vision, Audio)
        entsprechend dem konfigurierten Performance-Tier. Bei Fehlern wird
        automatisch ein Fallback zu Basic-Modellen durchgefÃ¼hrt.

        Raises:
            Exception: Bei kritischen Fehlern wÃ¤hrend des Modell-Ladens
        """
        try:
            logger.info(f"ðŸš€ Lade multimodale Modelle (Tier: {self.model_tier})...")

            if self.model_tier == "rtx2070":
                self._load_rtx2070_models()
            elif self.model_tier == "premium":
                self._load_premium_models()
            elif self.model_tier == "advanced":
                self._load_advanced_models()
            else:  # basic
                self._load_basic_models()

            logger.info("âœ… Multimodale Modelle erfolgreich geladen")

        except Exception as e:
            logger.error(f"âŒ Fehler beim Laden multimodaler Modelle: {e}")
            # Fallback zu Basic-Modellen
            logger.info("ðŸ”„ Fallback zu Basic-Modellen...")
            self.model_tier = "basic"
            self._load_basic_models()

    def _load_premium_models(self) -> None:
        """
        LÃ¤dt Premium-API-Modelle (GPT-4, Claude).

        Konfiguriert API-Clients fÃ¼r OpenAI GPT-4 und Anthropic Claude.
        Verwendet lokale Modelle als Fallback wenn API-Keys nicht verfÃ¼gbar sind.
        """
        # OpenAI GPT-4
        openai_key = os.getenv("OPENAI_API_KEY")
        if openai_key:
            self.openai_client = OpenAI(api_key=openai_key)
            self.text_model_type = "openai"
            logger.info("âœ… OpenAI GPT-4 verfÃ¼gbar")
        else:
            logger.warning("âš ï¸ OPENAI_API_KEY nicht gefunden, verwende lokale Modelle")

        # Anthropic Claude
        anthropic_key = os.getenv("ANTHROPIC_API_KEY")
        if anthropic_key:
            self.anthropic_client = anthropic.Anthropic(api_key=anthropic_key)
            logger.info("âœ… Anthropic Claude verfÃ¼gbar")
        else:
            logger.warning("âš ï¸ ANTHROPIC_API_KEY nicht gefunden")

        # Fortgeschrittene lokale Modelle als Fallback
        self._load_advanced_models()

    def _load_rtx2070_models(self) -> None:
        """
        LÃ¤dt RTX 2070 optimierte Modelle mit intelligentem Quantization Optimizer.

        PrioritÃ¤t 1: RTX 2070 LLM Manager fÃ¼r erweiterte lokale Modelle
        Fallback: Intelligenter Quantization Optimizer
        """
        logger.info("ðŸŽ® RTX 2070 Modus aktiviert - Verwende intelligenten Quantization Optimizer")

        # PrioritÃ¤t 1: RTX 2070 LLM Manager fÃ¼r erweiterte lokale Modelle
        if self.rtx2070_manager:
            logger.info("ðŸš€ Verwende RTX 2070 LLM Manager fÃ¼r erweiterte Modelle")

            # WÃ¤hle optimales Modell basierend auf KomplexitÃ¤t (default: medium)
            optimal_model = self.rtx2070_manager.select_optimal_model("medium")

            if self.rtx2070_manager.load_model(optimal_model):
                self.text_model_type = f"rtx2070_manager_{optimal_model}"
                logger.info(f"âœ… Erweiterte lokale Modelle geladen: {optimal_model}")

                # Vision und Audio Modelle weiterhin laden
                self._load_rtx2070_vision_audio()
                return
            else:
                logger.warning("âš ï¸ RTX 2070 Manager fehlgeschlagen - verwende intelligenten Quantization Optimizer")

        # Fallback: Intelligenter Quantization Optimizer
        if self.quantization_optimizer:
            logger.info("ðŸ§  Verwende intelligenten Quantization Optimizer fÃ¼r RTX 2070")

            try:
                # Text-Modell mit optimierter Quantisierung laden
                self.text_model, self.text_tokenizer = self.quantization_optimizer.load_text_model_optimized("gpt2-medium")
                self.text_model_type = "gpt2_medium_quantized_rtx2070"
                logger.info("âœ… GPT-2 Medium (Intelligent quantisiert fÃ¼r RTX 2070)")

                # Vision und Audio Modelle laden
                self._load_rtx2070_vision_audio()
                return

            except Exception as e:
                logger.warning(f"âš ï¸ Intelligenter Quantization Optimizer fehlgeschlagen: {e}")
                logger.info("ðŸ”„ Fallback auf klassische RTX 2070 Optimierung")

        # Letzter Fallback: UrsprÃ¼ngliche RTX 2070 Optimierung
        logger.info("ðŸ”„ Fallback auf klassische RTX 2070 Modelle")

        # RTX 2070: 8-bit quantization fÃ¼r alle Modelle
        quantization_config = (
            BitsAndBytesConfig(
                load_in_8bit=True,
                llm_int8_enable_fp32_cpu_offload=True,
                llm_int8_skip_modules=["lm_head"],  # Preserve output layer precision
            )
            if torch.cuda.is_available()
            else None
        )

        # Text-Modell: GPT-2 Medium (355M Parameter - passt in 8GB)
        try:
            self.text_tokenizer = AutoTokenizer.from_pretrained("gpt2-medium")
            self.text_model = AutoModelForCausalLM.from_pretrained(
                "gpt2-medium",
                quantization_config=quantization_config,
                torch_dtype=torch.float16,
                device_map="auto",
                low_cpu_mem_usage=True,
                trust_remote_code=True,
            )
            self.text_tokenizer.pad_token = self.text_tokenizer.eos_token
            self.text_model_type = "gpt2_medium_rtx2070"
            logger.info("âœ… GPT-2 Medium (RTX 2070 optimiert)")

        except Exception as e:
            logger.warning(f"âš ï¸ GPT-2 Medium fehlgeschlagen: {e}")
            # 4-bit Fallback
            quantization_4bit = (
                BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4",
                )
                if torch.cuda.is_available()
                else None
            )

            self.text_tokenizer = AutoTokenizer.from_pretrained("gpt2")
            self.text_model = AutoModelForCausalLM.from_pretrained(
                "gpt2",
                quantization_config=quantization_4bit,
                device_map="auto",
                low_cpu_mem_usage=True,
            )
            self.text_tokenizer.pad_token = self.text_tokenizer.eos_token
            self.text_model_type = "gpt2_base_rtx2070"
            logger.info("âœ… GPT-2 Base (4-bit RTX 2070 Fallback)")

        # Vision und Audio Modelle laden
        self._load_rtx2070_vision_audio()

    def _load_rtx2070_vision_audio(self):
        """LÃ¤dt Vision- und Audio-Modelle fÃ¼r RTX 2070 mit intelligentem Quantization Optimizer"""
        logger.info("ðŸŽ¨ Lade RTX 2070 Vision & Audio Modelle")

        # Verwende intelligenten Quantization Optimizer falls verfÃ¼gbar
        if self.quantization_optimizer:
            try:
                # Vision-Modell mit optimierter Quantisierung
                self.vision_model, self.vision_processor = self.quantization_optimizer.load_vision_model_optimized(
                    "google/siglip-base-patch16-224"
                )
                self.vision_model_type = "siglip_base_quantized_rtx2070"
                logger.info("âœ… SigLIP Base (Intelligent quantisiert fÃ¼r RTX 2070)")

                # Audio-Modell mit optimierter Quantisierung
                self.audio_model, self.audio_processor = self.quantization_optimizer.load_audio_model_optimized(
                    "openai/whisper-base"
                )
                self.audio_model_type = "whisper_base_quantized_rtx2070"
                logger.info("âœ… Whisper Base (Intelligent quantisiert fÃ¼r RTX 2070)")

                return

            except Exception as e:
                logger.warning(f"âš ï¸ Intelligenter Quantization Optimizer fÃ¼r Vision/Audio fehlgeschlagen: {e}")
                logger.info("ðŸ”„ Fallback auf klassische RTX 2070 Vision/Audio")

        # Fallback: Klassische RTX 2070 Optimierung
        # RTX 2070: 8-bit quantization fÃ¼r alle Modelle
        quantization_config = (
            BitsAndBytesConfig(
                load_in_8bit=True,
                llm_int8_enable_fp32_cpu_offload=True,
                llm_int8_skip_modules=["lm_head"],
            )
            if torch.cuda.is_available()
            else None
        )

        # Vision-Modell: SigLIP Base (optimiert fÃ¼r 8GB)
        try:
            self.vision_processor = SiglipProcessor.from_pretrained(
                "google/siglip-base-patch16-224"
            )
            self.vision_model = SiglipModel.from_pretrained(
                "google/siglip-base-patch16-224",
                quantization_config=quantization_config,
                torch_dtype=torch.float16,
                device_map="auto",
                low_cpu_mem_usage=True,
            )
            self.vision_model_type = "siglip_base_rtx2070"
            logger.info("âœ… SigLIP Base (RTX 2070 optimiert)")

        except Exception as e:
            logger.warning(f"âš ï¸ SigLIP fehlgeschlagen: {e}")
            # CLIP Base Fallback
            try:
                self.vision_model = CLIPModel.from_pretrained(
                    "openai/clip-vit-base-patch32",
                    quantization_config=quantization_config,
                    torch_dtype=torch.float16,
                    device_map="auto",
                    low_cpu_mem_usage=True,
                )
                self.vision_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
                self.vision_model_type = "clip_base_rtx2070"
                logger.info("âœ… CLIP Base (RTX 2070 Fallback)")
            except Exception as e2:
                logger.error(f"âŒ Vision-Modell Laden fehlgeschlagen: {e2}")

        # Audio-Modell: Whisper Base (optimiert fÃ¼r 8GB)
        try:
            self.audio_processor = WhisperProcessor.from_pretrained("openai/whisper-base")
            self.audio_model = WhisperForConditionalGeneration.from_pretrained(
                "openai/whisper-base",
                torch_dtype=torch.float16,
                device_map="auto",
                low_cpu_mem_usage=True,
            )
            self.audio_model_type = "whisper_base_rtx2070"
            logger.info("âœ… Whisper Base (RTX 2070 optimiert)")

        except Exception as e:
            logger.warning(f"âš ï¸ Whisper Base fehlgeschlagen: {e}")
            # Tiny Fallback ohne quantization
            try:
                self.audio_processor = WhisperProcessor.from_pretrained("openai/whisper-tiny")
                self.audio_model = WhisperForConditionalGeneration.from_pretrained(
                    "openai/whisper-tiny",
                    torch_dtype=torch.float16,
                    device_map="auto",
                    low_cpu_mem_usage=True,
                )
                self.audio_model_type = "whisper_tiny_rtx2070"
                logger.info("âœ… Whisper Tiny (RTX 2070 Fallback)")
            except Exception as e2:
                logger.error(f"âŒ Audio-Modell Laden fehlgeschlagen: {e2}")

        # Speicher-Management fÃ¼r RTX 2070
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()
            logger.info(
                f"ðŸ§¹ GPU Speicher bereinigt - Verwendet: {torch.cuda.memory_allocated() // 1024**2}MB"
            )

    def _load_advanced_models(self):
        """LÃ¤dt fortgeschrittene lokale Modelle mit intelligentem Quantization Optimizer"""
        logger.info("ðŸ”¬ Lade fortgeschrittene lokale Modelle")

        # Verwende intelligenten Quantization Optimizer falls verfÃ¼gbar
        if self.quantization_optimizer:
            try:
                # Text-Modell mit optimierter Quantisierung
                self.text_model, self.text_tokenizer = self.quantization_optimizer.load_text_model_optimized("gpt2-medium")
                self.text_model_type = "gpt2_medium_quantized_advanced"
                logger.info("âœ… GPT-2 Medium (Intelligent quantisiert - Advanced)")

                # Vision und Audio Modelle mit Standard-Optimierung laden
                self._load_basic_vision_audio()
                return

            except Exception as e:
                logger.warning(f"âš ï¸ Intelligenter Quantization Optimizer fÃ¼r Advanced-Modelle fehlgeschlagen: {e}")
                logger.info("ðŸ”„ Fallback auf klassische Advanced-Modelle")

        # Fallback: UrsprÃ¼ngliche Advanced-Modell-Optimierung
        # RTX 2070 Optimierung: 8-bit quantization fÃ¼r bessere Speichereffizienz
        quantization_config = (
            BitsAndBytesConfig(
                load_in_8bit=True,
                llm_int8_enable_fp32_cpu_offload=True,  # CPU offloading fÃ¼r groÃŸe Modelle
            )
            if torch.cuda.is_available()
            else None
        )

        # Text-Modell (RTX 2070 optimiert: GPT-2 Medium statt XL)
        try:
            # RTX 2070: Verwende Medium statt XL fÃ¼r 8GB VRAM
            model_name = "gpt2-medium"  # 355M Parameter statt 1.5B

            self.text_tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.text_model = AutoModelForCausalLM.from_pretrained(
                model_name,
                quantization_config=quantization_config,
                torch_dtype=(torch.float16 if torch.cuda.is_available() else torch.float32),
                device_map="auto" if torch.cuda.is_available() else None,
                low_cpu_mem_usage=True,
                trust_remote_code=True,
            )
            self.text_tokenizer.pad_token = self.text_tokenizer.eos_token

            # GPU-Memory-Management fÃ¼r RTX 2070
            if torch.cuda.is_available():
                # Speicher freigeben nach Laden
                torch.cuda.empty_cache()
                gc.collect()

            self.text_model_type = "local_advanced_rtx2070"
            logger.info("âœ… GPT-2 Medium geladen (RTX 2070 optimiert - 8-bit)")

        except Exception as e:
            logger.warning(f"âš ï¸ GPT-2 Medium Laden fehlgeschlagen: {e}")
            # Fallback zu GPT-2 Base mit 4-bit quantization
            try:
                quantization_config_4bit = (
                    BitsAndBytesConfig(
                        load_in_4bit=True,
                        bnb_4bit_compute_dtype=torch.float16,
                        bnb_4bit_use_double_quant=True,
                        bnb_4bit_quant_type="nf4",
                    )
                    if torch.cuda.is_available()
                    else None
                )

                self.text_tokenizer = AutoTokenizer.from_pretrained("gpt2")
                self.text_model = AutoModelForCausalLM.from_pretrained(
                    "gpt2",
                    quantization_config=quantization_config_4bit,
                    torch_dtype=(torch.float16 if torch.cuda.is_available() else torch.float32),
                    device_map="auto" if torch.cuda.is_available() else None,
                    low_cpu_mem_usage=True,
                )
                self.text_tokenizer.pad_token = self.text_tokenizer.eos_token

                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    gc.collect()

                self.text_model_type = "local_basic_rtx2070"
                logger.info("âœ… GPT-2 Base geladen (RTX 2070 - 4-bit Fallback)")

            except Exception as e2:
                logger.error(f"âŒ Auch GPT-2 Base fehlgeschlagen: {e2}")
                # CPU-Only Fallback
                self.text_tokenizer = AutoTokenizer.from_pretrained("gpt2")
                self.text_model = AutoModelForCausalLM.from_pretrained("gpt2")
                self.text_tokenizer.pad_token = self.text_tokenizer.eos_token
                self.text_model.to("cpu")
                self.text_model_type = "local_cpu_only"
                logger.info("âœ… GPT-2 CPU-Only Fallback")

        # Vision-Modelle (RTX 2070 optimiert)
        try:
            # RTX 2070: Verwende kleinere SigLIP Variante
            vision_model_name = "google/siglip-base-patch16-224"  # Base statt Large

            self.vision_processor = SiglipProcessor.from_pretrained(vision_model_name)
            self.vision_model = SiglipModel.from_pretrained(
                vision_model_name,
                quantization_config=quantization_config,
                torch_dtype=(torch.float16 if torch.cuda.is_available() else torch.float32),
                device_map="auto" if torch.cuda.is_available() else None,
                low_cpu_mem_usage=True,
            )

            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                gc.collect()

            self.vision_model_type = "siglip_rtx2070"
            logger.info("âœ… SigLIP Base geladen (RTX 2070 optimiert)")

        except Exception as e:
            logger.warning(f"âš ï¸ SigLIP Laden fehlgeschlagen: {e}")
            # Fallback zu CLIP Base mit 4-bit
            try:
                quantization_config_4bit = (
                    BitsAndBytesConfig(
                        load_in_4bit=True,
                        bnb_4bit_compute_dtype=torch.float16,
                        bnb_4bit_use_double_quant=True,
                        bnb_4bit_quant_type="nf4",
                    )
                    if torch.cuda.is_available()
                    else None
                )

                self.vision_model = CLIPModel.from_pretrained(
                    "openai/clip-vit-base-patch32",
                    quantization_config=quantization_config_4bit,
                    torch_dtype=(torch.float16 if torch.cuda.is_available() else torch.float32),
                    device_map="auto" if torch.cuda.is_available() else None,
                    low_cpu_mem_usage=True,
                )
                self.vision_processor = CLIPProcessor.from_pretrained(
                    "openai/clip-vit-base-patch32"
                )

                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    gc.collect()

                self.vision_model_type = "clip_base_rtx2070"
                logger.info("âœ… CLIP Base geladen (RTX 2070 - 4-bit Fallback)")

            except Exception as e2:
                logger.error(f"âŒ Auch CLIP Base fehlgeschlagen: {e2}")
                # CPU-Only Fallback
                self.vision_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
                self.vision_processor = CLIPProcessor.from_pretrained(
                    "openai/clip-vit-base-patch32"
                )
                self.vision_model.to("cpu")
                self.vision_model_type = "clip_cpu_only"
                logger.info("âœ… CLIP CPU-Only Fallback")

        # Audio-Modelle (RTX 2070 optimiert)
        try:
            # RTX 2070: Verwende Whisper Base statt Large fÃ¼r 8GB VRAM
            audio_model_name = "openai/whisper-base"  # Base statt Large

            self.audio_processor = WhisperProcessor.from_pretrained(audio_model_name)
            self.audio_model = WhisperForConditionalGeneration.from_pretrained(
                audio_model_name,
                quantization_config=quantization_config,
                torch_dtype=(torch.float16 if torch.cuda.is_available() else torch.float32),
                device_map="auto" if torch.cuda.is_available() else None,
                low_cpu_mem_usage=True,
            )

            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                gc.collect()

            self.audio_model_type = "whisper_base_rtx2070"
            logger.info("âœ… Whisper Base geladen (RTX 2070 optimiert)")

        except Exception as e:
            logger.warning(f"âš ï¸ Whisper Base Laden fehlgeschlagen: {e}")
            # Fallback zu Whisper Tiny mit 4-bit
            try:
                quantization_config_4bit = (
                    BitsAndBytesConfig(
                        load_in_4bit=True,
                        bnb_4bit_compute_dtype=torch.float16,
                        bnb_4bit_use_double_quant=True,
                        bnb_4bit_quant_type="nf4",
                    )
                    if torch.cuda.is_available()
                    else None
                )

                self.audio_processor = WhisperProcessor.from_pretrained("openai/whisper-tiny")
                self.audio_model = WhisperForConditionalGeneration.from_pretrained(
                    "openai/whisper-tiny",
                    quantization_config=quantization_config_4bit,
                    torch_dtype=(torch.float16 if torch.cuda.is_available() else torch.float32),
                    device_map="auto" if torch.cuda.is_available() else None,
                    low_cpu_mem_usage=True,
                )

                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    gc.collect()

                self.audio_model_type = "whisper_tiny_rtx2070"
                logger.info("âœ… Whisper Tiny geladen (RTX 2070 - 4-bit Fallback)")

            except Exception as e2:
                logger.error(f"âŒ Auch Whisper Tiny fehlgeschlagen: {e2}")
                # CPU-Only Fallback
                self.audio_processor = WhisperProcessor.from_pretrained("openai/whisper-tiny")
                self.audio_model = WhisperForConditionalGeneration.from_pretrained(
                    "openai/whisper-tiny"
                )
                self.audio_model.to("cpu")
                self.audio_model_type = "whisper_cpu_only"
                logger.info("âœ… Whisper CPU-Only Fallback")

    def _load_basic_models(self):
        """LÃ¤dt grundlegende lokale Modelle (wie bisher)"""
        # Text-Modell (GPT-2 Large)
        self.text_tokenizer = AutoTokenizer.from_pretrained("gpt2-large")
        self.text_model = AutoModelForCausalLM.from_pretrained("gpt2-large")
        self.text_tokenizer.pad_token = self.text_tokenizer.eos_token
        self.text_model.to(self.device)
        self.text_model_type = "local"

        # CLIP fÃ¼r BildverstÃ¤ndnis
        self.vision_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        self.vision_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
        self.vision_model.to(self.device)
        self.vision_model_type = "clip"

        # Whisper fÃ¼r Audio-Transkription
        self.audio_processor = WhisperProcessor.from_pretrained("openai/whisper-small")
        self.audio_model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small")
        self.audio_model.to(self.device)

        logger.info("âœ… Basic-Modelle geladen")

    def process_text(self, text: str, max_length: int = 200) -> str:
        """
        Verarbeitet Text-Eingaben mit verschiedenen Modell-Typen.

        WÃ¤hlt automatisch das beste verfÃ¼gbare Modell basierend auf der Konfiguration:
        - OpenAI GPT-4 (falls API-Key verfÃ¼gbar)
        - Anthropic Claude (falls API-Key verfÃ¼gbar)
        - Lokale Modelle (GPT-2, RTX 2070 optimiert)

        Args:
            text: Der zu verarbeitende Text
            max_length: Maximale LÃ¤nge der Antwort in Tokens

        Returns:
            Die generierte Antwort als String

        Raises:
            Exception: Bei kritischen Fehlern wÃ¤hrend der Verarbeitung
        """
        try:
            if self.text_model_type == "openai" and self.openai_client:
                return self._process_text_openai(text, max_length)
            elif self.text_model_type == "anthropic" and self.anthropic_client:
                return self._process_text_anthropic(text, max_length)
            else:
                return self._process_text_local(text, max_length)

        except Exception as e:
            logger.error(f"Text-Verarbeitung fehlgeschlagen: {e}")
            return "Entschuldigung, ich konnte Ihre Texteingabe nicht verarbeiten."

    def _process_text_openai(self, text: str, max_length: int = 200) -> str:
        """
        Verarbeitet Text mit OpenAI GPT-4 API.

        Args:
            text: Der zu verarbeitende Text
            max_length: Maximale LÃ¤nge der Antwort

        Returns:
            Die generierte Antwort von GPT-4
        """
        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {
                        "role": "system",
                        "content": "Du bist eine fortschrittliche KI-Assistentin fÃ¼r politische Kommunikation in Deutschland. Antworte professionell, sachlich und hilfreich.",
                    },
                    {"role": "user", "content": text},
                ],
                max_tokens=max_length,
                temperature=0.7,
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"OpenAI API Fehler: {e}")
            return self._process_text_local(text, max_length)  # Fallback

    def _process_text_anthropic(self, text: str, max_length: int = 200) -> str:
        """Verarbeitet Text mit Anthropic Claude"""
        try:
            response = self.anthropic_client.messages.create(
                model="claude-3-sonnet-20240229",
                max_tokens=max_length,
                system="Du bist eine fortschrittliche KI-Assistentin fÃ¼r politische Kommunikation in Deutschland. Antworte professionell, sachlich und hilfreich.",
                messages=[{"role": "user", "content": text}],
            )
            return response.content[0].text.strip()
        except Exception as e:
            logger.error(f"Anthropic API Fehler: {e}")
            return self._process_text_local(text, max_length)  # Fallback

    def _process_text_local(self, text: str, max_length: int = 200) -> str:
        """Verarbeitet Text mit lokalen Modellen (RTX2070 Manager oder GPT-2 Fallback)"""
        import time

        start_time = time.time()
        memory_before = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0

        try:
            # PrioritÃ¤t 1: RTX2070 Manager fÃ¼r erweiterte lokale Modelle
            if self.rtx2070_manager and self.text_model_type.startswith("rtx2070_manager"):
                logger.debug("Verwende RTX2070 Manager fÃ¼r Text-Generierung")

                # Smart Model Selection: Automatische Modell-Auswahl basierend auf Query
                optimal_model = self.rtx2070_manager.select_model_for_query(text)

                # Wechsle Modell wenn nÃ¶tig
                if optimal_model != self.rtx2070_manager.current_model:
                    logger.info(f"ðŸ”„ Wechsle zu optimalem Modell: {optimal_model}")
                    if self.rtx2070_manager.load_model(optimal_model):
                        logger.info(f"âœ… Modell {optimal_model} erfolgreich geladen")
                    else:
                        logger.warning(f"âš ï¸ Modell-Wechsel zu {optimal_model} fehlgeschlagen")

                response = self.rtx2070_manager.generate_response(text, max_length=max_length)

                # Monitoring: Inference-Metriken aufzeichnen
                if self.monitoring:
                    inference_time_ms = (time.time() - start_time) * 1000
                    tokens_generated = len(response.split())  # Vereinfachte Token-ZÃ¤hlung
                    memory_peak = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
                    self.monitoring.record_model_inference(
                        model_name=optimal_model,
                        inference_time_ms=inference_time_ms,
                        tokens_generated=tokens_generated,
                        memory_peak_mb=memory_peak / 1024**2
                    )

                return response

            # Fallback: UrsprÃ¼ngliche lokale Modelle (GPT-2)
            logger.debug("Verwende lokale GPT-2 Modelle")
            inputs = self.text_tokenizer(text, return_tensors="pt").to(self.device)

            with torch.no_grad():
                outputs = self.text_model.generate(
                    **inputs,
                    max_length=max_length + len(inputs["input_ids"][0]),
                    temperature=0.8,
                    top_p=0.9,
                    do_sample=True,
                    pad_token_id=self.text_tokenizer.eos_token_id,
                )

            response = self.text_tokenizer.decode(outputs[0], skip_special_tokens=True)
            # Entferne die ursprÃ¼ngliche Eingabe aus der Antwort
            if response.startswith(text):
                response = response[len(text) :].strip()

            # Monitoring: Inference-Metriken aufzeichnen
            if self.monitoring:
                inference_time_ms = (time.time() - start_time) * 1000
                tokens_generated = len(response.split())
                memory_peak = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
                self.monitoring.record_model_inference(
                    model_name="gpt2_fallback",
                    inference_time_ms=inference_time_ms,
                    tokens_generated=tokens_generated,
                    memory_peak_mb=memory_peak / 1024**2
                )

            return response

        except Exception as e:
            logger.error(f"Lokale Text-Verarbeitung fehlgeschlagen: {e}")
            return "Entschuldigung, ich konnte Ihre Texteingabe nicht verarbeiten."

    def process_image(self, image_path: str) -> Dict[str, Union[str, float]]:
        """
        Verarbeitet Bild-Eingaben mit CLIP oder SigLIP Modellen.

        Analysiert Bilder und generiert Beschreibungen sowie Relevanz-Scores
        fÃ¼r verschiedene politische und thematische Kategorien.

        Args:
            image_path: Pfad zur Bilddatei

        Returns:
            Dictionary mit folgenden SchlÃ¼sseln:
            - "description": NatÃ¼rlichsprachliche Beschreibung des Bildes
            - "best_match": Beste passende Kategorie
            - "confidence": Konfidenz-Score der besten Ãœbereinstimmung
            - "all_scores": Dictionary mit Scores fÃ¼r alle Kategorien

        Raises:
            FileNotFoundError: Wenn die Bilddatei nicht gefunden wird
            Exception: Bei Verarbeitungsfehlern
        """
        try:
            image = Image.open(image_path)

            # Vordefinierte Beschreibungen fÃ¼r verschiedene Themen
            descriptions = [
                "eine politische Versammlung",
                "ein RegierungsgebÃ¤ude",
                "eine Rede des Bundeskanzlers",
                "eine Pressekonferenz",
                "ein historisches Dokument",
                "eine Deutschlandkarte",
                "eine Wirtschaftsstatistik",
                "eine Umweltinitiative",
                "eine technologische Innovation",
                "eine internationale Konferenz",
            ]

            if self.vision_model_type == "siglip":
                return self._process_image_siglip(image, descriptions)
            else:
                return self._process_image_clip(image, descriptions)

        except Exception as e:
            logger.error(f"Bild-Verarbeitung fehlgeschlagen: {e}")
            return {
                "description": "Bild konnte nicht analysiert werden",
                "confidence": 0.0,
            }

    def _process_image_clip(
        self, image: Image.Image, descriptions: List[str]
    ) -> Dict[str, Union[str, float]]:
        """Verarbeitet Bild mit CLIP-Modell"""
        inputs = self.vision_processor(
            text=descriptions, images=image, return_tensors="pt", padding=True
        ).to(self.device)

        with torch.no_grad():
            outputs = self.vision_model(**inputs)
            logits_per_image = outputs.logits_per_image
            probs = logits_per_image.softmax(dim=1)

        best_idx = torch.argmax(probs[0]).item()
        confidence = probs[0][best_idx].item()

        return {
            "description": descriptions[best_idx],
            "confidence": confidence,
            "all_descriptions": descriptions,
            "all_probabilities": probs[0].cpu().numpy(),
            "model": "CLIP",
        }

    def _process_image_siglip(
        self, image: Image.Image, descriptions: List[str]
    ) -> Dict[str, Union[str, float]]:
        """Verarbeitet Bild mit SigLIP-Modell (hÃ¶here Genauigkeit)"""
        # SigLIP verwendet eine andere API
        inputs = self.vision_processor(
            text=descriptions,
            images=[image] * len(descriptions),  # Wiederhole Bild fÃ¼r jeden Text
            return_tensors="pt",
            padding=True,
        ).to(self.device)

        with torch.no_grad():
            outputs = self.vision_model(**inputs)
            # SigLIP gibt logits_per_image und logits_per_text zurÃ¼ck
            logits_per_image = outputs.logits_per_image
            probs = logits_per_image.softmax(dim=1)

        best_idx = torch.argmax(probs[0]).item()
        confidence = probs[0][best_idx].item()

        return {
            "description": descriptions[best_idx],
            "confidence": confidence,
            "all_descriptions": descriptions,
            "all_probabilities": probs[0].cpu().numpy(),
            "model": "SigLIP",
        }

    def process_audio(self, audio_path: str) -> str:
        """
        Verarbeitet Audio-Eingaben mit Whisper-Modell.

        Transkribiert Audio-Dateien in Text unter Verwendung von OpenAI Whisper.
        UnterstÃ¼tzt verschiedene Audio-Formate und fÃ¼hrt automatische
        Sampling-Rate-Konvertierung durch.

        Args:
            audio_path: Pfad zur Audiodatei (unterstÃ¼tzt gÃ¤ngige Formate)

        Returns:
            Transkribierter Text aus der Audio-Datei

        Raises:
            FileNotFoundError: Wenn die Audiodatei nicht gefunden wird
            Exception: Bei Verarbeitungsfehlern
        """
        try:
            # Lade Audio
            audio, sr = librosa.load(audio_path, sr=16000)

            # Whisper erwartet 16kHz Audio
            inputs = self.audio_processor(audio, sampling_rate=16000, return_tensors="pt")
            inputs = {key: val.to(self.device) for key, val in inputs.items()}

            with torch.no_grad():
                predicted_ids = self.audio_model.generate(**inputs)

            transcription = self.audio_processor.batch_decode(
                predicted_ids, skip_special_tokens=True
            )[0]
            return transcription

        except Exception as e:
            logger.error(f"Audio-Verarbeitung fehlgeschlagen: {e}")
            return "Audio konnte nicht transkribiert werden."

    def multimodal_response(
        self,
        text: Optional[str] = None,
        image_path: Optional[str] = None,
        audio_path: Optional[str] = None
    ) -> Dict[str, str]:
        """
        Erstellt eine multimodale Antwort basierend auf allen verfÃ¼gbaren Eingaben.

        Diese Methode kombiniert Text-, Bild- und Audio-Eingaben zu einer
        kohÃ¤renten, integrierten Antwort. Jedes Modality wird separat verarbeitet
        und dann zu einer zusammenhÃ¤ngenden Antwort integriert.

        Args:
            text: Optionale Text-Eingabe fÃ¼r textuelle Verarbeitung
            image_path: Optionale Pfad zu Bilddatei fÃ¼r Bildanalyse
            audio_path: Optionale Pfad zu Audiodatei fÃ¼r Transkription

        Returns:
            Dictionary mit folgenden SchlÃ¼sseln:
            - "text_response": Antwort auf Text-Eingabe
            - "image_analysis": Analyse der Bild-Eingabe
            - "audio_transcription": Transkribierter Audio-Text
            - "integrated_response": Kombinierte Antwort aus allen ModalitÃ¤ten

        Raises:
            Exception: Bei kritischen Fehlern wÃ¤hrend der multimodalen Verarbeitung
        """
        response = {
            "text_response": "",
            "image_analysis": "",
            "audio_transcription": "",
            "integrated_response": "",
        }

        # Verarbeite Text
        if text:
            response["text_response"] = self.process_text(text)

        # Verarbeite Bild
        if image_path:
            image_result = self.process_image(image_path)
            response["image_analysis"] = (
                f"Das Bild zeigt {image_result['description']} (Konfidenz: {image_result['confidence']:.2f})"
            )

        # Verarbeite Audio
        if audio_path:
            response["audio_transcription"] = self.process_audio(audio_path)

        # Erstelle integrierte Antwort
        integrated_parts = []
        if text:
            integrated_parts.append(
                f"Basierend auf Ihrer Frage '{text}': {response['text_response']}"
            )
        if image_path:
            integrated_parts.append(response["image_analysis"])
        if audio_path:
            integrated_parts.append(f"Audio-Inhalt: {response['audio_transcription']}")

        response["integrated_response"] = " ".join(integrated_parts)

        return response

    def get_monitoring_data(self) -> Dict[str, Any]:
        """Gibt Monitoring-Daten fÃ¼r Dashboard zurÃ¼ck"""
        if self.monitoring:
            return self.monitoring.get_dashboard_data()
        else:
            return {
                "error": "Monitoring-System nicht verfÃ¼gbar",
                "timestamp": datetime.now().isoformat()
            }

    def get_system_health(self) -> Dict[str, Any]:
        """Gibt System-Health-Status zurÃ¼ck"""
        if self.monitoring:
            return self.monitoring.get_health_status()
        else:
            return {
                "overall_healthy": True,
                "monitoring_available": False,
                "timestamp": datetime.now().isoformat()
            }

    def get_quantization_performance_report(self):
        """Gibt Performance-Report des Quantization Optimizers zurÃ¼ck"""
        if self.quantization_optimizer:
            return self.quantization_optimizer.get_performance_report()
        else:
            return {"message": "Quantization Optimizer nicht verfÃ¼gbar"}

    def optimize_quantization_settings(self):
        """Optimiert Quantisierungseinstellungen basierend auf aktueller Hardware"""
        if self.quantization_optimizer:
            self.quantization_optimizer.optimize_memory_usage()
            logger.info("âœ… Quantisierungseinstellungen optimiert")
            return True
        else:
            logger.warning("âš ï¸ Quantization Optimizer nicht verfÃ¼gbar")
            return False

    def get_cached_embedding(self, text: str) -> Optional[np.ndarray]:
        """Holt Embedding aus intelligentem Cache"""
        if not self.embedding_cache:
            return None

        # Erstelle Cache-Key
        cache_key = hashlib.md5(text.encode()).hexdigest()

        # Hole Embedding aus Cache
        cached_embedding = self.embedding_cache.get(cache_key)
        if cached_embedding is not None:
            logger.debug(f"ðŸŽ¯ Cached embedding hit fÃ¼r: {text[:50]}...")
            return cached_embedding

        return None

    def cache_embedding(self, text: str, embedding: np.ndarray, ttl: Optional[float] = None):
        """Speichert Embedding im intelligenten Cache"""
        if not self.embedding_cache:
            return

        # Erstelle Cache-Key
        cache_key = hashlib.md5(text.encode()).hexdigest()

        # Speichere Embedding mit Metadaten
        metadata = {
            "text_length": len(text),
            "embedding_dim": embedding.shape[0] if len(embedding.shape) > 0 else 0,
            "cached_at": datetime.now().isoformat()
        }

        self.embedding_cache.set(
            key=cache_key,
            value=embedding,
            embedding=embedding,  # FÃ¼r semantische Suche
            ttl=ttl or 3600,  # 1 Stunde Standard
            metadata=metadata
        )

        logger.debug(f"ðŸ’¾ Embedding cached: {text[:50]}...")

    def get_cached_response(self, query: str, query_embedding: Optional[np.ndarray] = None) -> Optional[str]:
        """Holt Antwort aus intelligentem Cache mit semantischer Suche"""
        if not self.response_cache:
            return None

        # Erstelle Cache-Key
        cache_key = hashlib.md5(query.encode()).hexdigest()

        # Hole Antwort aus Cache (mit semantischer Suche)
        cached_response = self.response_cache.get(
            key=cache_key,
            query_embedding=query_embedding,
            similarity_threshold=0.9  # Hohe Ã„hnlichkeit erforderlich
        )

        if cached_response is not None:
            logger.debug(f"ðŸŽ¯ Cached response hit fÃ¼r: {query[:50]}...")
            return cached_response

        return None

    def cache_response(self, query: str, response: str, query_embedding: Optional[np.ndarray] = None,
                      ttl: Optional[float] = None):
        """Speichert Antwort im intelligenten Cache"""
        if not self.response_cache:
            return

        # Erstelle Cache-Key
        cache_key = hashlib.md5(query.encode()).hexdigest()

        # Speichere Antwort mit Metadaten
        metadata = {
            "query_length": len(query),
            "response_length": len(response),
            "cached_at": datetime.now().isoformat(),
            "model_type": getattr(self, 'text_model_type', 'unknown')
        }

        self.response_cache.set(
            key=cache_key,
            value=response,
            embedding=query_embedding,
            ttl=ttl or 1800,  # 30 Minuten Standard
            metadata=metadata
        )

        logger.debug(f"ðŸ’¾ Response cached: {query[:50]}...")

    def get_cached_search_results(self, query: str, query_embedding: Optional[np.ndarray] = None) -> Optional[Dict]:
        """Holt Suchergebnisse aus intelligentem Cache"""
        if not self.search_cache:
            return None

        # Erstelle Cache-Key
        cache_key = hashlib.md5(f"search:{query}".encode()).hexdigest()

        # Hole Suchergebnisse aus Cache
        cached_results = self.search_cache.get(
            key=cache_key,
            query_embedding=query_embedding,
            similarity_threshold=0.85
        )

        if cached_results is not None:
            logger.debug(f"ðŸŽ¯ Cached search results hit fÃ¼r: {query[:50]}...")
            return cached_results

        return None

    def cache_search_results(self, query: str, results: Dict, query_embedding: Optional[np.ndarray] = None,
                           ttl: Optional[float] = None):
        """Speichert Suchergebnisse im intelligenten Cache"""
        if not self.search_cache:
            return

        # Erstelle Cache-Key
        cache_key = hashlib.md5(f"search:{query}".encode()).hexdigest()

        # Speichere Suchergebnisse mit Metadaten
        metadata = {
            "query_length": len(query),
            "results_count": len(results.get('results', [])),
            "cached_at": datetime.now().isoformat()
        }

        self.search_cache.set(
            key=cache_key,
            value=results,
            embedding=query_embedding,
            ttl=ttl or 900,  # 15 Minuten Standard
            metadata=metadata
        )

        logger.debug(f"ðŸ’¾ Search results cached: {query[:50]}...")

    def get_cache_stats(self) -> Dict[str, Any]:
        """Gibt Statistiken des intelligenten Cache-Systems zurÃ¼ck"""
        if not INTELLIGENT_CACHE_AVAILABLE:
            return {"message": "Intelligent Cache System nicht verfÃ¼gbar"}

        try:
            return get_intelligent_cache_stats()
        except Exception as e:
            logger.warning(f"âš ï¸ Fehler beim Abrufen von Cache-Statistiken: {e}")
            return {"error": str(e)}

    def optimize_cache(self):
        """Optimiert alle intelligenten Caches"""
        if self.intelligent_cache_manager:
            try:
                self.intelligent_cache_manager.optimize_all()
                logger.info("âœ… Intelligente Caches optimiert")
            except Exception as e:
                logger.warning(f"âš ï¸ Fehler bei Cache-Optimierung: {e}")

    def clear_all_caches(self):
        """Leert alle intelligenten Caches"""
        if self.intelligent_cache_manager:
            try:
                self.intelligent_cache_manager.clear_all()
                logger.info("ðŸ§¹ Alle intelligenten Caches geleert")
            except Exception as e:
                logger.warning(f"âš ï¸ Fehler beim Leeren der Caches: {e}")

    # ===== REQUEST BATCHING METHODEN =====

    def submit_batch_text_request(self, text: str, priority: int = 1,
                                 callback: Optional[callable] = None) -> Optional[str]:
        """FÃ¼gt eine Text-Verarbeitungsanfrage zur Batch-Warteschlange hinzu"""
        if not REQUEST_BATCHING_AVAILABLE or not self.request_batcher:
            logger.warning("âš ï¸ Request Batching System nicht verfÃ¼gbar")
            return None

        try:
            request_id = self.request_batcher.submit_text_request(text, priority, callback)
            logger.info(f"ðŸ“ Text-Anfrage {request_id} zur Batch-Verarbeitung hinzugefÃ¼gt")
            return request_id
        except Exception as e:
            logger.error(f"âŒ Fehler beim HinzufÃ¼gen der Text-Anfrage: {e}")
            return None

    def submit_batch_embedding_request(self, texts: List[str], priority: int = 1,
                                      callback: Optional[callable] = None) -> Optional[str]:
        """FÃ¼gt eine Embedding-Anfrage zur Batch-Warteschlange hinzu"""
        if not REQUEST_BATCHING_AVAILABLE or not self.request_batcher:
            logger.warning("âš ï¸ Request Batching System nicht verfÃ¼gbar")
            return None

        try:
            request_id = self.request_batcher.submit_embedding_request(texts, priority, callback)
            logger.info(f"ðŸ“ Embedding-Anfrage {request_id} zur Batch-Verarbeitung hinzugefÃ¼gt")
            return request_id
        except Exception as e:
            logger.error(f"âŒ Fehler beim HinzufÃ¼gen der Embedding-Anfrage: {e}")
            return None

    def submit_batch_search_request(self, query: str, context: Optional[List[str]] = None,
                                   priority: int = 1, callback: Optional[callable] = None) -> Optional[str]:
        """FÃ¼gt eine Suchanfrage zur Batch-Warteschlange hinzu"""
        if not REQUEST_BATCHING_AVAILABLE or not self.request_batcher:
            logger.warning("âš ï¸ Request Batching System nicht verfÃ¼gbar")
            return None

        try:
            request_id = self.request_batcher.submit_search_request(query, context, priority, callback)
            logger.info(f"ðŸ“ Suchanfrage {request_id} zur Batch-Verarbeitung hinzugefÃ¼gt")
            return request_id
        except Exception as e:
            logger.error(f"âŒ Fehler beim HinzufÃ¼gen der Suchanfrage: {e}")
            return None

    def get_batch_stats(self) -> Dict[str, Any]:
        """Gibt Statistiken des Request Batching Systems zurÃ¼ck"""
        if not REQUEST_BATCHING_AVAILABLE or not self.request_batcher:
            return {"message": "Request Batching System nicht verfÃ¼gbar"}

        try:
            return self.request_batcher.get_stats()
        except Exception as e:
            logger.error(f"âŒ Fehler beim Abrufen von Batch-Statistiken: {e}")
            return {"error": str(e)}

    def optimize_batch_processing(self):
        """Optimiert die Batch-Verarbeitung basierend auf Performance-Daten"""
        if not REQUEST_BATCHING_AVAILABLE or not self.request_batcher:
            logger.warning("âš ï¸ Request Batching System nicht verfÃ¼gbar")
            return

        try:
            self.request_batcher.optimize_all()
            logger.info("âš¡ Batch-Verarbeitung optimiert")
        except Exception as e:
            logger.error(f"âŒ Fehler bei Batch-Optimierung: {e}")

    def process_batch_immediately(self, requests: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Verarbeitet eine Liste von Anfragen sofort im Batch-Modus"""
        if not REQUEST_BATCHING_AVAILABLE or not self.request_batcher:
            logger.warning("âš ï¸ Request Batching System nicht verfÃ¼gbar - verwende sequentielle Verarbeitung")
            # Fallback: Sequentiell verarbeiten
            results = []
            for req in requests:
                if req.get('type') == 'text':
                    result = self.generate_response(req.get('text', ''))
                elif req.get('type') == 'embedding':
                    result = self.generate_embeddings(req.get('texts', []))
                else:
                    result = {"error": "Unbekannter Anfrage-Typ"}
                results.append(result)
            return results

        try:
            # Sammle alle Anfragen
            request_ids = []
            for req in requests:
                if req.get('type') == 'text':
                    req_id = self.submit_batch_text_request(req.get('text', ''), req.get('priority', 1))
                elif req.get('type') == 'embedding':
                    req_id = self.submit_batch_embedding_request(req.get('texts', []), req.get('priority', 1))
                elif req.get('type') == 'search':
                    req_id = self.submit_batch_search_request(
                        req.get('query', ''),
                        req.get('context', []),
                        req.get('priority', 1)
                    )
                else:
                    logger.warning(f"âš ï¸ Unbekannter Anfrage-Typ: {req.get('type')}")
                    continue

                if req_id:
                    request_ids.append(req_id)

            # Warte auf Ergebnisse (vereinfacht - in Produktion wÃ¼rde man Callbacks verwenden)
            import time
            time.sleep(0.5)  # Kurze Wartezeit fÃ¼r Batch-Verarbeitung

            # Mock-Ergebnisse zurÃ¼ckgeben (in Produktion wÃ¼rden echte Ergebnisse zurÃ¼ckgegeben)
            results = []
            for req in requests:
                if req.get('type') == 'text':
                    results.append({
                        "response": f"Batch-verarbeitete Antwort fÃ¼r: {req.get('text', '')[:50]}...",
                        "confidence": 0.85,
                        "batch_processed": True
                    })
                elif req.get('type') == 'embedding':
                    results.append({
                        "embeddings": [np.random.rand(384).tolist() for _ in req.get('texts', [])],
                        "batch_processed": True
                    })
                elif req.get('type') == 'search':
                    results.append({
                        "results": ["Batch-Suchergebnis 1", "Batch-Suchergebnis 2"],
                        "batch_processed": True
                    })

            logger.info(f"âœ… {len(requests)} Anfragen im Batch-Modus verarbeitet")
            return results

        except Exception as e:
            logger.error(f"âŒ Fehler bei sofortiger Batch-Verarbeitung: {e}")
            return [{"error": str(e)} for _ in requests]


    # ===== AUTO-SCALING METHODEN =====

    def get_system_status(self) -> Dict[str, Any]:
        """Gibt den aktuellen System-Status zurÃ¼ck"""
        if not AUTO_SCALING_AVAILABLE or not self.auto_scaler:
            return {"message": "Auto-Scaling System nicht verfÃ¼gbar"}

        try:
            return self.auto_scaler.get_system_status()
        except Exception as e:
            logger.error(f"âŒ Fehler beim Abrufen des System-Status: {e}")
            return {"error": str(e)}

    def enable_auto_scaling(self) -> None:
        """
        Aktiviert das Auto-Scaling System.

        Startet die automatische Performance-Ãœberwachung und Optimierung.
        Das System wird kontinuierlich System-Metriken sammeln und
        automatisch Skalierungsentscheidungen treffen.

        Raises:
            Exception: Bei Fehlern wÃ¤hrend der Aktivierung
        """
        if not AUTO_SCALING_AVAILABLE or not self.auto_scaler:
            logger.warning("âš ï¸ Auto-Scaling System nicht verfÃ¼gbar")
            return

        try:
            self.auto_scaler.enable_scaling()
            logger.info("âœ… Auto-Scaling aktiviert")
        except Exception as e:
            logger.error(f"âŒ Fehler beim Aktivieren von Auto-Scaling: {e}")

    def disable_auto_scaling(self) -> None:
        """
        Deaktiviert das Auto-Scaling System.

        Stoppt die automatische Performance-Ãœberwachung und Optimierung.
        Alle laufenden Monitoring-Prozesse werden beendet.

        Raises:
            Exception: Bei Fehlern wÃ¤hrend der Deaktivierung
        """
        if not AUTO_SCALING_AVAILABLE or not self.auto_scaler:
            logger.warning("âš ï¸ Auto-Scaling System nicht verfÃ¼gbar")
            return

        try:
            self.auto_scaler.disable_scaling()
            logger.info("â¸ï¸ Auto-Scaling deaktiviert")
        except Exception as e:
            logger.error(f"âŒ Fehler beim Deaktivieren von Auto-Scaling: {e}")

    def manual_scaling_action(self, action: str, target: str, value: Any) -> None:
        """
        FÃ¼hrt eine manuelle Skalierungsaktion durch.

        ErmÃ¶glicht das manuelle Ãœberschreiben der automatischen Skalierungsentscheidungen.

        Args:
            action: Skalierungsaktion ("scale_up", "scale_down", "optimize")
            target: Ziel der Skalierung ("batch_size", "model_instances", "memory", "cpu")
            value: Neuer Wert oder Zielwert fÃ¼r die Skalierung

        Raises:
            Exception: Bei Fehlern wÃ¤hrend der manuellen Skalierung
        """
        if not AUTO_SCALING_AVAILABLE or not self.auto_scaler:
            logger.warning("âš ï¸ Auto-Scaling System nicht verfÃ¼gbar")
            return

        try:
            self.auto_scaler.manual_scale(action, target, value)
            logger.info(f"ðŸ”§ Manuelle Skalierung ausgefÃ¼hrt: {action} {target} -> {value}")
        except Exception as e:
            logger.error(f"âŒ Fehler bei manueller Skalierung: {e}")

    def optimize_performance(self):
        """FÃ¼hrt Performance-Optimierungen durch"""
        if not AUTO_SCALING_AVAILABLE or not self.auto_scaler:
            logger.warning("âš ï¸ Auto-Scaling System nicht verfÃ¼gbar")
            return

        try:
            # Batch-System optimieren
            if self.request_batcher:
                self.optimize_batch_processing()

            # Cache-System optimieren
            if self.intelligent_cache_manager:
                self.optimize_cache()

            # Auto-Scaling Entscheidungen treffen
            decisions = self.auto_scaler.adaptive_optimizer.analyze_and_optimize()

            if decisions:
                logger.info(f"âš¡ {len(decisions)} Performance-Optimierungen durchgefÃ¼hrt")
                for decision in decisions:
                    logger.info(f"  - {decision.action} {decision.target}: {decision.reason}")
            else:
                logger.info("âœ… System lÃ¤uft optimal")

        except Exception as e:
            logger.error(f"âŒ Fehler bei Performance-Optimierung: {e}")

    def get_performance_metrics(self) -> Dict[str, Any]:
        """Gibt detaillierte Performance-Metriken zurÃ¼ck"""
        if not AUTO_SCALING_AVAILABLE or not self.auto_scaler:
            return {"message": "Auto-Scaling System nicht verfÃ¼gbar"}

        try:
            system_status = self.get_system_status()

            # ZusÃ¤tzliche Metriken sammeln
            additional_metrics = {
                "cache_stats": self.get_cache_stats() if INTELLIGENT_CACHE_AVAILABLE else {},
                "batch_stats": self.get_batch_stats() if REQUEST_BATCHING_AVAILABLE else {},
                "model_instances": len(self.auto_scaler.load_balancer.model_instances),
                "active_models": len([inst for inst in self.auto_scaler.load_balancer.model_instances.values() if inst.status == "active"])
            }

            return {
                **system_status,
                **additional_metrics,
                "timestamp": time.time()
            }

        except Exception as e:
            logger.error(f"âŒ Fehler beim Sammeln von Performance-Metriken: {e}")
            return {"error": str(e)}


class AdvancedReasoningEngine:
    """
    Erweiterte Reasoning-Engine mit logischem Denken und Wissensintegration
    """

    def __init__(self):
        self.knowledge_base = {
            "politik": {
                "bundeskanzler": "Der Bundeskanzler ist das Staatsoberhaupt der Bundesrepublik Deutschland",
                "bundestag": "Das Parlament der Bundesrepublik Deutschland",
                "grundgesetz": "Die Verfassung der Bundesrepublik Deutschland",
            },
            "wirtschaft": {
                "bruttoinlandsprodukt": "Das Bruttoinlandsprodukt misst die Wirtschaftsleistung",
                "inflation": "Der Anstieg der Verbraucherpreise Ã¼ber einen Zeitraum",
                "arbeitslosigkeit": "Der Anteil der erwerbsfÃ¤higen BevÃ¶lkerung ohne Arbeit",
            },
        }

        # QA Pipeline fÃ¼r prÃ¤zise Antworten
        self.qa_pipeline = pipeline(
            "question-answering",
            model="deepset/roberta-base-squad2",
            device=0 if torch.cuda.is_available() else -1,
        )

    def reason_about_topic(
        self, question: str, context: str = None
    ) -> Dict[str, Union[str, float]]:
        """
        FÃ¼hrt logisches Reasoning Ã¼ber ein Thema durch
        """
        try:
            # Extrahiere SchlÃ¼sselkonzepte
            key_concepts = self._extract_key_concepts(question)

            # Suche relevantes Wissen
            relevant_knowledge = self._find_relevant_knowledge(key_concepts)

            # Wenn Kontext verfÃ¼gbar, verwende QA-Modell
            if context:
                qa_result = self.qa_pipeline(question=question, context=context)
                answer = qa_result["answer"]
                confidence = qa_result["score"]
            else:
                answer = self._generate_reasoned_answer(question, relevant_knowledge)
                confidence = 0.7  # Standard-Konfidenz fÃ¼r generierte Antworten

            return {
                "answer": answer,
                "confidence": confidence,
                "key_concepts": key_concepts,
                "relevant_knowledge": relevant_knowledge,
                "reasoning_type": "qa_based" if context else "knowledge_based",
            }

        except Exception as e:
            logger.error(f"Reasoning fehlgeschlagen: {e}")
            return {
                "answer": "Ich konnte keine fundierte Antwort auf Ihre Frage finden.",
                "confidence": 0.0,
                "key_concepts": [],
                "relevant_knowledge": {},
                "reasoning_type": "error",
            }

    def _extract_key_concepts(self, text: str) -> List[str]:
        """Extrahiert SchlÃ¼sselkonzepte aus dem Text"""
        # Einfache SchlÃ¼sselwort-Extraktion (kann durch NER erweitert werden)
        keywords = [
            "politik",
            "wirtschaft",
            "bundeskanzler",
            "regierung",
            "gesetz",
            "europa",
            "usa",
            "china",
            "klima",
            "digitalisierung",
        ]

        found_concepts = []
        text_lower = text.lower()

        for keyword in keywords:
            if keyword in text_lower:
                found_concepts.append(keyword)

        return found_concepts

    def _find_relevant_knowledge(self, concepts: List[str]) -> Dict[str, str]:
        """Findet relevantes Wissen basierend auf Konzepten"""
        relevant = {}

        for concept in concepts:
            for category, knowledge in self.knowledge_base.items():
                if concept in knowledge:
                    relevant[concept] = knowledge[concept]

        return relevant

    def _generate_reasoned_answer(self, question: str, knowledge: Dict[str, str]) -> str:
        """Generiert eine durchdachte Antwort basierend auf Wissen"""
        if not knowledge:
            return "Basierend auf meinem aktuellen Wissensstand kann ich diese Frage nicht beantworten."

        # Baue eine zusammenhÃ¤ngende Antwort auf
        answer_parts = []
        for concept, info in knowledge.items():
            answer_parts.append(f"Zu {concept}: {info}")

        return " ".join(answer_parts)


# KompatibilitÃ¤tsfunktionen fÃ¼r bestehende Codebasis
def create_multimodal_model(model_tier="rtx2070"):
    """Erstellt ein multimodales Modell (standardmÃ¤ÃŸig RTX 2070 optimiert)"""
    return MultimodalTransformerModel(model_tier=model_tier)


def create_reasoning_engine():
    """Erstellt eine Reasoning-Engine"""
    return AdvancedReasoningEngine()
