#!/usr/bin/env python3
"""
üöÄ GPU-Accelerated RAG System f√ºr RTX 2070
============================================

Advanced RAG mit RTX 2070 GPU-Optimierung:
- CUDA-optimierte sentence-transformers
- GPU-accelerated vector search
- Mixed Precision (FP16) f√ºr Tensor Cores
- Smart Memory Management f√ºr 8GB VRAM
- Batch Processing Optimization

Kompatibel mit Advanced RAG System 2.0
Autor: Claude-3.5-Sonnet
Datum: 15. September 2025
"""

import json
import logging
import os
import pickle
import re
import time
import warnings
from collections import defaultdict
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple, Union

import faiss
import numpy as np
import torch
from rank_bm25 import BM25Okapi
from sentence_transformers import SentenceTransformer

# Import unserer RTX 2070 GPU Manager
from core.gpu_manager import (
    get_optimal_batch_size,
    get_rtx2070_manager,
    is_rtx2070_available,
    move_to_rtx2070,
    rtx2070_context,
)

logger = logging.getLogger(__name__)


class GPUAcceleratedRAG:
    """
    üöÄ GPU-Accelerated RAG System f√ºr RTX 2070

    Optimiert f√ºr NVIDIA GeForce RTX 2070:
    - 8GB VRAM optimal genutzt
    - Tensor Cores f√ºr FP16 Mixed Precision
    - CUDA-accelerated embeddings
    - Batch processing f√ºr hohen Durchsatz
    - Fallback zu CPU bei GPU-Problemen
    """

    def __init__(
        self,
        corpus_path: str = None,
        german_model: str = "deepset/gbert-large",
        fallback_model: str = "paraphrase-multilingual-MiniLM-L12-v2",
        use_hybrid_search: bool = True,
        bm25_weight: float = 0.3,
        semantic_weight: float = 0.7,
        use_gpu: bool = True,
        max_seq_length: int = 512,  # Optimiert f√ºr RTX 2070
    ):
        """
        Initialisiert GPU-Accelerated RAG System

        Args:
            corpus_path: Pfad zur Corpus-Datei
            german_model: Prim√§res deutsches GPU-Modell
            fallback_model: CPU Fallback Modell
            use_hybrid_search: Hybrid Search aktivieren
            bm25_weight: BM25 Gewichtung (0.0-1.0)
            semantic_weight: Semantic Search Gewichtung (0.0-1.0)
            use_gpu: RTX 2070 GPU verwenden
            max_seq_length: Maximale Sequenzl√§nge (512 optimal f√ºr RTX 2070)
        """
        self.corpus_path = corpus_path
        self.german_model_name = german_model
        self.fallback_model_name = fallback_model
        self.use_hybrid_search = use_hybrid_search
        self.bm25_weight = bm25_weight
        self.semantic_weight = semantic_weight
        self.use_gpu = use_gpu and is_rtx2070_available()
        self.max_seq_length = max_seq_length

        # RTX 2070 Manager
        self.gpu_manager = get_rtx2070_manager()

        # Model Storage
        self.model = None
        self.corpus = []
        self.corpus_entries = []  # F√ºr erweiterte Metadaten
        self.embeddings = None
        self.faiss_index = None
        self.bm25 = None

        # Performance Tracking
        self.performance_stats = {
            "embeddings_generated": 0,
            "searches_performed": 0,
            "average_search_time_ms": 0.0,
            "gpu_utilization_avg": 0.0,
            "cache_hits": 0,
        }

        # Embedding Cache f√ºr bessere Performance
        self.embedding_cache = {}
        self.cache_max_size = 1000

        logger.info(f"üöÄ GPU-Accelerated RAG initialisiert:")
        logger.info(f"   üéØ GPU Support: {'‚úÖ RTX 2070' if self.use_gpu else '‚ùå CPU Only'}")
        logger.info(f"   üìè Max Sequence Length: {self.max_seq_length}")
        logger.info(f"   üîÑ Hybrid Search: {'‚úÖ' if self.use_hybrid_search else '‚ùå'}")

        # Initialize System
        self._initialize_models()
        if corpus_path:
            self._load_corpus()

    def _initialize_models(self):
        """Initialisiert German Language Model mit RTX 2070 Optimierung"""
        try:
            logger.info("üöÄ Initialisiere GPU-optimiertes German Model...")

            if self.use_gpu:
                # RTX 2070 GPU Model Loading
                with rtx2070_context() as gpu_manager:
                    # Load model mit GPU-Optimierung
                    self.model = SentenceTransformer(
                        self.german_model_name, device=gpu_manager.get_device()
                    )

                    # RTX 2070 Optimierung
                    self.model.max_seq_length = self.max_seq_length

                    # Mixed Precision f√ºr Tensor Cores
                    if gpu_manager.tensor_cores_enabled:
                        self.model = gpu_manager.optimize_model_for_rtx2070(
                            self.model, enable_fp16=True
                        )
                        logger.info("‚úÖ Model mit FP16 Mixed Precision optimiert")

                    logger.info(f"‚úÖ German Model auf RTX 2070 geladen: {self.german_model_name}")

            else:
                # CPU Fallback
                logger.warning("‚ö†Ô∏è GPU nicht verf√ºgbar - verwende CPU Model")
                self.model = SentenceTransformer(self.fallback_model_name)
                self.model.max_seq_length = self.max_seq_length

        except Exception as e:
            logger.error(f"‚ùå Model Loading fehlgeschlagen: {e}")
            # Emergency CPU Fallback
            try:
                logger.info("üîÑ Versuche CPU Fallback Model...")
                self.model = SentenceTransformer(self.fallback_model_name)
                self.use_gpu = False
                logger.info("‚úÖ CPU Fallback Model geladen")
            except Exception as fallback_error:
                logger.error(f"‚ùå Auch CPU Fallback fehlgeschlagen: {fallback_error}")
                raise

    def _load_corpus(self):
        """L√§dt und indexiert Corpus mit GPU-Acceleration"""
        try:
            logger.info(f"üìö Lade Corpus: {self.corpus_path}")

            # Load Raw Corpus
            if self.corpus_path.endswith(".json"):
                with open(self.corpus_path, "r", encoding="utf-8") as f:
                    raw_data = json.load(f)
                    if isinstance(raw_data, list):
                        # Alte Format: Liste von Strings
                        self.corpus = raw_data
                        self.corpus_entries = [
                            {
                                "text": text,
                                "topic": "unbekannt",
                                "source": "unbekannt",
                                "date": "unbekannt",
                                "verified": False,
                            }
                            for text in raw_data
                        ]
                    elif isinstance(raw_data, dict) and "entries" in raw_data:
                        # Neue Format: Dictionary mit entries
                        self.corpus_entries = raw_data["entries"]
                        self.corpus = [entry["text"] for entry in self.corpus_entries]
                    else:
                        # Einzelnes Dictionary
                        self.corpus = [raw_data]
                        self.corpus_entries = [raw_data]
            else:
                with open(self.corpus_path, "r", encoding="utf-8") as f:
                    self.corpus = [line.strip() for line in f if line.strip()]
                    self.corpus_entries = [
                        {
                            "text": text,
                            "topic": "unbekannt",
                            "source": "unbekannt",
                            "date": "unbekannt",
                            "verified": False,
                        }
                        for text in self.corpus
                    ]

            logger.info(f"üìä Corpus geladen: {len(self.corpus)} Dokumente")

            # Generate Embeddings mit GPU-Acceleration
            self._generate_embeddings()

            # Build Search Indices
            self._build_indices()

            logger.info("‚úÖ Corpus erfolgreich indexiert")

        except Exception as e:
            logger.error(f"‚ùå Corpus Loading fehlgeschlagen: {e}")
            raise

    def _generate_embeddings(self):
        """Generiert Embeddings mit RTX 2070 GPU-Acceleration"""
        logger.info("üöÄ Generiere GPU-accelerated Embeddings...")

        start_time = time.time()

        try:
            if self.use_gpu:
                # RTX 2070 Batch Processing
                batch_size = get_optimal_batch_size("embedding_generation")

                with rtx2070_context() as gpu_manager:
                    logger.info(f"‚ö° GPU Batch Size: {batch_size}")

                    # Generate embeddings in batches
                    all_embeddings = []

                    for i in range(0, len(self.corpus), batch_size):
                        batch = self.corpus[i : i + batch_size]

                        # Mixed Precision Context f√ºr Tensor Cores
                        with gpu_manager.mixed_precision_context():
                            batch_embeddings = self.model.encode(
                                batch,
                                batch_size=batch_size,
                                show_progress_bar=True,
                                convert_to_numpy=True,  # Force NumPy conversion
                                normalize_embeddings=True,
                            )

                        # Convert zu NumPy
                        if isinstance(batch_embeddings, torch.Tensor):
                            batch_embeddings = batch_embeddings.cpu().numpy()
                        elif hasattr(batch_embeddings, "cpu"):
                            # Handle andere GPU Tensor Typen
                            batch_embeddings = batch_embeddings.cpu().numpy()

                        all_embeddings.append(batch_embeddings)

                        # Memory Management
                        if i % (batch_size * 4) == 0:
                            gpu_manager.clear_memory()
                            progress = (i + batch_size) / len(self.corpus) * 100
                            logger.info(f"   Progress: {progress:.1f}%")

                    # Combine alle Embeddings
                    self.embeddings = np.vstack(all_embeddings)

            else:
                # CPU Fallback
                logger.info("üíª CPU Embedding Generation...")
                self.embeddings = self.model.encode(
                    self.corpus, show_progress_bar=True, normalize_embeddings=True
                )

            generation_time = time.time() - start_time

            # Update Performance Stats
            self.performance_stats["embeddings_generated"] = len(self.corpus)
            avg_time_per_doc = (generation_time / len(self.corpus)) * 1000

            logger.info(f"‚úÖ Embeddings generiert:")
            logger.info(f"   üìä Dokumente: {len(self.corpus)}")
            logger.info(f"   ‚è±Ô∏è Gesamt: {generation_time:.2f}s")
            logger.info(f"   ‚ö° Pro Dokument: {avg_time_per_doc:.2f}ms")
            logger.info(f"   üìê Shape: {self.embeddings.shape}")

            # Save embeddings
            self._save_embeddings()

        except Exception as e:
            logger.error(f"‚ùå Embedding Generation fehlgeschlagen: {e}")
            raise

    def _build_indices(self):
        """Baut Search-Indices f√ºr Hybrid Search"""
        try:
            logger.info("üîß Baue Search-Indices...")

            # 1. FAISS Index f√ºr Semantic Search
            dimension = self.embeddings.shape[1]

            if self.use_gpu and is_rtx2070_available():
                # GPU FAISS Index (falls verf√ºgbar)
                try:
                    # Versuche GPU FAISS
                    import faiss

                    # Verwende L2-Normalized Index f√ºr bessere Scores mit normalisierten Embeddings
                    self.faiss_index = faiss.IndexFlatIP(
                        dimension
                    )  # Inner Product f√ºr normalisierte Vektoren
                    # Zus√§tzliche Normalisierung der Embeddings
                    norms = np.linalg.norm(self.embeddings, axis=1, keepdims=True)
                    norms[norms == 0] = 1  # Vermeide Division durch 0
                    normalized_embeddings = self.embeddings / norms
                    self.faiss_index.add(normalized_embeddings.astype("float32"))

                    logger.info(
                        f"‚úÖ FAISS Index erstellt: {dimension}D, {len(self.corpus)} docs (normalisiert)"
                    )

                except Exception as gpu_faiss_error:
                    logger.warning(f"‚ö†Ô∏è GPU FAISS nicht verf√ºgbar: {gpu_faiss_error}")
                    # Fallback zu CPU FAISS
                    self.faiss_index = faiss.IndexFlatIP(dimension)
                    norms = np.linalg.norm(self.embeddings, axis=1, keepdims=True)
                    norms[norms == 0] = 1
                    normalized_embeddings = self.embeddings / norms
                    self.faiss_index.add(normalized_embeddings.astype("float32"))
            else:
                # CPU FAISS Index
                self.faiss_index = faiss.IndexFlatIP(dimension)
                norms = np.linalg.norm(self.embeddings, axis=1, keepdims=True)
                norms[norms == 0] = 1
                normalized_embeddings = self.embeddings / norms
                self.faiss_index.add(normalized_embeddings.astype("float32"))
                logger.info(f"‚úÖ CPU FAISS Index erstellt: {dimension}D (normalisiert)")

            # 2. BM25 Index f√ºr Keyword Search
            if self.use_hybrid_search:
                # Preprocess f√ºr BM25
                preprocessed_corpus = [self._preprocess_text(doc) for doc in self.corpus]
                tokenized_corpus = [doc.split() for doc in preprocessed_corpus]

                self.bm25 = BM25Okapi(tokenized_corpus)
                logger.info("‚úÖ BM25 Index erstellt")

            logger.info("‚úÖ Alle Indices erfolgreich erstellt")

        except Exception as e:
            logger.error(f"‚ùå Index Building fehlgeschlagen: {e}")
            raise

    def retrieve_relevant_documents(
        self, query: str, top_k: int = 10, use_cache: bool = True
    ) -> List[Dict[str, Any]]:
        """
        Retrieve relevante Dokumente mit GPU-Acceleration

        Args:
            query: Suchanfrage
            top_k: Anzahl zur√ºckzugebender Dokumente
            use_cache: Embedding Cache verwenden

        Returns:
            Liste relevanter Dokumente mit Scores
        """
        start_time = time.time()

        try:
            # Cache Check
            cache_key = f"{query}_{top_k}"
            if use_cache and cache_key in self.embedding_cache:
                self.performance_stats["cache_hits"] += 1
                logger.debug(f"‚úÖ Cache Hit f√ºr Query: {query[:50]}...")
                return self.embedding_cache[cache_key]

            if self.use_hybrid_search and self.bm25:
                # Hybrid Search: BM25 + Semantic
                results = self._hybrid_search(query, top_k)
            else:
                # Nur Semantic Search
                results = self._semantic_search(query, top_k)

            # Cache Management
            if use_cache:
                self._update_cache(cache_key, results)

            # Performance Tracking
            search_time = (time.time() - start_time) * 1000
            self.performance_stats["searches_performed"] += 1

            # Update average
            current_avg = self.performance_stats["average_search_time_ms"]
            searches = self.performance_stats["searches_performed"]
            new_avg = ((current_avg * (searches - 1)) + search_time) / searches
            self.performance_stats["average_search_time_ms"] = new_avg

            logger.debug(f"üîç Search completed in {search_time:.2f}ms")

            return results

        except Exception as e:
            logger.error(f"‚ùå Document Retrieval fehlgeschlagen: {e}")
            return []

    def _semantic_search(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """GPU-accelerated Semantic Search"""
        try:
            # Generate Query Embedding mit GPU
            if self.use_gpu:
                with rtx2070_context() as gpu_manager:
                    with gpu_manager.mixed_precision_context():
                        query_embedding = self.model.encode(
                            [query], normalize_embeddings=True, convert_to_numpy=True
                        )
            else:
                query_embedding = self.model.encode([query], normalize_embeddings=True)

            # Zus√§tzliche Normalisierung f√ºr Konsistenz
            query_norm = np.linalg.norm(query_embedding, axis=1, keepdims=True)
            query_norm[query_norm == 0] = 1
            query_embedding = query_embedding / query_norm

            # FAISS Search
            scores, indices = self.faiss_index.search(query_embedding.astype("float32"), top_k)

            # Format Results mit erweiterten Metadaten
            results = []
            for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                if idx < len(self.corpus_entries):
                    entry = self.corpus_entries[idx]
                    results.append(
                        {
                            "text": entry["text"],
                            "score": float(score),
                            "rank": i + 1,
                            "method": "semantic",
                            "topic": entry.get("topic", "unbekannt"),
                            "source": entry.get("source", "unbekannt"),
                            "date": entry.get("date", "unbekannt"),
                            "language": entry.get("language", "de"),
                            "verified": entry.get("verified", False),
                            "confidence_level": self._calculate_confidence_level(float(score)),
                            "explanation": self._generate_explanation(entry, float(score)),
                        }
                    )

            return results

        except Exception as e:
            logger.error(f"‚ùå Semantic Search fehlgeschlagen: {e}")
            return []

    def _hybrid_search(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """GPU-accelerated Hybrid Search (BM25 + Semantic)"""
        try:
            # 1. Semantic Search
            semantic_results = self._semantic_search(query, top_k * 2)

            # 2. BM25 Search
            preprocessed_query = self._preprocess_text(query)
            tokenized_query = preprocessed_query.split()
            bm25_scores = self.bm25.get_scores(tokenized_query)

            # Top BM25 Results
            bm25_indices = np.argsort(bm25_scores)[::-1][: top_k * 2]
            bm25_results = []

            for i, idx in enumerate(bm25_indices):
                if idx < len(self.corpus_entries):
                    entry = self.corpus_entries[idx]
                    bm25_results.append(
                        {
                            "text": entry["text"],
                            "score": float(bm25_scores[idx]),
                            "rank": i + 1,
                            "method": "bm25",
                            "topic": entry.get("topic", "unbekannt"),
                            "source": entry.get("source", "unbekannt"),
                            "date": entry.get("date", "unbekannt"),
                            "language": entry.get("language", "de"),
                            "verified": entry.get("verified", False),
                            "confidence_level": self._calculate_confidence_level(
                                float(bm25_scores[idx])
                            ),
                            "explanation": self._generate_explanation(
                                entry, float(bm25_scores[idx])
                            ),
                        }
                    )

            # 3. Reciprocal Rank Fusion (RRF)
            combined_results = self._reciprocal_rank_fusion(semantic_results, bm25_results, top_k)

            return combined_results

        except Exception as e:
            logger.error(f"‚ùå Hybrid Search fehlgeschlagen: {e}")
            return self._semantic_search(query, top_k)

    def _reciprocal_rank_fusion(
        self, semantic_results: List[Dict], bm25_results: List[Dict], top_k: int
    ) -> List[Dict[str, Any]]:
        """Reciprocal Rank Fusion f√ºr Hybrid Search"""
        rrf_scores = defaultdict(float)
        doc_info = {}
        k = 60  # RRF parameter

        # Semantic Scores
        for rank, result in enumerate(semantic_results, 1):
            doc_text = result["text"]
            rrf_scores[doc_text] += self.semantic_weight / (k + rank)
            doc_info[doc_text] = result

        # BM25 Scores
        for rank, result in enumerate(bm25_results, 1):
            doc_text = result["text"]
            rrf_scores[doc_text] += self.bm25_weight / (k + rank)
            if doc_text not in doc_info:
                doc_info[doc_text] = result

        # Sort by RRF Score
        sorted_docs = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)

        # Format Final Results
        final_results = []
        for rank, (doc_text, rrf_score) in enumerate(sorted_docs[:top_k], 1):
            result = doc_info[doc_text].copy()
            result.update({"score": rrf_score, "rank": rank, "method": "hybrid_rrf"})
            final_results.append(result)

        return final_results

    def _calculate_confidence_level(self, score: float) -> str:
        """Berechnet Vertrauenslevel basierend auf Score"""
        # Normalisiere Score auf 0-1 Bereich falls n√∂tig
        if score > 1.0:
            score = score / 100.0  # Falls Score in Prozent angegeben

        if score >= 0.8:
            return "sehr hoch"
        elif score >= 0.7:
            return "hoch"
        elif score >= 0.6:
            return "mittel"
        elif score >= 0.5:
            return "niedrig"
        else:
            return "sehr niedrig"

    def _generate_explanation(self, entry: Dict[str, Any], score: float) -> str:
        """Generiert Erkl√§rung f√ºr das Suchergebnis"""
        confidence = self._calculate_confidence_level(score)
        source = entry.get("source", "unbekannt")
        date = entry.get("date", "unbekannt")
        verified = "verifiziert" if entry.get("verified", False) else "nicht verifiziert"

        explanation = f"Dieses Dokument stammt aus der Quelle '{source}' vom {date} "
        explanation += f"und ist {verified}. "
        explanation += f"Die √úbereinstimmung mit Ihrer Frage ist {confidence}."

        return explanation

    def _preprocess_text(self, text: str) -> str:
        """Text Preprocessing f√ºr Deutsche Texte"""
        # Lowercase
        text = text.lower()

        # Remove special characters but keep German umlauts
        text = re.sub(r"[^\w\s√§√∂√º√ü]", " ", text)

        # Remove extra whitespace
        text = " ".join(text.split())

        return text

    def _update_cache(self, cache_key: str, results: List[Dict]):
        """Update Embedding Cache mit LRU-√§hnlicher Logik"""
        if len(self.embedding_cache) >= self.cache_max_size:
            # Remove oldest entry
            oldest_key = next(iter(self.embedding_cache))
            del self.embedding_cache[oldest_key]

        self.embedding_cache[cache_key] = results

    def _save_embeddings(self):
        """Speichert Embeddings f√ºr sp√§tere Verwendung"""
        try:
            embeddings_path = "gpu_rag_embeddings.pkl"
            with open(embeddings_path, "wb") as f:
                pickle.dump(
                    {
                        "embeddings": self.embeddings,
                        "corpus": self.corpus,
                        "model_name": self.german_model_name,
                        "max_seq_length": self.max_seq_length,
                        "created_at": datetime.now().isoformat(),
                    },
                    f,
                )
            logger.info(f"üíæ Embeddings gespeichert: {embeddings_path}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Embeddings speichern fehlgeschlagen: {e}")

    def get_performance_stats(self) -> Dict[str, Any]:
        """Holt Performance-Statistiken"""
        stats = self.performance_stats.copy()

        if self.use_gpu:
            gpu_stats = self.gpu_manager.get_gpu_stats()
            if gpu_stats:
                stats.update(
                    {
                        "gpu_utilization_current": gpu_stats.gpu_utilization,
                        "gpu_memory_used_gb": gpu_stats.memory_used_gb,
                        "gpu_memory_utilization": gpu_stats.memory_utilization,
                        "gpu_temperature_c": gpu_stats.temperature_c,
                        "tensor_core_enabled": self.gpu_manager.tensor_cores_enabled,
                    }
                )

        stats["model_name"] = self.german_model_name
        stats["use_gpu"] = self.use_gpu
        stats["corpus_size"] = len(self.corpus)
        stats["cache_size"] = len(self.embedding_cache)

        return stats

    def benchmark_performance(self, test_queries: List[str], iterations: int = 5) -> Dict[str, Any]:
        """
        Performance Benchmark f√ºr RTX 2070 vs CPU

        Args:
            test_queries: Liste von Test-Queries
            iterations: Anzahl Benchmark-Iterationen

        Returns:
            Benchmark-Ergebnisse
        """
        logger.info(f"üöÄ Starte RAG Performance Benchmark ({iterations} Iterationen)...")

        results = {
            "gpu_enabled": self.use_gpu,
            "query_times_ms": [],
            "embedding_times_ms": [],
            "search_times_ms": [],
            "total_queries": len(test_queries) * iterations,
        }

        try:
            for iteration in range(iterations):
                logger.info(f"   Iteration {iteration + 1}/{iterations}")

                for query in test_queries:
                    # Measure Query Processing
                    start_time = time.perf_counter()

                    documents = self.retrieve_relevant_documents(
                        query, top_k=10, use_cache=False  # Disable cache f√ºr accurate timing
                    )

                    query_time = (time.perf_counter() - start_time) * 1000
                    results["query_times_ms"].append(query_time)

            # Calculate Statistics
            query_times = results["query_times_ms"]
            avg_time = sum(query_times) / len(query_times)
            min_time = min(query_times)
            max_time = max(query_times)

            benchmark_summary = {
                "avg_query_time_ms": avg_time,
                "min_query_time_ms": min_time,
                "max_query_time_ms": max_time,
                "queries_per_second": 1000 / avg_time,
                "gpu_enabled": self.use_gpu,
                "model_name": self.german_model_name,
                "corpus_size": len(self.corpus),
            }

            # GPU Stats hinzuf√ºgen
            if self.use_gpu:
                gpu_stats = self.gpu_manager.get_gpu_stats()
                if gpu_stats:
                    benchmark_summary.update(
                        {
                            "gpu_utilization": gpu_stats.gpu_utilization,
                            "gpu_memory_usage_gb": gpu_stats.memory_used_gb,
                            "gpu_temperature": gpu_stats.temperature_c,
                        }
                    )

            logger.info("‚úÖ RAG Benchmark abgeschlossen:")
            logger.info(f"   ‚ö° Avg Query Time: {avg_time:.1f}ms")
            logger.info(f"   üöÄ Queries/Second: {1000/avg_time:.1f}")
            logger.info(f"   üìä Total Queries: {len(query_times)}")

            return benchmark_summary

        except Exception as e:
            logger.error(f"‚ùå Benchmark fehlgeschlagen: {e}")
            return {"error": str(e)}

    def _calculate_confidence_level(self, score: float) -> str:
        """
        Berechnet Konfidenz-Level basierend auf Score

        Args:
            score: FAISS Inner Product Score (0-1 bei normalisierten Vektoren)

        Returns:
            Konfidenz-Level als String
        """
        try:
            # Bei normalisierten Vektoren mit Inner Product:
            # Score = 1.0 bedeutet perfekte √úbereinstimmung
            # Score = 0.0 bedeutet keine √úbereinstimmung
            # Typische Scores liegen zwischen 0.0 und 0.8

            confidence_percent = score * 100

            if confidence_percent >= 70:
                return "sehr hoch"
            elif confidence_percent >= 50:
                return "hoch"
            elif confidence_percent >= 30:
                return "mittel"
            elif confidence_percent >= 15:
                return "niedrig"
            else:
                return "sehr niedrig"
        except Exception as e:
            logger.warning(f"Fehler bei Konfidenzberechnung: {e}")
            return "unbekannt"

    def _generate_explanation(self, entry: Dict[str, Any], score: float) -> str:
        """
        Generiert Erkl√§rung f√ºr Retrieval-Ergebnis

        Args:
            entry: Dokument-Eintrag
            score: Retrieval-Score

        Returns:
            Erkl√§rung als String
        """
        try:
            confidence_percent = score * 100

            explanation_parts = []

            # Basis-Erkl√§rung
            explanation_parts.append(
                "Dieses Dokument stammt aus der Quelle '{source}' vom {date} und ist {verified}.".format(
                    source=entry.get("source", "unbekannt"),
                    date=entry.get("date", "unbekannt"),
                    verified="verifiziert" if entry.get("verified", False) else "nicht verifiziert",
                )
            )

            # √úbereinstimmungsgrad
            if confidence_percent >= 70:
                explanation_parts.append("Die √úbereinstimmung mit Ihrer Frage ist sehr hoch.")
            elif confidence_percent >= 50:
                explanation_parts.append("Die √úbereinstimmung mit Ihrer Frage ist hoch.")
            elif confidence_percent >= 30:
                explanation_parts.append("Die √úbereinstimmung mit Ihrer Frage ist mittel.")
            elif confidence_percent >= 15:
                explanation_parts.append("Die √úbereinstimmung mit Ihrer Frage ist niedrig.")
            else:
                explanation_parts.append("Die √úbereinstimmung mit Ihrer Frage ist sehr niedrig.")

            return " ".join(explanation_parts)

        except Exception as e:
            logger.warning(f"Fehler bei Erkl√§rungsgenerierung: {e}")
            return "Keine detaillierte Erkl√§rung verf√ºgbar."

    def _preprocess_text(self, text: str) -> str:
        """
        Preprocessing f√ºr BM25

        Args:
            text: Rohtext

        Returns:
            Vorverarbeiteter Text
        """
        try:
            # Kleinschreibung
            text = text.lower()

            # Entferne Sonderzeichen und Zahlen
            text = re.sub(r"[^\w\s√§√∂√º√ü]", " ", text)
            text = re.sub(r"\d+", " ", text)

            # Entferne extra Leerzeichen
            text = " ".join(text.split())

            return text

        except Exception as e:
            logger.warning(f"Fehler bei Text-Preprocessing: {e}")
            return text

    def _update_cache(self, key: str, results: List[Dict[str, Any]]):
        """
        Aktualisiert Embedding Cache

        Args:
            key: Cache-Schl√ºssel
            results: Suchergebnisse
        """
        try:
            # Cache Gr√∂√üe begrenzen
            if len(self.embedding_cache) >= self.cache_max_size:
                # Entferne √§ltesten Eintrag (FIFO)
                oldest_key = next(iter(self.embedding_cache))
                del self.embedding_cache[oldest_key]

            self.embedding_cache[key] = results

        except Exception as e:
            logger.warning(f"Fehler bei Cache-Aktualisierung: {e}")

    def _save_embeddings(self):
        """Speichert Embeddings f√ºr sp√§tere Verwendung"""
        try:
            if self.embeddings is not None:
                embeddings_path = os.path.join(
                    os.path.dirname(self.corpus_path) if self.corpus_path else ".",
                    "gpu_rag_embeddings.pkl",
                )

                with open(embeddings_path, "wb") as f:
                    pickle.dump(
                        {
                            "embeddings": self.embeddings,
                            "corpus_size": len(self.corpus),
                            "model_name": self.german_model_name,
                            "timestamp": datetime.now().isoformat(),
                        },
                        f,
                    )

                logger.info(f"üíæ Embeddings gespeichert: {embeddings_path}")

        except Exception as e:
            logger.warning(f"Fehler beim Speichern der Embeddings: {e}")


# Convenience Functions
def create_gpu_rag(corpus_path: str = None, use_gpu: bool = True, **kwargs) -> GPUAcceleratedRAG:
    """
    Erstellt GPU-Accelerated RAG System

    Args:
        corpus_path: Pfad zum Corpus
        use_gpu: RTX 2070 GPU verwenden
        **kwargs: Weitere Parameter

    Returns:
        GPUAcceleratedRAG Instanz
    """
    return GPUAcceleratedRAG(corpus_path=corpus_path, use_gpu=use_gpu, **kwargs)


if __name__ == "__main__":
    # Test GPU-Accelerated RAG
    print("üöÄ Testing GPU-Accelerated RAG System...")

    # Create test corpus
    test_corpus = [
        "Die Klimapolitik der Bundesregierung zielt auf Klimaneutralit√§t bis 2045 ab.",
        "Deutschland plant den Kohleausstieg bis 2038 zur Reduktion der CO2-Emissionen.",
        "Erneuerbare Energien sollen bis 2030 80% des Stromverbrauchs decken.",
        "Die Energiewende erfordert massive Investitionen in Wind- und Solarenergie.",
        "Das Klimaschutzgesetz definiert verbindliche Sektorziele f√ºr CO2-Reduktion.",
    ]

    # Test Embeddings direkt
    rag = GPUAcceleratedRAG()
    rag.corpus = test_corpus
    rag._generate_embeddings()
    rag._build_indices()

    # Test Query
    test_query = "Was sind die Klimaziele Deutschlands?"
    results = rag.retrieve_relevant_documents(test_query, top_k=3)

    print(f"\nQuery: {test_query}")
    for i, result in enumerate(results[:3], 1):
        print(f"{i}. Score: {result['score']:.3f}")
        print(f"   Text: {result['text'][:100]}...")

    # Performance Stats
    stats = rag.get_performance_stats()
    print(
        f"\nPerformance: GPU={stats['use_gpu']}, Avg Time={stats['average_search_time_ms']:.1f}ms"
    )
